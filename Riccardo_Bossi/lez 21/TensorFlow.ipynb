{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0054f283",
   "metadata": {},
   "source": [
    "## Time to get your hands dirty. The Tensorflow playground.\n",
    "\n",
    "\n",
    "Today we'll play with the TensorFlow playground. We'll see it next time, but Tensorflow is one of the main open-source deep learning libraries.\n",
    "\n",
    "The playground provides some datasets and let you see what's going on during the training process in a nice, intuitive way. \n",
    "\n",
    "- We have a function $f(x_1,x_2)$ that returns +1 (blue) or -1 (orange).\n",
    "- We are given a dataset and split it into training and testing. \n",
    "- Hit play, and the network starts training.\n",
    "- The thickness of the line tells you how much that synapis contributes to the training.\n",
    "- You can monitor progress on the top-right.\n",
    "- The goal of the game is to minimize the loss function **on the test set** (I don't care about the loss function on the training set!).\n",
    "- You can change everything but the type of problem, the test/train ratio, and the noise level.\n",
    "- So feel free to play with activation function, number of layers, number of neuron per layers, learning rate, regularization, incoming features (data augmentation), etc. \n",
    "\n",
    "\n",
    "1) Let's start from [this dataset](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=35&networkShape=1&seed=0.50246&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Try to get the lowest score on the test set. Take a screenshot or we won't believe you! Let's draw a leaderboard.\n",
    "\n",
    "2) Then move on to [this dataset](https://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.1&regularizationRate=0.01&noise=50&networkShape=3,2&seed=0.65406&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), which is more challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28789780",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fe7ab",
   "metadata": {},
   "source": [
    "At the beginning, I chose a dataset to work on with the goal of minimizing the loss for that dataset.  \n",
    "I then applied my deep learning knowledge combined with insights from ChatGPT about optimization strategies.\n",
    "\n",
    "I selected the **Gaussian dataset**, which generates two clusters of points roughly centered around different means in 2D space.  \n",
    "The decision boundary is nonlinear but relatively simple — usually elliptical or curved.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "The next step was to identify the most relevant features for the problem. Here is my reasoning:\n",
    "\n",
    "| Feature             | Description           | Usefulness for Gaussian Dataset          |\n",
    "|---------------------|-----------------------|------------------------------------------|\n",
    "| `x`, `y`            | Raw input coordinates | ✅ Essential, should be kept               |\n",
    "| `x*y`               | Interaction term      | ✅ Helps model curvature                   |\n",
    "| `x²`, `y²`          | Quadratic terms       | ✅ Important for capturing elliptical shapes |\n",
    "| `sin(x)`            | Periodic function     | ❌ Not useful in this case                 |\n",
    "| `cos(x)`            | Periodic function     | ❌ Not useful                             |\n",
    "\n",
    "Therefore, I decided to keep the features `x`, `y`, `x*y`, `x²`, and `y²`, discarding the periodic ones that would add complexity without benefit.\n",
    "\n",
    "---\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "Regarding the neural network design, a common heuristic is that the number of neurons per layer should be at least greater than the number of input features.  \n",
    "I manually chose a network with **4 layers** and **8 neurons per layer**.\n",
    "\n",
    "Initially, I was concerned about overfitting, but both the **training loss** and **test loss** dropped to zero, indicating the model is generalizing well and not overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "I chose the **`tanh`** activation function because:\n",
    "\n",
    "- It handles nonlinearity smoothly, which is essential for the Gaussian dataset.\n",
    "- It works well with noisy data (my noise level is 35), providing stable training.\n",
    "- Its output is zero-centered, which helps the network converge more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization\n",
    "\n",
    "I used **L2 regularization (Ridge)** for the following reasons:\n",
    "\n",
    "- Since I already selected relevant features, there was no need for aggressive pruning (which L1 encourages).\n",
    "- The high noise level makes L2 more stable than L1 under these conditions.\n",
    "- The decision boundary is smooth, so L2 helps keep weights small and distributed rather than forcing sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "I manually chose the following hyperparameters with the awareness that:\n",
    "\n",
    "- **Regularization rate** = 0.01 → balances penalizing large weights without overly restricting model capacity.\n",
    "- **Learning rate** = 0.003 → slow enough to allow stable convergence without unwanted oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "Finally, I selected an intermediate batch size because:\n",
    "\n",
    "- Too small batches produce noisy gradients and unstable training.\n",
    "- Too large batches risk overfitting the noisy data, reducing generalization ability.\n",
    "- An intermediate batch size strikes the best balance between stability and generalization.\n",
    "\n",
    "I choose 14 (default value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed21524",
   "metadata": {},
   "source": [
    "![Description of the image](TensorFlowoptimization.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
