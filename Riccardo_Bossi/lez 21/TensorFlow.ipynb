{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0054f283",
   "metadata": {},
   "source": [
    "## Time to get your hands dirty. The Tensorflow playground.\n",
    "\n",
    "\n",
    "Today we'll play with the TensorFlow playground. We'll see it next time, but Tensorflow is one of the main open-source deep learning libraries.\n",
    "\n",
    "The playground provides some datasets and let you see what's going on during the training process in a nice, intuitive way. \n",
    "\n",
    "- We have a function $f(x_1,x_2)$ that returns +1 (blue) or -1 (orange).\n",
    "- We are given a dataset and split it into training and testing. \n",
    "- Hit play, and the network starts training.\n",
    "- The thickness of the line tells you how much that synapis contributes to the training.\n",
    "- You can monitor progress on the top-right.\n",
    "- The goal of the game is to minimize the loss function **on the test set** (I don't care about the loss function on the training set!).\n",
    "- You can change everything but the type of problem, the test/train ratio, and the noise level.\n",
    "- So feel free to play with activation function, number of layers, number of neuron per layers, learning rate, regularization, incoming features (data augmentation), etc. \n",
    "\n",
    "\n",
    "1) Let's start from [this dataset](https://playground.tensorflow.org/#activation=linear&regularization=L2&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.01&regularizationRate=0&noise=35&networkShape=1&seed=0.50246&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false). Try to get the lowest score on the test set. Take a screenshot or we won't believe you! Let's draw a leaderboard.\n",
    "\n",
    "2) Then move on to [this dataset](https://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.1&regularizationRate=0.01&noise=50&networkShape=3,2&seed=0.65406&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false), which is more challenging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28789780",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90fe7ab",
   "metadata": {},
   "source": [
    "At the beginning, I chose a dataset to work on with the goal of minimizing the loss for that dataset.  \n",
    "I then applied my deep learning knowledge combined with insights from ChatGPT about optimization strategies.\n",
    "\n",
    "I selected the **Gaussian dataset**, which generates two clusters of points roughly centered around different means in 2D space.  \n",
    "The decision boundary is nonlinear but relatively simple — usually elliptical or curved.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "The next step was to identify the most relevant features for the problem. Here is my reasoning:\n",
    "\n",
    "| Feature             | Description           | Usefulness for Gaussian Dataset          |\n",
    "|---------------------|-----------------------|------------------------------------------|\n",
    "| `x`, `y`            | Raw input coordinates | ✅ Essential, should be kept               |\n",
    "| `x*y`               | Interaction term      | ✅ Helps model curvature                   |\n",
    "| `x²`, `y²`          | Quadratic terms       | ✅ Important for capturing elliptical shapes |\n",
    "| `sin(x)`            | Periodic function     | ❌ Not useful in this case                 |\n",
    "| `cos(x)`            | Periodic function     | ❌ Not useful                             |\n",
    "\n",
    "Therefore, I decided to keep the features `x`, `y`, `x*y`, `x²`, and `y²`, discarding the periodic ones that would add complexity without benefit.\n",
    "\n",
    "---\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "Regarding the neural network design, a common heuristic is that the number of neurons per layer should be at least greater than the number of input features.  \n",
    "I manually chose a network with **4 layers** and **8 neurons per layer**.\n",
    "\n",
    "Initially, I was concerned about overfitting, but both the **training loss** and **test loss** dropped to zero, indicating the model is generalizing well and not overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "I chose the **`tanh`** activation function because:\n",
    "\n",
    "- It handles nonlinearity smoothly, which is essential for the Gaussian dataset.\n",
    "- It works well with noisy data (my noise level is 35), providing stable training.\n",
    "- Its output is zero-centered, which helps the network converge more efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "### Regularization\n",
    "\n",
    "I used **L2 regularization (Ridge)** for the following reasons:\n",
    "\n",
    "- Since I already selected relevant features, there was no need for aggressive pruning (which L1 encourages).\n",
    "- The high noise level makes L2 more stable than L1 under these conditions.\n",
    "- The decision boundary is smooth, so L2 helps keep weights small and distributed rather than forcing sparsity.\n",
    "\n",
    "---\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "I manually chose the following hyperparameters with the awareness that:\n",
    "\n",
    "- **Regularization rate** = 0.01 → balances penalizing large weights without overly restricting model capacity.\n",
    "- **Learning rate** = 0.003 → slow enough to allow stable convergence without unwanted oscillations.\n",
    "\n",
    "---\n",
    "\n",
    "### Batch Size\n",
    "\n",
    "Finally, I selected an intermediate batch size because:\n",
    "\n",
    "- Too small batches produce noisy gradients and unstable training.\n",
    "- Too large batches risk overfitting the noisy data, reducing generalization ability.\n",
    "- An intermediate batch size strikes the best balance between stability and generalization.\n",
    "\n",
    "I choose 14 (default value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed21524",
   "metadata": {},
   "source": [
    "![Description of the image](TensorFlowoptimization.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd36bc2b",
   "metadata": {},
   "source": [
    "## Spiral Dataset Optimization Strategy\n",
    "\n",
    "Then I selected the **Spiral dataset**, a classic benchmark for testing a model’s ability to learn nonlinear and intertwined decision boundaries. The goal was to minimize the loss while maintaining generalization, despite the high noise level and fixed 50/50 train-test split.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Given the spiral’s complex, circular structure, I carefully selected features that could help the model capture nonlinear and periodic patterns:\n",
    "\n",
    "| Feature             | Description              | Usefulness for Spiral Dataset             |\n",
    "|---------------------|--------------------------|-------------------------------------------|\n",
    "| `x`, `y`            | Raw input coordinates    | Essential for spatial positioning         |\n",
    "| `x²`, `y²`          | Quadratic terms          | Help model radial curvature               |\n",
    "| `x*y`               | Interaction term         | Captures diagonal and rotational trends   | \n",
    "\n",
    "\n",
    "These features allow the network to learn curved and periodic boundaries, which are essential for distinguishing the spiral arms.\n",
    "\n",
    "### Network Architecture\n",
    "\n",
    "To model the spiral’s complexity, I used a deep feedforward neural network with the following structure:\n",
    "\n",
    "- 4 hidden layers\n",
    "  - Layer 1: 8 neurons\n",
    "  - Layer 2: 6 neurons\n",
    "  - Layer 3: 4 neurons\n",
    "  - Layer 4: 2 neurons\n",
    "\n",
    "This architecture balances depth and compression, enabling the network to learn high-level abstractions and then refine them into a compact decision boundary.\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "I chose **ReLU** as the activation function because:\n",
    "\n",
    "- It introduces nonlinearity without saturation (unlike sigmoid or tanh).\n",
    "- It speeds up convergence and works well with deeper networks.\n",
    "- Despite the spiral’s complexity, ReLU performed well when combined with the right features and architecture.\n",
    "\n",
    "### Regularization\n",
    "\n",
    "I applied **L2 regularization** with a rate of **0.001** to:\n",
    "\n",
    "- Prevent overfitting in the presence of high noise (50%).\n",
    "- Encourage smaller, more distributed weights.\n",
    "- Maintain smooth decision boundaries without forcing sparsity.\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "- **Learning rate**: 0.03 — fast enough to converge, but low enough to avoid instability.\n",
    "- **Epochs**: 824 — training continued until both training and test loss stabilized at 0.048, indicating excellent generalization.\n",
    "- **Batch size**: 25 — a balanced choice that smooths gradient updates while preserving generalization.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec3583",
   "metadata": {},
   "source": [
    "![Description of the image](TensorFlowoptimization_2.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
