{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816324f2",
   "metadata": {},
   "source": [
    "### LECTURE 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35648beb",
   "metadata": {},
   "source": [
    "### **Probability Density Function (PDF), Cumulative Distribution Function (CDF), and Quantile**\n",
    "\n",
    "- **PDF (Probability Density Function)**: Describes the probability for a continuous variable to take a specific value. The area under the PDF over an interval gives the probability of the variable falling within that interval.\n",
    "- **CDF (Cumulative Distribution Function)**: It is obtained by integrating the PDF from - infinity up to a certain values X. Gives the probability that a random variable is less than or equal to a certain value. \n",
    "- **Quantile**: it's the inverse of the CDF. The value below which a certain percentage of observations fall. For example, the 0.25 quantile (or 25th percentile) is the value below which 25% of the data lie.\n",
    "\n",
    "---\n",
    "\n",
    "### **Empirical and Theoretical Distributions**\n",
    "\n",
    "- **Theoretical Distribution**: A probability distribution derived from a known mathematical model (e.g., Normal, Poisson).\n",
    "- **Empirical Distribution**: Based on observed data. It approximates the distribution of a dataset and is typically represented by the empirical CDF or histogram.\n",
    "- Empirical distributions are used when the true distribution is unknown or difficult to model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Homoscedastic and Heteroscedastic Errors**\n",
    "\n",
    "- **Homoscedasticity**: The variance of the errors is constant.\n",
    "- **Heteroscedasticity**: The error variance changes with the data\n",
    "\n",
    "---\n",
    "\n",
    "### **Kolmogorov's Axioms and Probability**\n",
    "\n",
    "Kolmogorov formalized the foundation of probability with three axioms:\n",
    "\n",
    "1. **Non-negativity**: For any event A, the probability is non-negative:  \n",
    "   \\( P(A) >= 0 \\)\n",
    "2. **Normalization**: The probability of the entire sample space is 1:  \n",
    "   \\( P($\\Omega$) = 1 \\)\n",
    "3. **Additivity**: For any two mutually exclusive events A and B:  \n",
    "   \\( P(A $\\cup$ B) = P(A) + P(B) \\)\n",
    "\n",
    "These axioms form the basis of modern probability theory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem updates the probability of a hypothesis based on new evidence:\n",
    "\n",
    "$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$\n",
    "\n",
    "-  P(A|B) : Posterior probability (updated belief)  \n",
    "-  P(B|A) : Likelihood of observing B given A  \n",
    "-  P(A) : Prior probability of A  \n",
    "-  P(B) : Marginal probability of B  \n",
    "\n",
    "Used in many fields like medicine, machine learning, and decision theory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformations of Random Variables**\n",
    "\n",
    "Transforming a random variable means applying a function to it, creating a new variable.\n",
    "\n",
    "- **Example**: Let \\( X \\) be a random variable and \\( Y = g(X) \\) a transformation.\n",
    "- To find the **distribution of \\( Y \\)**:\n",
    "  - If \\( X \\) is continuous with PDF \\( $f_X$ \\) and \\( g \\) is invertible, then:\n",
    "\n",
    "$\n",
    "f_Y(y) = f_X(g^{-1}(y)) \\cdot \\left| \\frac{d}{dy} g^{-1}(y) \\right|\n",
    "$\n",
    "\n",
    "- This is used to derive distributions of functions of random variables (e.g., squares, sums, logarithms).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa401bbc",
   "metadata": {},
   "source": [
    "### LECTURE 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd6cd1",
   "metadata": {},
   "source": [
    "### **Monte Carlo Integration (Crude and Hit-or-Miss)**\n",
    "\n",
    "- **Monte Carlo integration** uses random sampling to approximate definite integrals.\n",
    "- **Crude Monte Carlo**:  \n",
    "  Estimate the integral $\\int_a^b f(x) \\, dx$ by sampling $x_i \\sim \\mathcal{U}(a, b)$ and computing:  \n",
    "  $$\n",
    "  I \\approx (b - a) \\cdot \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n",
    "  $$\n",
    "- **Hit-or-Miss method**:  \n",
    "  Sample uniformly in a rectangle that encloses the graph of $f(x)$.  \n",
    "  The integral is approximated by the fraction of points that fall below the curve times the area of the rectangle.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean, Median, and Expected Value**\n",
    "\n",
    "- **Mean**: Arithmetic average of a dataset.\n",
    "- **Median**: Middle value when data are ordered. Less sensitive to outliers.\n",
    "- **Expected value** ($\\mathbb{E}[X]$): Theoretical mean of a random variable. For continuous variables:  \n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int x f(x) \\, dx\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Standard Deviation, MAD (1), Variance, MAD (2), Quantile Region, Interquantile Range, Mode**\n",
    "\n",
    "- **Standard deviation** ($\\sigma$): Measures spread around the mean.\n",
    "- **MAD_1 (Mean Absolute Deviation)**:  \n",
    "  $$\n",
    "  \\text{MAD}_1 = \\frac{1}{N} \\sum_{i=1}^N |x_i - \\bar{x}|\n",
    "  $$\n",
    "- **Variance**:  \n",
    "  $$\n",
    "  \\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2]       with \\mu = \\mathbb{E}[X])\n",
    "  $$\n",
    "- **MAD_2**: Median Absolute Deviation = median $(|x_i - median({x_i})|)$\n",
    "- **Quantile region**: Range containing a central portion of the distribution (e.g., 95% interval).\n",
    "- **Interquantile range (IQR)**:  \n",
    "  $$\n",
    "  \\text{IQR} = Q_{75} - Q_{25}\n",
    "  $$\n",
    "\n",
    "It contain the 50% of the dataset\n",
    "- **Mode**: Most frequent value in a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Skewness and Kurtosis**\n",
    "\n",
    "- **Skewness**: Measures asymmetry of a distribution.\n",
    "  - Positive skew: tail to the right.\n",
    "  - Negative skew: tail to the left.\n",
    "- **Kurtosis**: Measures how likely extreme values (far from the average) are in a distribution.\n",
    "  - High kurtosis: heavy tails.\n",
    "  - Low kurtosis: light tails.\n",
    "  - Normal distribution has kurtosis $= 3$.\n",
    "\n",
    "---\n",
    "\n",
    "### **PDF vs Sample Statistics, Bessel's Correction**\n",
    "\n",
    "- **PDF statistics**: Theoretical values (mean, variance, etc.) computed from a probability distribution.\n",
    "- **Sample statistics**: Estimates of these quantities based on data.\n",
    "- **Besselâ€™s correction**: When estimating variance from a sample, divide by $N - 1$ instead of $N$ to correct bias:  \n",
    "  $$\n",
    "  s^2 = \\frac{1}{N - 1} \\sum_{i=1}^N (x_i - \\bar{x})^2\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "### **Uncertainties of Estimators**\n",
    "\n",
    "- Every estimator has **uncertainty** due to finite sample size.\n",
    "- For the **sample mean**:\n",
    "  $$\n",
    "  \\text{Standard error} = \\frac{\\sigma}{\\sqrt{N}}\n",
    "  $$\n",
    "- For the **sample variance** and **standard deviation** ($s$), the standard error can be approximated as:\n",
    "  $$\n",
    "  \\text{SE}(s) \\approx \\frac{\\sigma}{\\sqrt{2N}}\n",
    "  $$\n",
    "  where $\\sigma$ is the true standard deviation and $N$ is the sample size.\n",
    "- For the **Interquantile Range (IQR)**, the uncertainty depends on the density around the quartiles; a rough estimate of its standard error is:\n",
    "  $$\n",
    "  \\text{SE}(\\text{IQR}) \\approx \\frac{1.58 \\times \\text{IQR}}{\\sqrt{N}}\n",
    "  $$\n",
    "- Confidence intervals express the likely range of the true parameter.\n",
    "\n",
    "---\n",
    "\n",
    "### **PDFs: Uniform, Gaussian, Log-Normal, Chi-Squared, Poisson**\n",
    "\n",
    "- **Uniform**: All values in an interval have equal probability.  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{b - a}      \\text{    for   } x \\in [a, b]\n",
    "  $$\n",
    "\n",
    "  this distribution has $\\sigma = \\frac{b-a}{\\sqrt(12)}$\n",
    "- **Gaussian (Normal)**: Curve defined by mean $\\mu$ and std $\\sigma$.  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "  $$\n",
    "  - The convolution of two gaussian is a gaussian too.\n",
    "  - It's the quuen of distribution , because everything follow this shape and it's quite easy to use.\n",
    "  - $1\\sigma$ = 68% // $2\\sigma$ = 95%\n",
    "\n",
    "- **Log-Normal**: $X \\sim \\text{LogNormal}$ means $\\ln X \\sim \\text{Normal}$.\n",
    "- **Chi-squared** ($\\chi^2$):  \n",
    "  If we define standardized variables as  \n",
    "  $$\n",
    "  z_i = \\frac{x_i - \\mu}{\\sigma},\n",
    "  $$  \n",
    "  then the sum of their squares  \n",
    "  $$\n",
    "  Q = \\sum_{i=1}^K z_i^2\n",
    "  $$  \n",
    "  follows a **chi-squared distribution** with $K$ degrees of freedom.\n",
    "\n",
    "  The number of degrees of freedom $K$ is equal to the number of **independent** data points used in the sum.\n",
    "\n",
    "- **Poisson**: Discrete distribution for count data.  \n",
    "  $$\n",
    "  P(k; \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!}\n",
    "  $$\n",
    "  - Where: $\\mu$ is the mean, K is the number of events occouring\n",
    "  - Known as \"law of rare events\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Importance Sampling**\n",
    "\n",
    "- Hit or miss and Crude MC, are inefficient if the integrand has some null zone, or even if is really extendended... that's beacuse this 2 methode use the uniform distribution.\n",
    "- Instead of sampling from the uniform, sample from a **proposal distribution** $g(x)$ \n",
    "- Best when $g(x)$ is close to the shape of $f(x)$.\n",
    "- Reduces variance and computational cost if the $g(x)$ it's well chosen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c2c4",
   "metadata": {},
   "source": [
    "### LECTURE 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc59594",
   "metadata": {},
   "source": [
    "### **Central Limit Theorem (CLT)**\n",
    "\n",
    "- The CLT states that the sum (or mean) of a large number of independent, identically distributed random variables tends to follow a **normal distribution**, regardless of the original distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### **Law of Large Numbers (LLN)**\n",
    "\n",
    "- The LLN states that as the number of observations $N$ increases, the sample mean $\\bar{x}$ converges to the true mean $\\mu$:\n",
    "  $$\n",
    "  \\lim_{N \\to \\infty} \\bar{x} = \\mu\n",
    "  $$\n",
    "- This is a statement about convergence **in probability**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Multidimensional PDFs**\n",
    "\n",
    "- In 2D, the joint distribution can be described by:\n",
    "  - **Mean vector**:  \n",
    "    $$\n",
    "    \\vec{\\mu} = (\\mu_x, \\mu_y)\n",
    "    $$\n",
    "\n",
    "  - **Covariance matrix**:  \n",
    "    $$\n",
    "    \\Sigma = \\begin{pmatrix}\n",
    "    \\sigma_x^2 & \\text{cov}(x, y) \\\\\n",
    "    \\text{cov}(y, x) & \\sigma_y^2\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    The two off diagonal values are equal to 0 only if x & y are totaly uncorrelated\n",
    "\n",
    "  - **Correlation coefficient**:  \n",
    "    $$\n",
    "    \\rho = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
    "    $$\n",
    "    Express the percentual of correlation between the 2 variable\n",
    "\n",
    "  - **Principal axes**: determined by the eigenvectors of $\\Sigma$; note that the correlation vanish in this system by definition.\n",
    "  - **2D Confidence Ellipses**: regions where the joint probability is constant, keep attention, for each dimension the number of sigma has a different meaning: $1\\sigma = 39$% in 2 dimension! I can impose 68% for the similitude with 1D, but it's not $1\\sigma$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Correlation vs Causation**\n",
    "\n",
    "Correlation does not imply causation!\n",
    "Just because the sun burns our skin and also makes us thirsty, it doesn't mean that thirst causes sunburn!\n",
    "\n",
    "- **Pearson's correlation** (r) : Measures linear correlation between 2 different dataset; it's a value between -1 and 1, the 2 are uncorrelated only if r = 0.\n",
    "It has 2 problems:\n",
    "  - it's susceptible at the outliars\n",
    "  - doesn't count the error\n",
    "\n",
    "- **Spearman's rho**: Measures monotonic (rank-based) correlation.\n",
    "- **Kendall's tau**: Measures ordinal association between two variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rejection Sampling**\n",
    "\n",
    "Rejection sampling is a method to generate random samples from a complex distribution $p(x)$, using a simpler proposal distribution $q(x)$.\n",
    "\n",
    "The procedure works as follows:\n",
    "\n",
    "1. **Choose a proposal distribution** $q(x)$ from which it's easy to sample (often a uniform distribution).  \n",
    "   Make sure it's \"wide enough\" to cover the shape of $p(x)$, including its tails.\n",
    "\n",
    "2. **Find a constant** $M$ such that for all $x$:\n",
    "   $$\n",
    "   p(x) \\leq M q(x)\n",
    "   $$\n",
    "   This ensures the proposal dominates the target distribution.\n",
    "\n",
    "3. **Generate a candidate sample** $x $ from $q(x)$.\n",
    "\n",
    "4. **Draw a random number** $u $ from $ \\mathcal{U}(0, 1)$.\n",
    "\n",
    "5. **Accept or reject**:\n",
    "   - Accept $x$ if  \n",
    "     $$\n",
    "     u < \\frac{p(x)}{M q(x)}\n",
    "     $$\n",
    "   - Otherwise, reject $x$ and go back to step 3.\n",
    "\n",
    "The set of accepted $x$ values will follow the target distribution $p(x)$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Inverse Transform Sampling**\n",
    "\n",
    "- Used to sample from a distribution with known CDF $F(x)$ and Quantile.\n",
    "- Steps:\n",
    "  1. Sample $u$ from  ${U}(0, 1)$.\n",
    "  2. Compute $x = F^{-1}(u)$.\n",
    "Normalizarion here are rellly important.\n",
    "you can retrive the quantile and the CDF by numerically solution if you are not able to do in by hand"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b4cd6",
   "metadata": {},
   "source": [
    "### LECTURE 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
