{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "816324f2",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 2** - Probability & Statistic I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de5df3",
   "metadata": {},
   "source": [
    "- PDF, CDF, quantile\n",
    "- Real and empirical distributions\n",
    "- Errors: heteroscedastic and homoscedastic\n",
    "- Kolmogorov axioms and probability\n",
    "- Bayes’ theorem\n",
    "- Transformations of random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35648beb",
   "metadata": {},
   "source": [
    "### **Probability Density Function (PDF), Cumulative Distribution Function (CDF), and Quantile**\n",
    "\n",
    "- **PDF (Probability Density Function)**: Describes the probability for a continuous variable to take a specific value. The area under the PDF over an interval gives the probability of the variable falling within that interval.\n",
    "- **CDF (Cumulative Distribution Function)**: It is obtained by integrating the PDF from - infinity up to a certain values X. Gives the probability that a random variable is less than or equal to a certain value. \n",
    "\n",
    "$$\n",
    "H(x) = \\int_{-\\infty}^{x} h(x')\\, dx'\n",
    "$$\n",
    "\n",
    "\n",
    "- **Quantile**: it's the inverse of the CDF. The value below which a certain percentage of observations fall. For example, the 0.25 quantile (or 25th percentile) is the value below which 25% of the data lie.\n",
    "\n",
    "---\n",
    "\n",
    "### **Empirical and Theoretical Distributions**\n",
    "\n",
    "- **Theoretical Distribution**: A probability distribution derived from a known mathematical model (e.g., Normal, Poisson).\n",
    "- **Empirical Distribution**: Based on observed data. It approximates the distribution of a dataset and is typically represented by the empirical CDF or histogram.\n",
    "- Empirical distributions are used when the true distribution is unknown or difficult to model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Homoscedastic and Heteroscedastic Errors**\n",
    "\n",
    "- **Homoscedasticity**: The variance of the errors is constant.\n",
    "- **Heteroscedasticity**: The error variance changes with the data\n",
    "\n",
    "---\n",
    "\n",
    "### **Kolmogorov's Axioms and Probability**\n",
    "\n",
    "Kolmogorov formalized the foundation of probability with three axioms:\n",
    "\n",
    "1. **Non-negativity**: For any event A, the probability is non-negative:  \n",
    "   \\( P(A) >= 0 \\)\n",
    "2. **Normalization**: The probability of the entire sample space is 1:  \n",
    "   \\( P($\\Omega$) = 1 \\)\n",
    "3. **Additivity**: For any two mutually exclusive events A and B:  \n",
    "   \\( P(A $\\cup$ B) = P(A) + P(B) \\)\n",
    "\n",
    "These axioms form the basis of modern probability theory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem updates the probability of a hypothesis based on new evidence:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "-  P(A|B) : Posterior probability (updated belief)  \n",
    "-  P(B|A) : Likelihood of observing B given A  \n",
    "-  P(A) : Prior probability of A  \n",
    "-  P(B) : Marginal probability of B. We can write the marginal probability of x as:   $ p(x) = \\int p(x,y) dy = \\int p(x|y)p(y)dy$\n",
    "\n",
    "Used in many fields like medicine, machine learning, and decision theory.\n",
    "\n",
    "### Example: COVID Test\n",
    "\n",
    "Suppose:\n",
    "- 1% of the population has COVID:  **P(COVID) = 0.01**\n",
    "- The test is 99% sensitive:  **P(Positive | COVID) = 0.99**\n",
    "- The test is 95% specific:  **P(Negative | No COVID) = 0.95**, so **P(Positive | No COVID) = 0.05**\n",
    "\n",
    "We want to find the probability that someone actually has COVID **given that they tested positive**, i.e., **P(COVID | Positive)**.\n",
    "\n",
    "**Apply Bayes' Theorem:**\n",
    "\n",
    "$$\n",
    "P(\\text{COVID} | \\text{Positive}) = \\frac{P(\\text{Positive} | \\text{COVID}) \\cdot P(\\text{COVID})}{P(\\text{Positive})}\n",
    "$$\n",
    "\n",
    "We compute the denominator using the law of total probability:\n",
    "\n",
    "$$\n",
    "P(\\text{Positive}) = P(\\text{Positive} | \\text{COVID}) \\cdot P(\\text{COVID}) + P(\\text{Positive} | \\text{No COVID}) \\cdot P(\\text{No COVID})\n",
    "$$\n",
    "\n",
    "Plug in the numbers:\n",
    "\n",
    "$$\n",
    "P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0099 + 0.0495 = 0.0594\n",
    "$$\n",
    "\n",
    "Now compute the posterior:\n",
    "\n",
    "$$\n",
    "P(\\text{COVID} | \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx \\frac{0.0099}{0.0594} \\approx 0.1667\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "Even after testing positive, the probability that the person actually has COVID is only **~16.7%**. This highlights the importance of considering base rates (prior probabilities) when interpreting test results.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformations of Random Variables**\n",
    "\n",
    "Transforming a random variable means applying a function to it to create a new one. This is often necessary when we are interested in a quantity that depends on a known random variable.\n",
    "\n",
    "#### Basic Idea\n",
    "\n",
    "Let $X$ be a continuous random variable with known probability density function (PDF) $f_X(x)$, and let $Y = g(X)$ be a transformation of $X$. If the function $g$ is **invertible** and **differentiable**, then the probability density function of $Y$, denoted $f_Y(y)$, is given by:\n",
    "\n",
    "$$\n",
    "f_Y(y) = f_X(g^{-1}(y)) \\cdot \\left| \\frac{d}{dy} g^{-1}(y) \\right|\n",
    "$$\n",
    "\n",
    "\n",
    "#### Intuitive Explanation\n",
    "\n",
    "Suppose you know how $X$ behaves — for example, its PDF describes how likely different values of $X$ are. Now, you define a new variable $Y$ by applying a function $g$ to $X$. You want to understand how $Y$ behaves: how its probabilities are distributed.\n",
    "\n",
    "Here’s a simple way to think about it:\n",
    "\n",
    "- When you apply $g$ to $X$, you're **remapping** the real line: every value of $x$ is being shifted or stretched into a new value $y = g(x)$.\n",
    "- But probability doesn’t disappear or appear — it just **moves around**. So, the probability that $X$ is near some value $x$ becomes the probability that $Y$ is near $g(x)$.\n",
    "\n",
    "To compute the new density $f_Y(y)$:\n",
    "\n",
    "1. **Start from the original density**: You look at the density of $X$ at the point $x = g^{-1}(y)$. That’s the value of $X$ that maps to $y$.\n",
    "2. **Adjust for stretching or compressing**: The function $g$ might stretch or squeeze the space around $y$. This change is measured by the **derivative** of the inverse function, $\\left| \\frac{d}{dy} g^{-1}(y) \\right|$.\n",
    "   - If the transformation **compresses** values together (e.g., maps a wide range of $x$ to a narrow range of $y$), the density increases.\n",
    "   - If it **stretches** values apart (e.g., maps a small range of $x$ to a large range of $y$), the density decreases.\n",
    "\n",
    "In short:  \n",
    "The formula tells you how to translate probabilities from $X$ to $Y$ by \"following the mass\" through the transformation and adjusting for any stretching or shrinking.\n",
    "\n",
    "This ensures that the total probability remains 1, and the new PDF correctly reflects the behavior of the transformed variable.\n",
    "\n",
    "\n",
    "\n",
    "#### **Example: Squaring a Uniform Variable**\n",
    "\n",
    "Let $X \\sim \\text{Uniform}(0, 1)$\n",
    "\n",
    "Now define $Y = X^2$. We want to find the distribution of $Y$.\n",
    "\n",
    "- Invert the transformation: $X = \\sqrt{Y}$\n",
    "- Compute the derivative: $\\frac{d}{dy} \\sqrt{y} = \\frac{1}{2\\sqrt{y}}$\n",
    "\n",
    "Now apply the change-of-variable formula:\n",
    "\n",
    "$$\n",
    "f_Y(y) = f_X(\\sqrt{y}) \\cdot \\left| \\frac{1}{2\\sqrt{y}} \\right| = \\begin{cases}\n",
    "\\frac{1}{2\\sqrt{y}} & \\text{if } 0 \\leq y \\leq 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The transformed variable $Y = X^2$ has a distribution that is concentrated more near 0 than near 1, even though $X$ was uniform. This shows how transformations can significantly affect the shape of a distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa401bbc",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 3** - Probability & Statistic II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9b6e4",
   "metadata": {},
   "source": [
    "- Monte Carlo integration (crude / hit-or-miss/ importance sampling)  \n",
    "- Mean, median, and expected value  \n",
    "- Standard deviation, MAD1, variance, MAD2, quantile region, interquantile range, mode  \n",
    "- Skewness  \n",
    "- Kurtosis  \n",
    "- Statistics of the PDF and sample; Bessel’s correction  \n",
    "- Uncertainties of estimators  \n",
    "- PDFs: uniform, Gaussian, log-normal, chi-squared, Poisson   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd6cd1",
   "metadata": {},
   "source": [
    "### **Monte Carlo Integration**\n",
    "\n",
    "**Monte Carlo methode** uses random generated number to approximate matemathical and physical problem, such as integration.\n",
    "- **Crude Monte Carlo**:  \n",
    "  Estimate the integral $\\int_a^b f(x) \\, dx$ by sampling $x_i \\sim \\mathcal{U}(a, b)$ and computing:  \n",
    "  $$\n",
    "  I \\approx (b - a) \\cdot \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n",
    "  $$\n",
    "- **Hit-or-Miss method**:  \n",
    "  Sample uniformly in a rectangle that encloses the graph of $f(x)$.  \n",
    "  The integral is approximated by the fraction of points that fall below the curve times the area of the rectangle.\n",
    "\n",
    "-  **Importance Sampling**\n",
    "    - Hit or miss and Crude MC, are inefficient if the integrand has some null zone, or even if is really extendended... that's beacuse this 2 methode  use the uniform distribution.\n",
    "    - Instead of sampling from the uniform, sample from a **proposal distribution** $g(x)$ \n",
    "    - Best when $g(x)$ is close to the shape of $f(x)$.\n",
    "    - Reduces variance and computational cost if the $g(x)$ it's well chosen\n",
    "---\n",
    "\n",
    "### **Mean, Median, Expected Value, and Mode**\n",
    "\n",
    "KEEP ATTENTION AT THE DIFFERENT USE OF $\\bar{x}$ AND $\\mu$\n",
    "\n",
    "- **Mean**: Arithmetic average of a dataset. $\\mu = \\mathbb{E}[X]$, Where X will denote an entire dataset.\n",
    "- **Median**: Middle value when data are ordered. Less sensitive to outliers.\n",
    "- **Expected value** $\\mathbb{E}[X]$ : Theoretical mean of a random variable. For continuous variables:  \n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int x f(x) \\, dx\n",
    "  $$\n",
    "- **Mode**: Most frequent value in a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Standard Deviation, Variance, MAD, Quantiles, and IQR**\n",
    "\n",
    "- **Variance** (2nd-order moment):  \n",
    "  $$\n",
    "  \\sigma^2 = \\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\int_{-\\infty}^{+\\infty}(x - \\mu)^2 f(x) \\, dx\n",
    "  $$\n",
    "  f(x) is the probability density function (PDF) of the continuous random variable x\n",
    "- **Standard Deviation** $\\sigma$: Measures spread around the mean. It is the square root of the variance:\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\text{Var}(X)} = \\sqrt{\\sigma^2}\n",
    "  $$\n",
    "- **MAD_1 (Mean Absolute Deviation)**:  \n",
    "  $$\n",
    "  \\text{MAD}_1 = \\frac{1}{N} \\sum_{i=1}^N |x_i - \\bar{x}|\n",
    "  $$\n",
    "  Note: this is not differentiable at  x = 0 , so it's sometimes avoided in optimization.\n",
    "- **MAD_2 (Median Absolute Deviation)**:  \n",
    "  $$\n",
    "  \\text{MAD}_2 = \\frac{1}{N} \\sum_{i=1}^N |x_i - M|\n",
    "  $$\n",
    "  where M is the median.\n",
    "- **Quantile region**: Range containing a central portion of the distribution (e.g., 95% interval).\n",
    "- **Interquantile Range (IQR)**:  \n",
    "  $$\n",
    "  \\text{IQR} = Q_{75} - Q_{25} \n",
    "  $$\n",
    "  Contains the central 50% of the data.\n",
    "  - It' usefull to define $\\sigma_G = \\frac{IQR}{1.349} $ stands for the robust estimate of the standard deviation, typically using the interquartile range (IQR) instead of the usual standard deviation.\n",
    "\n",
    "---\n",
    "\n",
    "### **Skewness and Kurtosis**\n",
    "\n",
    "- **Skewness**: Measures asymmetry of a distribution (3rd-order moment).  \n",
    "  - Positive skew: tail to the right.  \n",
    "  - Negative skew: tail to the left.  \n",
    "  - Formula:\n",
    "    $$\n",
    "    \\text{Skewness} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{x_i - \\bar{x}}{\\sigma} \\right)^3\n",
    "    $$\n",
    "- **Kurtosis**: Measures how likely extreme values (far from the mean) are (4th-order moment).  \n",
    "  - High kurtosis: heavy tails.  \n",
    "  - Low kurtosis: light tails.  \n",
    "  - Normal distribution has kurtosis = 3.  \n",
    "  - Formula:\n",
    "    $$\n",
    "    \\text{Kurtosis} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{x_i - \\bar{x}}{\\sigma} \\right)^4\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### **PDFs: Uniform, Gaussian, Log-Normal, Chi-Squared, Poisson**\n",
    "\n",
    "This section summarizes key properties of common probability distributions.\n",
    "\n",
    "\n",
    "####  **Uniform Distribution**  (on $[a, b]$)\n",
    "\n",
    "- **PDF**:  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{b - a}, \\quad \\text{for } x \\in [a, b]\n",
    "  $$\n",
    "- **Mean**: $\\mu = \\frac{a + b}{2}$\n",
    "- **Variance**: $\\sigma^2 = \\frac{(b - a)^2}{12}$\n",
    "- **Skewness**: 0 (symmetric)\n",
    "- **Kurtosis** (excess): $-1.2$\n",
    "\n",
    "\n",
    "\n",
    "####  **Gaussian (Normal) Distribution**  ($X \\sim \\mathcal{N}(\\mu, \\sigma^2)$)\n",
    "\n",
    "- **PDF**:  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "  $$\n",
    "- **Mean**: $\\mu$\n",
    "- **Variance**: $\\sigma^2$\n",
    "- **Skewness**: 0 (perfectly symmetric)\n",
    "- **Kurtosis** (excess): 0\n",
    "\n",
    "Notes:\n",
    "- The sum (or convolution) of two Gaussian variables is still Gaussian.\n",
    "- Called the \"queen of distributions\" — many natural phenomena approximate a normal distribution due to the Central Limit Theorem.\n",
    "- Probability within:\n",
    "  - $1\\sigma$: ~68%\n",
    "  - $2\\sigma$: ~95%\n",
    "  - $3\\sigma$: ~99.7%\n",
    "\n",
    "\n",
    "#### **Chi-Squared Distribution** ($X \\sim \\chi^2(k)$)\n",
    "\n",
    "- Defined as:  \n",
    "  $$\n",
    "  X = \\sum_{i=1}^k \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2\n",
    "  $$\n",
    "- **Mean**: $\\mu = k$\n",
    "- **Variance**: $\\sigma^2 = 2k$\n",
    "- **Skewness**: $\\sqrt{8/k}$\n",
    "- **Kurtosis** (excess): $12/k$\n",
    "- **What is $k$?**\n",
    "    - $k$ is the number of **degrees of freedom** (DOF).\n",
    "    - It typically corresponds to the number of **independent standardized variables** being squared and summed.\n",
    "    - For example, in hypothesis testing, $k$ is often the number of data points **minus the number of fitted parameters**.\n",
    "\n",
    "\n",
    "#### **Reduced Chi-Squared ($\\chi^2_{\\text{red}}$)**\n",
    "\n",
    "When evaluating the goodness-of-fit of a model, it's common to compute the **reduced chi-squared**:\n",
    "\n",
    "$$\n",
    "\\chi^2_{\\text{red}} = \\frac{\\chi^2}{k} = \\frac{1}{k} \\sum_{i=1}^N \\left( \\frac{y_i - f(x_i)}{\\sigma_i} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\chi^2$ is the total chi-squared statistic.\n",
    "- $k = N - p$ is the degrees of freedom (number of data points $N$ minus number of fitted parameters $p$).\n",
    "- $f(x_i)$ is the model prediction for data point $i$.\n",
    "- $\\sigma_i$ is the uncertainty on $y_i$.\n",
    "\n",
    "\n",
    "#### **Poisson Distribution** ($X \\sim \\text{Poisson}(\\mu)$)\n",
    "\n",
    "- **PMF**:  \n",
    "  $$\n",
    "  P(k; \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!}, \\quad k = 0, 1, 2, \\dots\n",
    "  $$\n",
    "- **Mean**: $\\mu$\n",
    "- **Variance**: $\\mu$\n",
    "- **Skewness**: $\\frac{1}{\\sqrt{\\mu}}$\n",
    "- **Kurtosis** (excess): $\\frac{1}{\\mu}$\n",
    "\n",
    "Models counts of rare events in fixed intervals (e.g., radioactive decay, emails per hour).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c2c4",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 4** - Probability & Statistic III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaca67",
   "metadata": {},
   "source": [
    "- Central Limit Theorem  \n",
    "- Law of Large Numbers  \n",
    "- Multidimensional PDFs (mean, sigma x and y, covariance, correlation coefficient, principal axes, 2D confidence level)  \n",
    "- Correlation vs causation (Pearson, Spearman, Kendall)  \n",
    "- Rejection sampling  \n",
    "- Inverse sampling  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc59594",
   "metadata": {},
   "source": [
    "### **Central Limit Theorem (CLT)**\n",
    "\n",
    "The CLT states that the sum (or mean) of a large number of independent, identically distributed random variables tends to follow a **normal distribution**, regardless of the original distribution.\n",
    "This theoreme is the faundation for the repeated measurments .\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/330px-IllustrationCentralTheorem.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### **Law of Large Numbers (LLN)** (Bernoulli's theoreme)\n",
    "\n",
    "- The LLN states that as the number of observations $N$ increases, the sample mean $\\bar{x}$ converges to the true mean $\\mu$, this is also valid fot the variance:\n",
    "  $$\n",
    "  \\lim_{N \\to \\infty} \\bar{x} = \\mu \\quad, \\quad \\lim_{N \\to \\infty} s = \\sigma\n",
    "  $$\n",
    "- This is a statement about convergence **in probability**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Multidimensional PDFs**\n",
    "\n",
    "- In 2D, the joint distribution can be described by:\n",
    "  - **Mean vector**:  \n",
    "    $$\n",
    "    \\vec{\\mu} = (\\mu_x, \\mu_y)\n",
    "    $$\n",
    "    where, of course, $\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$\n",
    "  - **Covariance matrix**:  \n",
    "    $$\n",
    "    \\Sigma = \\begin{pmatrix}\n",
    "    \\sigma_x^2 & \\text{cov}(x, y) \\\\\n",
    "    \\text{cov}(y, x) & \\sigma_y^2\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    The two off diagonal values are equal to 0 only if x & y are totaly uncorrelated.\n",
    "    $$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "    $$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/negative-and-positive-covariance.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "  - **Correlation coefficient**:  \n",
    "    $$\n",
    "    \\rho = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
    "    $$\n",
    "    Express the percentual of correlation between the 2 variable\n",
    "\n",
    "  - **Principal axes**: determined by the eigenvectors of $\\Sigma$; note that the correlation vanish in this system by definition.\n",
    "  - **2D Confidence Ellipses**: regions where the joint probability is constant, keep attention, for each dimension the number of sigma has a different meaning: $1\\sigma = 39$% in 2 dimension! I can impose 68% for the similitude with 1D, but it's not $1\\sigma$.\n",
    "\n",
    "  Outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$\n",
    "\n",
    "- **Marginalized PDFs**\n",
    "\n",
    "    If you have the joint PDF $h(x, y)$, you can obtain the **marginal distribution** of one variable by **integrating out** the other:\n",
    "\n",
    "    - Marginal PDF of $x$:\n",
    "      $$\n",
    "      f_X(x) = \\int_{-\\infty}^{\\infty} h(x, y) \\, dy\n",
    "      $$\n",
    "\n",
    "    These marginalized PDFs describe the individual behavior of each variable, regardless of the other.\n",
    "---\n",
    "\n",
    "\n",
    "### **Correlation vs Causation**\n",
    "\n",
    "Correlation does not imply causation!\n",
    "Just because the sun burns our skin and also makes us thirsty, it doesn't mean that thirst causes sunburn!\n",
    "\n",
    "- **Pearson's coefficient** ($r$) : Measures linear correlation between 2 different dataset; it's a value between -1 and 1, the 2 are uncorrelated only if r = 0.\n",
    "It has 2 problems:\n",
    "  - it's susceptible at the outliars\n",
    "  - doesn't count the error\n",
    "\n",
    "- **Spearman's coefficient** ($r_s$): Measures monotonic (rank-based) correlation.\n",
    "- **Kendall's coefficient** ($\\tau$): Measures ordinal association between two variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rejection Sampling**\n",
    "\n",
    "Rejection sampling is a method for generating random samples from a complex **target distribution** $p(x)$, using a simpler and easy-to-sample **proposal distribution** $q(x)$.\n",
    "\n",
    "The idea is to generate candidate points from $q(x)$, and accept or reject them in a way that ensures the final samples follow the desired distribution $p(x)$.\n",
    "\n",
    "#### Algorithm\n",
    "\n",
    "1. **Choose a proposal distribution** $q(x)$ that is easy to sample from and covers the support of $p(x)$.\n",
    "2. **Find a constant** $M$ such that $p(x) \\leq M \\cdot q(x)$ for all $x$. (so that the distribution will cover all the function)\n",
    "3. **Repeat until enough samples are accepted**:\n",
    "   - Sample $x \\sim q(x)$\n",
    "   - Sample $u \\sim \\text{Uniform}(0, 1)$\n",
    "   - If $u \\leq \\frac{p(x)}{M \\cdot q(x)}$, **accept** $x$ (I find the ratio between the hight of the two distribution and transform it in an acceptance treshold)\n",
    "   - Otherwise, **reject** $x$\n",
    "\n",
    "The set of accepted values will follow the target distribution $p(x)$.\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "- You're drawing points under the curve of $M \\cdot q(x)$, and keeping only those that also fall under $p(x)$.\n",
    "- The factor $\\frac{p(x)}{M q(x)} \\in [0, 1]$ determines the acceptance probability.\n",
    "- The better $q(x)$ approximates the shape of $p(x)$, the higher the acceptance rate.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/rejection_sampling_example.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c65a7b",
   "metadata": {},
   "source": [
    "### **Inverse Transform Sampling**\n",
    "Rejection sampling works, but wouldn't it be awesome if we didn't have to discard *any* points during our sampling? This is the power and simplicity of **inverse transform sampling**.\n",
    "\n",
    "- Used to sample from a distribution $h(x)$ with known CDF $H(x)$ and Quantile.\n",
    "- Steps:\n",
    "  1. Sample $u$ from  ${U}(0, 1)$.\n",
    "  2. Using the quantile function $H^{-1}(x)$, find the value of $x$ below which a fraction $u$ of the distribution is contained.\n",
    "  3. The $x$ value you get is a random sample from $h(x)$\n",
    "\n",
    "\n",
    "Normalizarion here are rellly important.\n",
    "you can retrive the quantile and the CDF by numerically solution if you are not able to do in by hand.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/Example-of-inverse-transform-sampling.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b4cd6",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 5** - Frequentist Inference I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bbc14",
   "metadata": {},
   "source": [
    "- Population  \n",
    "- Sample  \n",
    "- Statistics  \n",
    "- Estimators  \n",
    "- Uncertainties and intervals  \n",
    "- Frequentist vs Bayesian  \n",
    "- Maximum Likelihood Estimator (MLE)  \n",
    "- Properties of estimators  \n",
    "- Likelihood  \n",
    "- Chi-squared  \n",
    "- Minimization  \n",
    "- Mean and error of MLE with heteroscedastic and homoscedastic errors  \n",
    "- Non-Gaussian likelihoods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b71a25",
   "metadata": {},
   "source": [
    "### **Population, Sample, Statistic, Estimators, Uncertainty and Intervals**\n",
    "\n",
    "- A **population** is the full set of data or measurements we are interested in.\n",
    "- A **sample** is a subset of the population, used to infer properties of the whole.\n",
    "- A **statistic** is a function of the sample (e.g. the sample mean $\\bar{x}$).\n",
    "- An **estimator** is a rule or formula to estimate population parameters from the sample.\n",
    "- All estimators have **uncertainties** due to random sampling.\n",
    "- A **confidence interval**, gives a range likely to contain the true value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Frequentist vs Bayesian**\n",
    "\n",
    "- **Frequentist**: Probability is extract from the frequency of events. Parameters are fixed, data are random.\n",
    "Into Frequentist inference we have confidence levels,.\n",
    "- **Bayesian**: Probability expresses belief or uncertainty about what we know. Parameters have distributions while data are fixed. In Bayesian inference we have credible regions derive from posterior distribution of the parameters.\n",
    "\n",
    "\n",
    "- Bayesian statistic it's hold by the **Bayes’ theorem**:\n",
    "  $$\n",
    "  P(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n",
    "  $$\n",
    "  where $\\theta$ are the parameters.\n",
    "    -  $P(\\theta | \\text{data})$ : Posterior probability (updated belief)  \n",
    "    -  $P(\\text{data} | \\theta)$ : Likelihood of observing data given the parameter  \n",
    "    -  $P(\\theta)$ : Prior probability  \n",
    "    -  $P(data)$ : Marginal probability \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72811421",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Maximum Likelihood Estimator (MLE)**\n",
    "\n",
    "- The **MLE** is the value of the parameter $\\theta$ that **maximizes the likelihood** of the observed data:\n",
    "  $$\n",
    "  \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_\\theta \\mathcal{L}(\\theta)\n",
    "  $$\n",
    "- It's usefull in both frequentist e bayesian approach\n",
    "- **Remember**: the **likelihood** is defined as the product of the probabilities (or probability densities) of the observed data, assuming a given model or parameter value.\n",
    "\n",
    "  For independent data points $x_1, x_2, ..., x_N$:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = \\prod_{i=1}^{N} p(x_i \\mid \\theta)\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $\\mathcal{L}(\\theta)$ is the likelihood function,\n",
    "  - $p(x_i \\mid \\theta)$ is the probability (or density) of observing $x_i$ given parameter $\\theta$,\n",
    "  - The product assumes all $x_i$ are independent.\n",
    "\n",
    "  Often, we work with the **log-likelihood**:\n",
    "  $$\n",
    "  \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta)\n",
    "  $$\n",
    "  which is easier to compute and optimize.\n",
    "\n",
    "**Remember how this works:**\n",
    "You have a likelihood function (or log-likelihood).\n",
    "The values $x_i$ are known (observed data), but the parameters $\\theta$ are unknown.\n",
    "You use an algorithm (e.g., grid search, gradient ascent, or scipy.optimize) to:\n",
    "Try many values of $\\theta$, then compute the likelihood (or log-likelihood) for each value, and plot the result over the values of theta. Find the one that maximizes it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60ed64",
   "metadata": {},
   "source": [
    "\n",
    "### **Properties of Estimators**\n",
    "\n",
    "- **Unbiasedness**: $\\mathbb{E}[\\hat{\\theta}] = \\theta$\n",
    "- **Consistency**: $\\hat{\\theta} \\to \\theta_{true}$ as $N \\to \\infty$\n",
    "- **Normality** : As $N \\to \\infty$ , the distribution of the estimator approaches a normal distribution.\n",
    "- **Efficiency**: Minimum possible variance (called Cramer -Rao bound)\n",
    "\n",
    "\n",
    "  The **Cramér-Rao Bound** provides a theoretical lower limit on the **variance** of any **unbiased estimator** of a parameter.\n",
    "\n",
    "  **Statement**\n",
    "\n",
    "  Let $ \\hat{\\theta} $ be an **unbiased estimator** of a parameter $ \\theta $. Then:\n",
    "\n",
    "  $$\n",
    "  \\mathrm{Var}(\\hat{\\theta}) \\geq \\frac{1}{\\mathcal{F}(\\theta)}\n",
    "  $$\n",
    "\n",
    "  where $ \\mathcal{F}(\\theta) $ is the **Fisher Information**, defined as:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{F}(\\theta) = \\mathbb{E} \\left[ \\left( \\frac{\\partial}{\\partial \\theta} \\log p(x; \\theta) \\right)^2 \\right]\n",
    "  $$\n",
    "\n",
    "  - Applies **only to unbiased estimators**.\n",
    "  - Describes the **best possible precision** achievable.\n",
    "  - If an estimator **achieves** this bound, it is called **efficient**.\n",
    "\n",
    "  > You can't estimate a parameter with a smaller variance than the Cramér-Rao Bound—unless you allow for bias.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Likelihood, Chi-squared and Minimization**\n",
    "\n",
    "- The **likelihood** $\\mathcal{L}(\\theta)$ is the probability of the data given parameters. $P(data | \\theta)$\n",
    "- Suppose you have data points $(x_i, y_i)$ with known uncertainties $\\sigma_i$, and a model $f(x_i; \\theta)$ depending on parameters $\\theta$.\n",
    "      Each measurement $y_i$ is modeled as:\n",
    "    $$\n",
    "    y_i = f(x_i; \\theta)\n",
    "    $$\n",
    "\n",
    "    So the probability density of observing $y_i$ given $\\theta$ is:\n",
    "    $$\n",
    "    p(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\, \\exp\\left( -\\frac{(y_i - f(x_i;\\theta))^2}{2\\sigma_i^2} \\right)\n",
    "    $$\n",
    "\n",
    "    the total likelihood is the product over all data points:\n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = \\prod_{i=1}^N p(y_i \\mid \\theta)\n",
    "  $$\n",
    "\n",
    "  Taking the log:\n",
    "  $$\n",
    "  \\log \\mathcal{L}(\\theta) = -\\frac{1}{2} \\sum_{i=1}^N \\left[ \\log(2\\pi \\sigma_i^2) + \\frac{(y_i - f(x_i;\\theta))^2}{\\sigma_i^2} \\right]\n",
    "  $$\n",
    "\n",
    "  The first term is constant if $\\sigma_i$ are known.  \n",
    "  So **maximizing the log-likelihood** is equivalent to **minimizing** the second term\n",
    "\n",
    "- the Likelihood will follow the $\\exp(-\\chi^2/2)$\n",
    "- In Gaussian cases, maximizing the log-likelihood is equivalent to **minimizing the chi-squared**:\n",
    "  $$\n",
    "  \\chi^2 = \\sum_{i=1}^N \\left( \\frac{y_i - f(x_i; \\theta)}{\\sigma_i} \\right)^2\n",
    "  $$\n",
    "- Minimizing $\\chi^2$ gives the best-fit parameters.\n",
    "- The MLE method tell us to think the likelihood as a function of the (unknown) model parameters, and by minimizing the $\\chi^2$, we will find the values that maximize the values of the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean and MLE Error: Homoscedastic vs Heteroscedastic**\n",
    "\n",
    "- **Homoscedastic**: All data points have the same uncertainty $\\sigma$. If we minimize the $\\chi^2$ distribution, we will retrived the **sample mean**:\n",
    "    $$\n",
    "    \\bar{x} = \\frac{1}{N} \\sum x_i \\quad \\text{and} \\quad \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{N}}\n",
    "    $$\n",
    "- **Heteroscedastic**: Uncertainties vary for each data point $\\sigma_i$. Then you will retrive the **weighted mean**:\n",
    "    $$\n",
    "    \\bar{x} = \\frac{\\sum x_i / \\sigma_i^2}{\\sum 1 / \\sigma_i^2}\n",
    "    $$\n",
    "    $$\n",
    "    \\sigma_{\\bar{x}}^2 = \\frac{1}{\\sum 1/\\sigma_i^2}\n",
    "    $$\n",
    "This two formula are extracted from the derivative of the log-Likelihood = 0 , that' because we are searching for a maximum.\n",
    "\n",
    "Our Maximum Likelihood Estimator (MLE) is not perfect — every estimate has an associated **uncertainty** due to the finite sample size.\n",
    "\n",
    "Under general conditions, the MLE becomes **asymptotically normal**, meaning that for large $N$, the likelihood function can be approximated by a **Gaussian** centered at the true parameter value $\\theta_0$.\n",
    "\n",
    "To quantify the uncertainty, we expand the **log-likelihood** around its maximum using a second-order **Taylor expansion**:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta) \\approx \\log \\mathcal{L}(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^2 F(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "Here, $F(\\hat{\\theta})$ is the **Fisher Information Matrix**, defined as the negative second derivative (Hessian) of the log-likelihood:\n",
    "\n",
    "$$\n",
    "F(\\theta) = - \\frac{\\partial^2}{\\partial \\theta^2 } \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "The **covariance matrix** of the estimator $\\hat{\\theta}$ is then given by the **inverse** of the Fisher matrix (the Fisher Information tells you how \"sharp\" (peaked) the likelihood is around its maximum.\n",
    "The inverse of that sharpness gives the spread (uncertainty) — which is exactly the variance (or covariance, in multiple dimensions) of your estimator.):\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\hat{\\theta}) = F^{-1}(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "For a **single parameter** $\\theta$, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\hat{\\theta}} = \\sqrt{\\frac{1}{F(\\hat{\\theta})}}\n",
    "$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/LOGL.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Gaussian Likelihoods**\n",
    "\n",
    "- When the data doesn’t follow a Gaussian distribution, use the appropriate **likelihood model**, such as : **Poisson**, **Binomial**, **Exponential**, **Log-normal**, etc.\n",
    "- The MLE approach still applies: choose the model, write the likelihood, and maximize it numerically.\n",
    "- In most of the cases, you will find the same result as in the gaussian one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14af5",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 6** - Frequentist Inference II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a02e5",
   "metadata": {},
   "source": [
    "- Fit  \n",
    "- Outliers (Huber loss function)  \n",
    "- Goodness of fit  \n",
    "- Reduced chi-squared  \n",
    "- Model misspecification  \n",
    "- Occam’s Razor  \n",
    "- AIC (Akaike Information Criterion)  \n",
    "- Bootstrap  \n",
    "- Jackknife"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2ed34",
   "metadata": {},
   "source": [
    "### **Fit**\n",
    "\n",
    "- Fitting means adjusting model parameters so that the model best matches the observed data.\n",
    "- Typically done by minimizing a **loss function**, such as the **sum of squared residuals** or **negative log-likelihood**.\n",
    "- The goal is to find the best estimate $\\hat{\\theta}$ that explains the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outliers and Huber Loss Function**\n",
    "\n",
    "- **Outliers** are data points that deviate significantly from the trend of the rest of the data.\n",
    "- Summing the squares of the residuals ($\\chi^2=\\sum_{i=1}^N (y_i - M(x_i))^2/\\sigma^2$) is sensitive to outliers\n",
    "- How do we deal with outliers? By modifying the likelihood!\n",
    "- The **Huber loss** combines the squared loss for small errors and absolute loss for large errors:\n",
    "\n",
    "$$\n",
    "L_{\\text{Huber}}(t) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} t^2 & \\text{if } |t| \\leq c \\\\\n",
    "c |t| - \\frac{1}{2} c^2 & \\text{if } |t| > c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Where $t = \\left| \\frac{y - M(\\theta)}{\\sigma} \\right|$ represents the **standardized residual**, i.e. how far the observed value $y$ is from the model prediction $M(\\theta)$, in units of the known uncertainty $\\sigma$.\n",
    "- $c$ is the **tuning constant** (or confidence threshold), which determines the cutoff point where the loss switches from quadratic to linear. A common value is $c \\approx 1.345$, which gives good balance between efficiency and robustness under normal errors.\n",
    "- This approach makes the fit more **robust** to outliers: small residuals behave like in least squares, but large residuals are penalized less harshly.\n",
    "- Note that by doing this, we are effectively putting **prior information** into the analysis... infact, in a frequentist approach we prefear to re-do the measurments ore simply delete the few outliars.\n",
    "\n",
    "![title](https://upload.wikimedia.org/wikipedia/commons/c/cc/Huber_loss.svg)\n",
    "\n",
    "---\n",
    "\n",
    "### **Goodness of Fit : Reduced Chi-squared**\n",
    "\n",
    "- Measures how well the model describes the data. Remember GIGO (Garbage In Garbage Out), if the model is wrong , finding the \"best\" parameter doesn't really mean something ...\n",
    "- A good fit should show residuals randomly scattered around zero.\n",
    "\n",
    "- The **reduced chi-squared** is defined as:\n",
    "\n",
    "  $$\n",
    "  \\chi^2_{\\text{red}} = \\frac{1}{\\nu} \\sum_{i=1}^N \\left( \\frac{y_i - f(x_i)}{\\sigma_i} \\right)^2\n",
    "  $$\n",
    "\n",
    "  where **$\\nu = N - k$** is the number of **degrees of freedom** (data points minus number of parameters).\n",
    "\n",
    "- Interpretation:\n",
    "  - $\\chi^2_{\\text{red}} \\approx 1$: good fit\n",
    "  - $\\chi^2_{\\text{red}} \\gg 1$: underfitting or underestimated errors\n",
    "  - $\\chi^2_{\\text{red}} \\ll 1$: overfitting or overestimated errors\n",
    "- If the model is **wrong** (misspecified), goodness-of-fit measures can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Comparison, Occam’s Razor , AIC and BIC**\n",
    "- You can't do $\\chi^2_{\\text{red}}$ with Huber function, because it's not gaussian! We have to find other possibilities...\n",
    "- When comparing two models with the **same number of parameters**, we can simply compare their **maximum log-likelihood** values:\n",
    "- Larger log-likelihood ⇒ better fit\n",
    "\n",
    "The **Huber loss** clearly performs better (less negative log-likelihood), meaning it fits the data more effectively, especially in the presence of outliers.\n",
    "\n",
    "\n",
    "When models have **different numbers of parameters**, simply comparing likelihoods is not fair: more complex models might fit better **just by chance**. We need to penalize complexity — this is known as the **Occam penalty**.\n",
    "\n",
    "\n",
    "A simple method to compare models with different complexity is the **AIC** (Akaike Information Criterion):\n",
    "  \n",
    "- Lower AIC is better for the explaination of the dataset\n",
    "- It's composed by lot of term, the first one it's the $\\chi^2$, the second and third penalize model complexity\n",
    "- If models fit the data equally well, AIC prefers the one with fewer parameters.\n",
    "\n",
    "\n",
    "\n",
    "The **BIC** (Bayesian Information Criterion) is another way to compare models, especially when they have different numbers of parameters.\n",
    "It’s similar to AIC, but it **penalizes complex models more strongly**, especially when the dataset is large.\n",
    "\n",
    "\n",
    "- Lower **BIC** means a better model.\n",
    "- BIC prefers **simpler models**, especially when N is large.\n",
    "- It’s often used in **Bayesian statistics**, but doesn’t need a full Bayesian analysis, often use in frequentist analysis too.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Bootstrap**\n",
    "\n",
    "- A **resampling method** to estimate uncertainties and confidence intervals.\n",
    "- Keep attention : it create information out of nothing!\n",
    "- Steps:\n",
    "  1. Resample data (with replacement) to create many datasets. \n",
    "    The probability of getting the original dataset it's extreamly low ($N! / N^N$)\n",
    "  2. Fit the model to each resampled dataset.\n",
    "  3. Analyze the distribution of the fitted parameters.\n",
    "- Useful when analytical uncertainty is hard to compute or it's too big (such as when we have few point for a gaussian distribution).\n",
    "\n",
    "#### Bootstrap Resampling Example\n",
    "\n",
    "Suppose you have an original dataset of size $N=5$:\n",
    "\n",
    "Original data:   $[10,\\, 20,\\, 30,\\, 40,\\, 50]$\n",
    "\n",
    "**Bootstrap Sample 1**: $[20,\\, 50,\\, 10,\\, 50,\\, 30]$\n",
    "\n",
    "**Bootstrap Sample 2**: $[10,\\, 10,\\, 40,\\, 20,\\, 20]$ and so on ...\n",
    "\n",
    "Each bootstrap sample:\n",
    "\n",
    "- Has the **same size** as the original dataset ($N=5$),\n",
    "- Includes data points sampled **randomly with replacement**, so some points can repeat and some may be left out.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Jackknife Method** \n",
    "\n",
    "The **Jackknife** is a method to estimate the **uncertainty** (standard error) and **bias** of a statistic — like the **mean** or **standard deviation** — using your data.\n",
    "\n",
    "Suppose you have a dataset of $N$ values.\n",
    "\n",
    "1. Leave out **one** data point at a time → you get $N$ new datasets containing (N-1) points.\n",
    "2. Compute your statistic (e.g. mean, std) on each of these.\n",
    "3. From the $N$ results, estimate:\n",
    "   - A **better (bias-corrected)** value of the statistic\n",
    "   - The **uncertainty** on that value\n",
    "\n",
    "\n",
    "Jackknife works **well** when the statistic is:\n",
    "- The **mean**\n",
    "- The **standard deviation**\n",
    "\n",
    "It works **poorly** for:\n",
    "- The **median**\n",
    "- **Quantiles** (e.g. the 25th percentile)\n",
    "\n",
    "These are called **rank-based statistics**, and removing one point at a time doesn’t change them much — so the jackknife underestimates the uncertainty.\n",
    "\n",
    "#### Jackknife Resampling Example\n",
    "\n",
    "Suppose you have an original dataset of size $N=5$:\n",
    "\n",
    "Original data:   $[10,\\, 20,\\, 30,\\, 40,\\, 50]$\n",
    "\n",
    "**Jackknife Sample 1**: $[20,\\, 30,\\, 40,\\, 50]$\n",
    "\n",
    "**Jackknife Sample 2**: $[10,\\, 30,\\, 40,\\, 50]$  and so on ...\n",
    "\n",
    "\n",
    "Each jackknife sample:\n",
    "\n",
    "- Has size **$N-1 = 4$**,\n",
    "- Is formed by **leaving out exactly one data point** from the original dataset,\n",
    "- Is used to estimate variability by systematically omitting each observation once.\n",
    "\n",
    "####  **Jackknife vs Bootstrap**\n",
    "\n",
    "|                | Jackknife                | Bootstrap                |\n",
    "|----------------|--------------------------|--------------------------|\n",
    "| Type           | Leaves out one point     | Resamples with replacement |\n",
    "| Fast?          | ✅ Yes                   | ❌ Slower               |\n",
    "| Repeatable?    | ✅ Always same result     | ❌ Changes each time    |\n",
    "| Works for all stats? | ❌ Not for medians       | ✅ Yes                 |\n",
    "| Confidence intervals | ❌ Approximate         | ✅ Full distribution     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a611a0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 7** - Frequentist Inference III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f4b80",
   "metadata": {},
   "source": [
    "- Hypothesis testing (p-value)  \n",
    "- Null hypothesis  \n",
    "- Type I and Type II errors  \n",
    "- KS test (Kolmogorov–Smirnov)  \n",
    "- Histograms  \n",
    "- Number of bins (Scott’s & Freedman–Diaconis rules)  \n",
    "- Rug plot  \n",
    "- Kernel Density Estimation (Gaussian and Epanechnikov)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0c6fe",
   "metadata": {},
   "source": [
    "### **Hypothesis Testing and p-value**\n",
    "\n",
    "Hypothesis testing is a fundamental procedure in statistics used to decide whether there is enough evidence in a sample of data to infer that a certain condition holds for the entire population.\n",
    "\n",
    "- **Null Hypothesis ($H_0$):** This is the starting assumption or the default claim about the population. It usually represents the idea that there is **no effect**, **no difference**, or **no relationship** between variables. For example, $H_0$ might state that the mean of a population is equal to a specific value.\n",
    "\n",
    "- **Alternative Hypothesis ($H_1$):** This is the hypothesis you want to test or provide evidence for. It represents a change, effect, or difference from what the null hypothesis states. \n",
    "\n",
    "- **Test Statistic:** To test the hypotheses, a test statistic is computed from the sample data. This statistic measures how far the observed data are from what would be expected if $H_0$ were true. Different tests have different statistics (e.g., t-test, z-test, chi-square test).\n",
    "\n",
    "- **p-value:** The p-value is the probability, assuming the null hypothesis $H_0$ is true, of obtaining a test statistic at least as extreme as the one observed. In other words, it quantifies how likely your data would be if there were actually no effect.\n",
    "\n",
    "  - A **small p-value** indicates that the observed data is unlikely under $H_0$, so we have evidence to reject the null hypothesis.\n",
    "  - A **large p-value** suggests the data is consistent with $H_0$, and we do not reject it.\n",
    "\n",
    "$$\n",
    "p_i = \\int_{x_i}^{\\infty} h_0(x)dx = 1 - \\int_{-\\infty}^{x_i}h_0(x)dx = 1- H_0(x_i)\n",
    "$$\n",
    "\n",
    "- **Significance Level ($\\alpha$):** This is a threshold probability set before the test (commonly 0.05 or 5%). If the p-value is less than $\\alpha$, the result is called statistically significant, and we reject the null hypothesis in favor of the alternative.\n",
    "\n",
    "\n",
    "#### Important notes:\n",
    "\n",
    "- **Failing to reject $H_0$ is not the same as accepting $H_0$.** It means the data do not provide strong enough evidence against $H_0$, but $H_0$ might still be false.\n",
    "- The p-value does **not** measure the probability that $H_0$ is true or false; it only measures data compatibility with $H_0$.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Suppose you want to test if a coin is fair.  \n",
    "- $H_0$: The coin is fair (probability of heads = 0.5).  \n",
    "- $H_1$: The coin is biased (probability of heads ≠ 0.5).\n",
    "\n",
    "You flip the coin 100 times, get 60 heads, and compute a test statistic. The p-value tells you how likely it is to get 60 or more heads assuming the coin is fair. If the p-value is below your threshold (e.g., 0.05), you reject $H_0$ and conclude the coin is likely biased.\n",
    "\n",
    "--- \n",
    "\n",
    "### **Facts about p-value**\n",
    "1) Not the chance the hypothesis is true:\n",
    "A p-value does not tell you the probability that the null hypothesis is true. Instead, it tells you how likely your results are if the null hypothesis were true.\n",
    "\n",
    "2) Not the chance it's \"just random\":\n",
    "A p-value is not the probability that your results happened by chance alone. It's based on the assumption that the null hypothesis is correct and measures how well your data fit that assumption.\n",
    "\n",
    "3) The 0.05 rule is just a guideline:\n",
    "The 0.05 cutoff for “significance” is a tradition, not a scientific rule. A result just below or above 0.05 should not be seen as automatically meaningful or meaningless.\n",
    "\n",
    "4) Doesn’t tell how big or important an effect is:\n",
    "A small p-value doesn’t mean the effect is big or important. Even tiny effects can be “significant” if the sample is large enough — that’s why we also need to consider effect size.\n",
    "---\n",
    "\n",
    "### **Type I and Type II Errors**\n",
    "\n",
    "**TYPE I ERRORS (false positives, or false alarms)**\n",
    "\n",
    "- The null hypothesis is true, but incorrectly rejected.\n",
    "- False positive probability is dictated by the significance level $\\alpha$. \n",
    "\n",
    "aka *That pixel was just background but I think it's a real source.*\n",
    "\n",
    "**TYPE II ERRORS (false negatives, or false dismissals)**\n",
    "\n",
    "- The null hypothesis is false, but not rejected.\n",
    "- False negatives probability is dictated by a variable called $\\beta$, related to $(1-\\beta)$, called the ***detection probability***.\n",
    "\n",
    "aka *That was a real galaxy but I missed it!*\n",
    "\n",
    "**NOTE:** the value of $ \\alpha $ it's the area of the null hypotesis greater then the treshold value (see immage below),  meaning that the red area = $ \\alpha = \\int_{x_c}^{\\infty} h_0(x)dx $ \n",
    "\n",
    "For a sample of size $N$ (containing background noise and sources), the **expected number of spurious sources (Type I / false positives)** is \n",
    "\n",
    "$$ \n",
    "n_\\mathrm{spurious} = N \\alpha = N \\int_{x_c}^{\\infty} h_0(x)dx\n",
    "$$ \n",
    "\n",
    "and the **expected number of missed sources (Type II / false negatives)** is\n",
    "\n",
    "$$ \n",
    "n_\\mathrm{missed} = N\\beta = N\\int_0^{x_c}h_1(x)dx.\n",
    "$$\n",
    "\n",
    "The **total number of classified sources** (that is number of instances where we reject the null hypothesis) is\n",
    "\n",
    "$$\n",
    "n_\\mathrm{source} = N - n_\\mathrm{missed} + n_\\mathrm{spurious} = N (1 + \\alpha - \\beta) \n",
    "$$\n",
    "\n",
    "The **sample completeness** (or **detection probability**) is defined as\n",
    "\n",
    "$$ \\eta = \\frac{N - n_\\mathrm{missed}}{N} = 1-\\int_0^{x_c}h_1(x)dx = 1-\\beta$$\n",
    "\n",
    "Finally, the **sample contamination** is\n",
    "\n",
    "$$ \\epsilon = \\frac{n_\\mathrm{spurious}}{n_\\mathrm{source}}$$\n",
    "\n",
    "where $(1-\\epsilon)$ is sometimes called the **classification efficiency**.\n",
    "\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/type_err.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342dc8d2",
   "metadata": {},
   "source": [
    "### **Kolmogorov–Smirnov (KS) Test**\n",
    "\n",
    "- we'd like to compare two different sample and understand if they were taken from the same distribution\n",
    "- KS is a non-parametric test to compare a sample with a reference distribution, or two samples.\n",
    "- Measures the maximum distance between the empirical CDF $\\rightarrow D = max|F_1 - F_2|$\n",
    "- Outputs a statistic $D$ and a p-value.\n",
    "- Useful to test goodness-of-fit.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/KS.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### **Histograms and Number of Bins**\n",
    "\n",
    "Choosing the number of bins affects the histogram shape. The bin's width it's a hyper-parameter that has to be tune for correctly extracting the true statistics:\n",
    "\n",
    "- **Scott’s Rule:**  \n",
    "$$\n",
    "\\text{bin width} = \\Delta_b =\\frac{3.5 \\times \\sigma}{N^{1/3}}\n",
    "$$\n",
    "That's a grat rule only if we know sigma of the distribution... often it's not usable\n",
    "\n",
    "- **Freedman-Diaconis Rule:**  \n",
    "$$\n",
    "\\text{bin width} = \\Delta_b =  \\frac{2 \\times IQR}{N^{1/3}} = \\frac{2.7 \\times \\sigma_G}{N^{1/3}}\n",
    "$$\n",
    "where $IQR$ = interquartile range between 75% and 25% and $N$ = number of data points.\n",
    "\n",
    "- By making histogram you are losing some information depending on the width of the bin you are choosing; It's possible to define the bin height uncertainty by a simple rule: \n",
    "$$\n",
    "  \\sigma_k = \\frac{\\sqrt{n_k}}{\\Delta_b \\cdot N}\n",
    "$$\n",
    "where: N is the total number of data, $n_k$ it's the numer of count in the k-bin and $\\Delta$ is the bin width\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Rug Plot**\n",
    "\n",
    "- A simple plot showing individual data points as small vertical lines (ticks) along an axis.\n",
    "- Useful to visualize the distribution of data points on top of other plots (like histograms or density plots).\n",
    "\n",
    "---\n",
    "\n",
    "### non parametric **Kernel Density Estimation (KDE)**\n",
    "\n",
    "- The core idea it's not to usa a Dirac - delta in each point, but rather a distribution.\n",
    "- All this distribution (kernel) are summed up to produce the PDF.\n",
    "- Any distribution could be use:\n",
    "\n",
    "#### Common kernels:\n",
    "\n",
    "- **Gaussian kernel:**  \n",
    "$$\n",
    "K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n",
    "$$\n",
    "\n",
    "- **Epanechnikov kernel:**  \n",
    "  $$\n",
    "  K(x) = \\frac{3}{4} (1 - x^2) \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$\n",
    "  parabolic with a fix support, more localized then the gaussian\n",
    "\n",
    "- **Linear (triangular) kernel:**  \n",
    "  $$\n",
    "  K(x) = 1 - |x| \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$  \n",
    "  Linearly decreasing weights as distance from the reference point increases. **Less smooth** than Gaussian.\n",
    "\n",
    "\n",
    "- **Uniform (or tophat) kernel:**  \n",
    "  $$\n",
    "  K(x) = \\frac{1}{2} \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$  \n",
    "  Assigns equal weight to all points in a fixed window. Simple but **can produce less accurate estimates** due to lack of smoothness.\n",
    "\n",
    "\n",
    "KDE bandwidth controls the smoothness (similar to bin width for histograms). That's an hyper parameter that has to be tune fine before the analysis thanks to Cross Validation. \n",
    "\n",
    "---\n",
    "\n",
    "### non parametric **Nearest-Neighbor Density Estimation** (lez 17)\n",
    "\n",
    "- Estimates density based on the volume (or hypervolume) $V_k$ containing the $k$ nearest neighbors of $x$.\n",
    "- Formula:   $ \\rho = \\frac{k}{n \\cdot V_k}$\n",
    "- $\\rho$ is the density value, k is the number of pointin the selected volume, $V_k$ is the spherical hypervolume calculated thans to $\\frac{2d^D\\pi^{D/2}}{D\\Gamma(D/2)}$\n",
    "- Adapts well to **local variations** in data density.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8008fa4",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 8** - Bayesian Inference I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef596f",
   "metadata": {},
   "source": [
    "- Bayes recap  \n",
    "- Bayesian method  \n",
    "- Prior  \n",
    "- 3 Bayesian principles  \n",
    "- Credibility regions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805e709",
   "metadata": {},
   "source": [
    "\n",
    "### **Bayes Recap – Principles and Rules**\n",
    "\n",
    "Bayes' theorem allows us to **update our belief** about a hypothesis or a parameter after observing new data. The core formula is:\n",
    "\n",
    "$$P(\\theta| D) = \\dfrac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$$\n",
    "\n",
    "Where:\n",
    "- $\\theta$:  parameters values\n",
    "- $D$: observed data\n",
    "- $P(\\theta)$: **prior** – initial belief before seeing the data\n",
    "- $P(D | \\theta)$: **likelihood** – probability of observing $D$ assuming $\\theta$ is true\n",
    "- $P(\\theta | D)$: **posterior** – updated belief after seeing the data\n",
    "- $P(D)$: normalization constant \n",
    "\n",
    "\n",
    "$$\n",
    "  \\text{Posterior probability} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n",
    "$$\n",
    "\n",
    "**IMPORTANT NOTE:** We don't often care about **the evidence** in simply application, because it does not depend on model parameters. We usually set it to $1$ for parameter estimation. **BUT** the evidence is crucial for Bayesian model selection because it quantifies how well the data are explained by the model as a whole. It acts as a normalization factor that allows us to fairly compare different models by providing their marginal likelihood, enabling ranking of models based on their overall plausibility given the data. (See lecture 9 for a better explaination)\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayesian Method** \n",
    "\n",
    "1. **Define the problem** – Choose the model and the parameter $\\theta$ to estimate.\n",
    "2. **Assign the prior** $P(\\theta)$ – Express your knowledge or assumptions about $\\theta$ before the data.\n",
    "3. **Define the likelihood** $P(D | \\theta)$ – Describe how the data is generated from $\\theta$.\n",
    "4. **Compute the posterior** $P(\\theta | D)$ – Using Bayes’ theorem.\n",
    "5. **Estimate the parameter** – Use the posterior to get a point estimate (e.g. MAP, mean, median).\n",
    "6. **Quantify uncertainty** – Through credibility intervals, variance, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **Prior Distributions** \n",
    "\n",
    "In Bayesian statistics, the prior distribution represents what we believe about a parameter before observing any data. Choosing a prior is a crucial step because it influences the final result (the posterior).\n",
    "\n",
    "There are two main types of priors:\n",
    "\n",
    "1. **Informative Prior**\n",
    "\n",
    "    - A prior that incorporates specific, pre-existing knowledge or beliefs about the parameter.\n",
    "\n",
    "    - When to use it: When you have reliable background information, from previous experiments, expert opinion, or strong theoretical expectations.\n",
    "\n",
    "Example:\n",
    "Suppose you’re estimating the probability that a coin is biased towards heads.\n",
    "If previous tests suggest it lands heads ~70% of the time, you could use a Beta(7, 3) prior (centered around 0.7), which reflects your prior belief.\n",
    "\n",
    "2. **Uninformative (or non-informative) Prior**\n",
    "\n",
    "    - A prior that is intentionally vague or flat, expressing no strong belief about the parameter before seeing the data.\n",
    "\n",
    "    - When to use it: When you want the data to speak for itself and avoid influencing the result with prior assumptions.\n",
    "\n",
    "Example:\n",
    "For the same coin-flip scenario, if you have no idea about the bias, you might use a uniform prior over [0, 1], this assumes all probabilities are equally likely.\n",
    "\n",
    "\n",
    "\n",
    "When no strong prior information is available, there are three main principles to guide the choice:\n",
    "\n",
    "- **Principle of indifference**: Assign equal probabilities when there is no reason to prefer one value over another. \n",
    "- **Invariance principle**: The prior should remain consistent under reparameterization.\n",
    "- **Maximum entropy**: Among all distributions compatible with known constraints, choose the one with the highest entropy (least informative).\n",
    "\n",
    "If you choose the wrong prior, it's your fault (GIGO), only an anormus amount of data could correct your initial belif, in this case you can say that are **\"data dominated\"**, otherwise you are **\"prior dominated\"**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayesian Credible Regions**\n",
    "\n",
    "In the **frequentist paradigm**, the meaning of the confidence interval $\\mu_0 \\pm \\sigma_{\\mu}$ is \n",
    "the interval that would contain the true $\\mu$ (from which the data were drawn) in $68\\%$ (or X\\%) cases\n",
    "of a large number of imaginary repeated experiments (each with a different N values of $\\{x_i\\}$). \n",
    "\n",
    "However, the meaning of the so-called **Bayesian credible region** is fundamentally different: it is the interval that **I believe** contains the true $\\mu$  with a probability of $68%$ (or $X\\%$) after I've collected my data (my dear, one and only dataset; no imaginary repetitions). This credible region is the \n",
    "relevant quantity in the context of scientific measurements. \n",
    "\n",
    "There are several important features of a Bayesian posterior distribution:\n",
    "- They represent how well we believe a parameter is constrained within a certain range\n",
    "- We often quote the posterior maximum (**Maximum A Posteriori (MAP)**).\n",
    "- We also often quote the posterior expectation value (i.e. mean) $\\bar{\\theta} = \\int \\theta\\, p(\\theta|D)d\\theta$, or most often median (recall: robust estimator).\n",
    "- **The credible regions are not unique**. We can compute them in two different ways\n",
    "    1. We can integrate downwards from the MAP to enclose $X\\%$ (\"highest probability density interval\"), or\n",
    "    2. We can integrate inwards from each tail by $X/2\\%$ (\"equal-tailed interval\")\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/bayes_credible_region.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714fef0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 9** - Bayesian Inference II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d661e",
   "metadata": {},
   "source": [
    "- Odds ratios  \n",
    "- Bayes factors  \n",
    "- Frequentist vs Bayesian  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8038b3",
   "metadata": {},
   "source": [
    "The following to produces the log-posterior pdf of parameters $\\mu$ and $\\sigma$ of an underlying Gaussian distribution. The dataset has $N=10$ values, drawn from $\\mu=1$, $\\sigma=1$, each measured with an uncertainty drawn from a uniform distribution $0<e_i<3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e60268b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bossi_ricky/WSL_Shared/myenv/lib/python3.12/site-packages/astroML/linear_model/linear_regression_errors.py:10: UserWarning: LinearRegressionwithErrors requires PyMC3 to be installed\n",
      "  warnings.warn('LinearRegressionwithErrors requires PyMC3 to be installed')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mastroML\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mplotting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmcmc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m convert_to_stdev  \u001b[38;5;66;03m# Converts log-likelihood to contour levels (1σ, 2σ, 3σ)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mnp\u001b[49m.random.seed()  \u001b[38;5;66;03m# Set random seed for reproducibility\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m#------------------------------------------------------------\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Define the log-likelihood function for the gauss-gauss model\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgaussgauss_logL\u001b[39m(xi, ei, mu, sigma):\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from astroML.plotting.mcmc import convert_to_stdev  # Converts log-likelihood to contour levels (1σ, 2σ, 3σ)\n",
    "\n",
    "np.random.seed()  # Set random seed for reproducibility\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Define the log-likelihood function for the gauss-gauss model\n",
    "def gaussgauss_logL(xi, ei, mu, sigma):\n",
    "    \"\"\"\n",
    "    Log-likelihood for a model where:\n",
    "    - xi are observed data points\n",
    "    - ei are uncertainties in the data\n",
    "    - mu, sigma are the model parameters (mean and std of underlying distribution)\n",
    "    \n",
    "    Assumes xi ~ Normal(mu, sqrt(sigma^2 + ei^2))\n",
    "    \"\"\"\n",
    "    # Prepare shapes for broadcasting: make mu and sigma broadcastable with xi and ei\n",
    "    ndim = len(np.broadcast(sigma, mu).shape)\n",
    "\n",
    "    # Reshape xi and ei to broadcast properly over mu and sigma grids\n",
    "    xi = xi.reshape(xi.shape + tuple(ndim * [1]))\n",
    "    ei = ei.reshape(ei.shape + tuple(ndim * [1]))\n",
    "\n",
    "    # Total variance = model variance + measurement variance\n",
    "    s2_e2 = sigma ** 2 + ei ** 2 # eteroschedastic\n",
    "\n",
    "    # Return the total log-likelihood (log product = sum of logs)\n",
    "    return -0.5 * np.sum(np.log(s2_e2) + (xi - mu) ** 2 / s2_e2, 0)\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Simulate a dataset\n",
    "mu_true = 1.0       # True mean\n",
    "sigma_true = 1.0    # True standard deviation\n",
    "N = 100              # Number of data points\n",
    "\n",
    "# Measurement uncertainties ei ~ Uniform(0, 3)\n",
    "ei = 3 * np.random.random(N)\n",
    "\n",
    "# Generate observed data xi from N(mu_true, sqrt(sigma_true^2 + ei^2))\n",
    "xi = np.random.normal(mu_true, np.sqrt(sigma_true ** 2 + ei ** 2))\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Create a grid of mu and sigma values to evaluate log-likelihood\n",
    "sigma = np.linspace(0.01, 5, 70)           # 70 values of sigma from 0.01 to 5\n",
    "mu = np.linspace(-3, 5, 70)                # 70 values of mu from -3 to 5\n",
    "\n",
    "# Evaluate log-likelihood on the mu-sigma grid\n",
    "logL = gaussgauss_logL(xi, ei, mu, sigma[:, np.newaxis])\n",
    "\n",
    "# Normalize log-likelihood: subtract max for numerical stability\n",
    "logL -= logL.max()\n",
    "\n",
    "#------------------------------------------------------------\n",
    "# Plot the log-likelihood heatmap and contours\n",
    "fig = plt.figure(figsize=(10, 7.5))\n",
    "\n",
    "# Show log-likelihood as image\n",
    "plt.imshow(logL, origin='lower',\n",
    "           extent=(mu[0], mu[-1], sigma[0], sigma[-1]),\n",
    "           cmap=plt.cm.binary,\n",
    "           aspect='auto')\n",
    "\n",
    "# Add colorbar\n",
    "plt.colorbar().set_label(r'$\\log(L)$')\n",
    "plt.clim(-5, 0)  # Clamp color scale for better contrast\n",
    "\n",
    "# Add a text box explaining the setup\n",
    "plt.text(0.5, 0.93,\n",
    "         (r'$L(\\mu,\\sigma)\\ \\mathrm{for}\\ \\mu_{\\rm true}=1,\\ '\n",
    "          r'\\sigma_{\\rm true}=1,\\ n=10$'),\n",
    "         bbox=dict(ec='k', fc='w', alpha=0.9),\n",
    "         ha='center', va='center', transform=plt.gca().transAxes)\n",
    "\n",
    "# Plot confidence contours (1σ, 2σ, 3σ)\n",
    "plt.contour(mu, sigma, convert_to_stdev(logL),\n",
    "            levels=(0.683, 0.955, 0.997),  # Corresponds to 1σ, 2σ, 3σ\n",
    "            colors='k')\n",
    "            \n",
    "# Draw dotted lines at true values of mu and sigma\n",
    "plt.axhline(mu_true, c='red', ls='dotted')\n",
    "plt.axvline(sigma_true, c='red', ls='dotted')\n",
    "\n",
    "# Labels\n",
    "plt.xlabel(r'$\\mu$')\n",
    "plt.ylabel(r'$\\sigma$')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35104226",
   "metadata": {},
   "source": [
    "**Key takeaways**\n",
    "- By eye,  $\\mu=1$, $\\sigma=1$ are not too far from the MAP parameter values. This is ok, but...\n",
    "- The posterior pdf is not symmetric around $\\mu=1$.\n",
    "- In fact it is consistent within the $99.7\\%$ credible region of having $\\sigma=0$!. \n",
    "- The marginal distributions of each parameter would not look Gaussian either."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d81f0d4",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c917c07",
   "metadata": {},
   "source": [
    "### **Bayesian Model Comparison vs. Frequentist Hypothesis Testing**\n",
    "\n",
    "In classical (frequentist) statistics, we test hypotheses using tools like **p-values**, which tell us how extreme the data would be *if the null hypothesis were true*. But:\n",
    "\n",
    "- **In Bayesian inference, we don't use p-values.**  \n",
    "- Instead, we **compare models probabilistically** based on how well they explain the observed data, considering our prior beliefs.\n",
    "\n",
    "---\n",
    "\n",
    "### **Posterior Probability of a Parameter**  \n",
    "\n",
    "Given: (particle approach)\n",
    "- Data $D$ (e.g., experimental results from LHC)  \n",
    "- Model $M$ (e.g., the Standard Model, SM)  \n",
    "- Parameter $m_H$ (e.g., a possible value for the Higgs boson mass)\n",
    "\n",
    "The **posterior probability distribution** for the parameter $m_H$ given the data and the model is:\n",
    "\n",
    "$$\n",
    "p(m_H \\mid D, \\text{SM}) = \\frac{p(D \\mid m_H, \\text{SM}) \\cdot p(m_H \\mid \\text{SM})}{p(D \\mid \\text{SM})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p(m_H \\mid \\text{SM})$ is the **prior** on the Higgs mass (within SM)  \n",
    "- $p(D \\mid m_H, \\text{SM})$ is the **likelihood**  \n",
    "- $p(D \\mid \\text{SM})$ is the **evidence** (a.k.a. marginal likelihood)  \n",
    "- $p(m_H \\mid D, \\text{SM})$ is the **posterior** distribution\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Comparison and Odds Ratio**  \n",
    "\n",
    "Now consider **two competing models**:  \n",
    "- $M_1$ = Standard Model (SM)  \n",
    "- $M_2$ = Alternative Model (AM)\n",
    "\n",
    "The goal is to compare which model better explains the observed data $D$.  \n",
    "We compute the **posterior probability** of each model:\n",
    "\n",
    "$$\n",
    "p(M_i \\mid D) = \\frac{p(D \\mid M_i) \\cdot p(M_i)}{p(D)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $p(D \\mid M_i)$ is the **evidence** for model $M_i$  (keep attention with the names, also known as marginal likelihood)\n",
    "- $p(M_i)$ is the **prior probability** of model $M_i$  \n",
    "- $p(D)$ is the same for all models (acts as normalization)\n",
    "\n",
    "The **odds ratio** between $M_2$ and $M_1$ is:\n",
    "\n",
    "$$\n",
    "\\mathcal{O}_{21} = \\frac{p(M_2 \\mid D)}{p(M_1 \\mid D)} = \n",
    "\\underbrace{\\frac{p(D \\mid M_2)}{p(D \\mid M_1)}}_{\\text{Bayes Factor } B_{21}} \\cdot \n",
    "\\underbrace{\\frac{p(M_2)}{p(M_1)}}_{\\text{Prior Odds}}\n",
    "$$\n",
    "\n",
    "> **Important**: The Bayes factor $B_{21}$ compares how well each model explains the data **on average over their parameter spaces**.  \n",
    "\n",
    "> To compare models, you need to marginalize (integrate) over parameters:\n",
    "> \n",
    "> $$\n",
    "> p(D \\mid M) = \\int p(D \\mid \\theta, M) \\cdot p(\\theta \\mid M) \\, d\\theta\n",
    "> $$\n",
    "\n",
    "\n",
    "###  Interpretation of Odds Ratio\n",
    "\n",
    "If $\\mathcal{O}_{21} = 10$, then:\n",
    "> *Model $M_2$ is ten times more probable than Model $M_1$, given the data and our prior knowledge.*\n",
    "\n",
    "This means that a **10-to-1 bet** in favor of $M_2$ over $M_1$ would be a **fair bet** based on current knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes Factor $ \\mathcal{B} $**\n",
    "\n",
    "The **Bayes Factor** compares how well two models predict the data:\n",
    "\n",
    "$$ B_{12} = \\frac{p(D \\mid M_1)}{p(D \\mid M_2)} = \\frac{\\mathcal{Z}_1}{\\mathcal{Z}_2} $$\n",
    "\n",
    "Where $ \\mathcal{Z}_i $ is called the **evidence** for model $M_i$: \n",
    "\n",
    "$$ \\mathcal{Z}_i = \\int p(D \\mid \\theta_i, M_i)\\, p(\\theta_i \\mid M_i)\\, d\\theta_i $$\n",
    "3£\n",
    "- $p(D \\mid \\theta_i , M_i)$: likelihood dei dati dati un certo valore di massa $m_H$ (che sarebbe il parametro $\\theta_i$) sotto il modello $M_i$ (es. SM o AM)\n",
    "\n",
    "- $p(\\theta_i \\mid M_i)$: prior sul valore della massa nel modello $M_i$\n",
    "\n",
    "This is a **weighted average of the likelihood** over all possible values of the model parameters, weighted by the prior.\n",
    "\n",
    "### Important distinction:\n",
    "- The **likelihood** $p(D \\mid \\theta, M)$ is a function of parameters.\n",
    "- The **evidence** $p(D \\mid M)$ is a **single number** that tells you how well the entire model explains the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Jeffreys’ Scale for Interpreting Bayes Factors\n",
    "\n",
    "| $B_{10}$ Value        | Evidence in favor of Model $M_1$         |\n",
    "|------------------------|------------------------------------------|\n",
    "| $<1$                   | Supports Model $M_2$                      |\n",
    "| $1$ to $3$             | Weak                                     |\n",
    "| $3$ to $10$            | Moderate                                 |\n",
    "| $10$ to $100$          | Strong                                   |\n",
    "| $>100$                 | Decisive                                 |\n",
    "\n",
    "Higher $B_{10}$ → stronger evidence for Model $M_1$\n",
    "\n",
    "---\n",
    "\n",
    "### **Frequentist vs Bayesian Approach**\n",
    "\n",
    "|              | **Frequentist**                                                 | **Bayesian**                                                        |\n",
    "|----------------------|------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Parameters**        | Treated as **fixed but unknown** values                         | Treated as **random variables** with probability distributions       |\n",
    "| **Probability**       | Interpreted as **long-run frequency** of events                 | Interpreted as **degree of belief** or subjective certainty          |\n",
    "| **Data**              | Considered as **repeatable random samples** from a population   | Considered as **fixed once observed**; inference is updated via Bayes’ rule |\n",
    "| **Use of Prior**      | **Not used**; all inference is based on data                    | **Essential**; priors express beliefs before seeing the data         |\n",
    "| **Inference**         | Based on **sampling distribution** and repeated hypothetical experiments | Based on **posterior distribution**, combining prior and data        |\n",
    "| **Model Comparison**  | Uses **p-values, confidence intervals, likelihood ratios**      | Uses **Bayes factors, posterior probabilities, credible intervals**  |\n",
    "| **Interpretation**    | Results apply to **what would happen in repeated experiments**  | Results apply to **the current data and model assumptions**          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e56e7",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 10** - Bayesian Inference III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfc965",
   "metadata": {},
   "source": [
    "- Monte Carlo  \n",
    "- Markov chains (detailed balance)  \n",
    "- MCMC (Markov Chain Monte Carlo)  \n",
    "- Metropolis–Hastings algorithm  \n",
    "- Corner plot  \n",
    "- Trace plot  \n",
    "- Burn-in  \n",
    "- Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1f519",
   "metadata": {},
   "source": [
    "###  **Monte Carlo Methods**\n",
    "\n",
    "- **Monte Carlo methods** are computational algorithms that rely on repeated **random sampling** to estimate numerical results.\n",
    "- Widely used in physics, statistics, and Bayesian inference.\n",
    "- Particularly helpful when analytical solutions are difficult or impossible.\n",
    "- However, in high-dimensional spaces, standard Monte Carlo can become highly **inefficient**, which motivates the use of improved techniques like MCMC.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Markov Chains**\n",
    "\n",
    "A **Markov chain** is a sequence of random variables (states) where the probability of moving to the next state depends **only on the current state**, not on the path taken to get there. This is called the **Markov property**, or **memorylessness**.\n",
    "\n",
    "Mathematically, if you're in state $x$ now, the probability of moving to state $x'$ is:\n",
    "$\n",
    "P(x | x')\n",
    "$\n",
    "\n",
    "#### **Stationary Distribution**\n",
    "\n",
    "A **stationary distribution** $\\pi(x)$ is a probability distribution over the states that **remains unchanged** as the Markov chain evolves.\n",
    "\n",
    "Quindi dice che se il sistema ha distribuzione $\\pi$ in un certo istante, la manterrà invariata in ogni istante futuro.\n",
    "\n",
    "It satisfies the condition:\n",
    "$$\n",
    "\\sum_{x} \\pi(x) \\cdot P(x \\to x') = \\pi(x') \\quad \\text{for every } x'\n",
    "$$\n",
    "\n",
    "This means that the probability of being in state $x'$ at the next step is equal to the sum of the probabilities of arriving at $x'$ from all other states, taking into account:\n",
    "- How likely it was to be in each of those states ($\\pi(x)$)\n",
    " \n",
    "- And the probability of transitioning from $x$ to $x'$ ($P(x \\to x')$).\n",
    "\n",
    "\n",
    "#### **Detailed Balance Condition**\n",
    "\n",
    "A **sufficient condition** for a Markov chain to have a stationary distribution $\\pi(x)$ is the **detailed balance condition**:\n",
    "\n",
    "$$\n",
    "\\pi(x) \\cdot P(x \\to x') = \\pi(x') \\cdot P(x' \\to x)\n",
    "$$\n",
    "\n",
    "This condition says that the **flow of probability** from state $x$ to $x'$ is the same as from $x'$ to $x$.  \n",
    "It implies **reversibility** of the chain and ensures that $\\pi(x)$ is a stationary distribution.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Markov Chain Monte Carlo (MCMC)**\n",
    "\n",
    "- **MCMC** combines the idea of Monte Carlo sampling with Markov chains to draw samples from complex distributions.\n",
    "- The key idea is to construct a Markov chain whose stationary distribution is the **target distribution** $\\pi(x)$ (typically the posterior in Bayesian inference).\n",
    "- Even if $\\pi(x)$ is only known **up to a normalization constant**, MCMC methods can still be used to sample from it.\n",
    "\n",
    "\n",
    "####  Why Use MCMC?\n",
    "\n",
    "In Bayesian inference, we are often interested in the **posterior distribution**, but computing it explicitly is hard.  \n",
    "MCMC allows us to **approximate** this distribution by generating samples from it, rather than calculating it directly.\n",
    "\n",
    "This makes MCMC a powerful and flexible tool for inference in complex models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Metropolis-Hastings Algorithm**\n",
    "\n",
    "- A widely used MCMC algorithm.\n",
    "- Generates a sequence of samples that approximates $\\pi(x)$, the posterior distribution.\n",
    "\n",
    "#### Steps:\n",
    "0. Define a prior based on your knowlwdge, such as a normal distribution centered at 0 with a define spread\n",
    "1. Start from an initial point $\\mu = -1$ (just an example). Draw another distribution **T($\\mu | \\mu'$)** centered at the propose value: Usually a normal distribution centered at the current $\\mu$, with spread $\\tau$.\n",
    "2. Propose a new point $\\mu'$ from the **proposal distribution T($\\mu | \\mu'$)** .\n",
    "3. calculate the prior value, i.e. the value of the prior at the 2 $\\mu$ we are evaluating, this expressed how much I belived in this value.\n",
    "4. calculate the likelihood between the original dataset and the distributions centered at the two value we are considering\n",
    "5. Compute the **acceptance probability** = $\\alpha$,  define as fraction between the 2 product of the likelihood x prior x T:\n",
    "    $$\n",
    "    \\alpha = \\min\\left(1,\\ \n",
    "    \\frac{L(\\text{data} \\mid \\mu') \\cdot P_{\\text{prior}}(\\mu') \\cdot T(\\mu \\mid \\mu')}\n",
    "        {L(\\text{data} \\mid \\mu) \\cdot P_{\\text{prior}}(\\mu) \\cdot T(\\mu' \\mid \\mu)}\\right)\n",
    "    $$ \n",
    "\n",
    "    If the proposal distribution is **symmetric**, i.e. $T(\\mu | \\mu') = T(\\mu' | \\mu)$, the acceptance ratio simplifies to:\n",
    "\n",
    "    $$\n",
    "    \\alpha = \\min\\left(1,\\ \n",
    "    \\frac{L(\\text{data} \\mid \\mu') \\cdot P_{\\text{prior}}(\\mu')}\n",
    "        {L(\\text{data} \\mid \\mu) \\cdot P_{\\text{prior}}(\\mu) }\\right)\n",
    "    $$ \n",
    "6. Draw a uniform random number between 0 and 1 ... Accept $\\mu'$ with probability $\\alpha$, otherwise stay at $\\mu$.\n",
    "7. Repeat the process to create a Markov chain.\n",
    "\n",
    "NOTE: likelihood x prior = posterior (idgaf about the evidence...) \n",
    "\n",
    "KEEP ATTENTION: MA gives you a sample dataset from the posterior PDF, but not the PDF itself! you have to run some density estimation methode (KDE) for achive the correct $\\pi(x)$\n",
    "\n",
    "---\n",
    "\n",
    "### **Burn-in and Plot**\n",
    "\n",
    "- The early steps of MCMC may not represent the target distribution well.\n",
    "- The **burn-in period** refers to the initial segment of the chain that is discarded.\n",
    "- A **trace plot** shows the sampled values over steps — used to check convergence.\n",
    "- A **corner plot** (also called pair plot) is used to visualize multidimensional posterior distributions, it Shows histograms of each parameter.\n",
    "- See the correspondent exercise for more comment on the joint distribution\n",
    "---\n",
    "\n",
    "### **Autocorrelation and Thinning in MCMC**\n",
    "\n",
    "In a Markov Chain Monte Carlo (MCMC) simulation, successive samples are typically **autocorrelated**, meaning each sample is statistically dependent on previous ones. This **reduces the efficiency** of the sampling process, because many steps are needed to obtain samples that are effectively independent.\n",
    "\n",
    "This dependence is quantified by the **correlation length** (also called the **autocorrelation time**) $\\lambda$, which measures how many steps it takes for the chain to \"forget\" it's current state. As a result, the **effective number of independent samples** is smaller than the total number of steps:\n",
    "\n",
    "$$\n",
    "\\text{Effective samples} \\approx \\frac{N_\\text{total}}{\\lambda}\n",
    "$$\n",
    "\n",
    "One historical method to address autocorrelation is **thinning**: keeping only every *k*-th sample (e.g., every 5th), discarding the rest. While this may reduce autocorrelation between stored samples, it also **throws away potentially useful information**. \n",
    "\n",
    "---\n",
    "\n",
    "### **Step Size Tuning**\n",
    "\n",
    "The **step size** (aka the spread of the T distribution, I call it $\\tau$ above) controls how far the chain jumps between states. Choosing it well is crucial:\n",
    "\n",
    "- If the step is **too small**: the chain moves slowly, samples are highly correlated → inefficient exploration.\n",
    "- If the step is **too large**: the chain proposes states far from the current one, and many of them get **rejected** → again, inefficient.\n",
    "\n",
    "the **Goal** is balance **acceptance rate** and **decorrelation**.\n",
    "\n",
    "Typical strategy:\n",
    "- Tune the step size to achieve a **moderate acceptance rate** (e.g. ~20–40% for Metropolis-Hastings).\n",
    "- Monitor the **autocorrelation** or use **diagnostic plots** (e.g. trace plot) to check if the chain is mixing well.\n",
    "\n",
    "There is no universal best step size: it depends on the shape of the target distribution and the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59df8be",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 11** - Bayesian Inference IV </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5d0e0",
   "metadata": {},
   "source": [
    "- Adaptive Metropolis  \n",
    "- Single Component Adaptive Metropolis  \n",
    "- Hamiltonian Monte Carlo  \n",
    "- Emcee  \n",
    "- Gibbs sampling  \n",
    "- Conjugate Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc1488",
   "metadata": {},
   "source": [
    "### **Adaptive Metropolis**\n",
    "\n",
    "**Definition**: An MCMC method that adapts the proposal distribution $T$ based on the history of the chain.\n",
    "\n",
    "- In the **Metropolis-Hastings** algorithm, choosing a good proposal distribution is crucial.\n",
    "- Adaptive Metropolis (AM) automatically tunes the **covariance matrix** of the proposal distribution as the chain progresses.\n",
    "- This allows better exploration of the posterior, especially in high-dimensional or correlated parameter spaces.\n",
    "- This method doesn't use only the last point, but it use the entire chain, **our chain is no longer markovian**.\n",
    "- To fix this, we often let the algorithm \"learn\" during an initial phase (called the tuning stage), where it adapts the proposal. After that, we stop adapting and keep the proposal fixed — from that moment on, the chain becomes Markovian again and gives valid Bayesian results.\n",
    "---\n",
    "\n",
    "### **Single Component Adaptive Metropolis (SCAM)**\n",
    "\n",
    "**Definition**: A variant of Adaptive Metropolis where only one parameter (component) is updated at a time.\n",
    "\n",
    "- Standard MCMC or AM methods like Metropolis-Hastings suffer of low rate in high-dimensional spaces.\n",
    "- SCAM is especially useful when parameters have **different scales or conditional dependencies**.\n",
    "- At each iteration, only one dimension of the parameter vector is updated, often using an adaptive univariate proposal.\n",
    "- This can be more efficient than updating all parameters jointly, especially in the presence of strong correlations.\n",
    "- The adaptation improves sampling efficiency over time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Other method**\n",
    "\n",
    "**Hamilton Monte Carlo**: An MCMC algorithm that uses concepts from physics (Hamiltonian dynamics) to make informed proposals in parameter space, improving efficiency in exploring complex posterior distributions.\n",
    "\n",
    "\n",
    "**Differential Evolution** : A population-based optimization algorithm that can be adapted for MCMC by evolving a set of candidate solutions using differences between randomly selected members of the population to guide proposals.\n",
    "\n",
    "---\n",
    "\n",
    "### **`emcee`**\n",
    "\n",
    "- It's a full python package.\n",
    "- Emcee is designed to efficiently sample from **complex, anisotropic posterior distributions**.\n",
    "- It uses multiple parallel \"**walkers**\" that share information and adapt proposals to the geometry of the target distribution.\n",
    "- The process need a starting guess, we don't need to be too precise, the chain will eventualy converge to the true value\n",
    "- Need also the **number of step** for the chain, and the **burn-in region** too\n",
    "- has some specific method to discard the **auto correlation lenght**\n",
    "\n",
    "---\n",
    "\n",
    "### **Gibbs Sampling**\n",
    "\n",
    "**Gibbs sampling** is a type of MCMC algorithm that avoids the need for rejection — **every proposed sample is accepted**. It's especially efficient when the conditional distributions of the parameters are known and easy to sample from.\n",
    "\n",
    "\n",
    "#### **How It Works**\n",
    "\n",
    "1. **Initialize** the sampler at some starting point in parameter space.\n",
    "2. **Iterate over each parameter** in turn.\n",
    "3. For each parameter:\n",
    "   - Keep all other parameters fixed.\n",
    "   - Sample a new value from the **conditional posterior distribution** (THAT YOU HAVE TO KNOW) of that parameter.\n",
    "  \n",
    "   In **Gibbs sampling**, you want to sample from a multivariate posterior distribution, like: $ p(\\theta_1, \\theta_2 \\mid \\text{data}) $\n",
    "\n",
    "   But instead of sampling from the joint distribution directly (which is often hard), Gibbs sampling works by cycling through the **conditional distributions**:\n",
    "\n",
    "   1. Fix $\\theta_2$, and sample $\\theta_1$ from the conditional distribution:\n",
    "\n",
    "      $$\n",
    "      p(\\theta_1 \\mid \\theta_2, \\text{data})\n",
    "      $$\n",
    "\n",
    "   2. Fix the new $\\theta_1$, and sample $\\theta_2$ from:\n",
    "\n",
    "      $$\n",
    "      p(\\theta_2 \\mid \\theta_1, \\text{data})\n",
    "      $$\n",
    "\n",
    "   You **must know** and be able to **sample from** these conditional distributions. If not, Gibbs sampling **cannot** be used, and you need to switch to a more general method like **Metropolis-Hastings**.\n",
    "\n",
    "4. Repeat this process for **many iterations (Gibbs steps)** to build your Markov chain.\n",
    "\n",
    "\n",
    "####  **Key Features**\n",
    "\n",
    "-  Every sample is **automatically accepted** (no rejection step).\n",
    "-  Very **fast and efficient**, especially in high-dimensional spaces.\n",
    "-  **Short burn-in period** — reaches equilibrium quickly.\n",
    "-  **Limitation**: Requires knowledge of the **conditional distributions** of all parameters.\n",
    "\n",
    "\n",
    "Use it when:\n",
    "- You know how to compute and sample from the **conditional posterior distributions**.\n",
    "- You need a **simple, efficient MCMC** method.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conjugate Prior**\n",
    "\n",
    "**Definition**:  \n",
    "In Bayesian statistics, a **conjugate prior** is a prior distribution that, when combined with a particular **likelihood function**, results in a **posterior distribution** that is in the same family as the prior.\n",
    "\n",
    "- It simplifies calculations.\n",
    "- The posterior has a known and tractable form.\n",
    "- Useful for analytical solutions and understanding posterior updates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8444528",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 12** - Bayesian Inference V </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa6731",
   "metadata": {},
   "source": [
    "- Savage – Dickey ratio  \n",
    "- Nested Sampling with Dynesty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39c60a",
   "metadata": {},
   "source": [
    "### **Savage – Dickey Density Ratio**\n",
    "\n",
    "It's a shortcut to compare two models Odd's factor when one is a **special case** of the other.\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "- $M_1$: the **simple model**, where a parameter $A = 0$ (e.g., \"no signal\", A stay for Amplitude)\n",
    "- $M_2$: the **full model**, where $A$ can be anything (e.g., \"signal allowed\")\n",
    "\n",
    "Then, instead of computing evidence for both models, we use this trick:\n",
    "\n",
    "$$\n",
    "\\mathcal{B} = \\frac{p(A = 0)}{p(A = 0 \\mid \\text{data})}\n",
    "$$\n",
    "\n",
    "This is the **Bayes factor** between $M_2$ and $M_1$.\n",
    "\n",
    "#### What Does It Mean?\n",
    "\n",
    "- $p(A = 0)$ is how much we believed in $A = 0$ **before seeing the data**.\n",
    "- $p(A = 0 \\mid \\text{data})$ is how much we believe in $A = 0$ **after seeing the data**.\n",
    "\n",
    "If the data makes $A = 0$ **less likely**, then $M_1$ is disfavored.\n",
    "\n",
    "\n",
    "#### How Do We Use It?\n",
    "\n",
    "1. Run MCMC on the full model $ M_2 $.\n",
    "2. From the samples, estimate $ p(A = 0 \\mid \\text{data}) $.\n",
    "   - For example, use a histogram or KDE on the samples of $ A $.\n",
    "3. Calculate $\\mathcal{B}$ using the ratio.\n",
    "\n",
    "\n",
    "#### Why is this useful?\n",
    "\n",
    "- You don’t need to compute full evidences $\\mathcal{Z}_1$ and $\\mathcal{Z}_2$.\n",
    "- Fast and simple, especially when comparing null hypotheses.\n",
    "\n",
    "\n",
    "\n",
    "| Concept                 | recup                                                                                                                                                                                                                                                                                                                                                                                       |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Bayes Factor**        | A **general method** to compare two models ($M_1$ and $M_2$) by taking the ratio of their **evidences**:   $ B_{21} = \\frac{P(\\text{data} \\mid M_2)}{P(\\text{data} \\mid M_1)} $ It tells you how much more the data supports one model over the other. Requires full integration over parameter space.                                                                               |\n",
    "| **Savage–Dickey Ratio** | A **special case** of the Bayes factor, used **only** when: $M_1$ is a **special case** of $M_2$ (e.g. $A=0$) You can compute the **prior** and **posterior density** at a single point (e.g. $A=0$). Then:  $\\mathcal{B}_{21} = \\frac{p(A=0)}{p(A=0 \\mid \\text{data})}$ It's a shortcut that **gives the Bayes factor** without needing to compute full evidences. |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Nested Sampling**\n",
    "\n",
    "Nested Sampling is a method to **compute the evidence** $\\mathcal{Z}$ in Bayesian inference:\n",
    "\n",
    "$$\n",
    "\\mathcal{Z} = \\int \\mathcal{L}(\\theta) \\, \\pi(\\theta) \\, d\\theta\n",
    "$$\n",
    "\n",
    "- $\\mathcal{L}(\\theta)$ = the likelihood function, i.e., how likely the observed data is given a specific value of $\\theta$.\n",
    "- $\\pi(\\theta)$ = the prior probability density of $\\theta$, expressing our belief about $\\theta$ before seeing the data.\n",
    "- $\\mathcal{Z}$ = the evidence or marginal likelihood, which is the normalization constant for Bayes' theorem.\n",
    "\n",
    "\n",
    "This is crucial for **comparing models**, since:\n",
    "\n",
    "$$\n",
    "\\text{Bayes Factor} = \\frac{\\mathcal{Z}_2}{\\mathcal{Z}_1}\n",
    "$$\n",
    "\n",
    "But computing $\\mathcal{Z}$ is hard — especially in high dimensions. That’s where Nested Sampling comes in.\n",
    "\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. Start with $N$ random points from the **prior**.\n",
    "2. Find the one with the **lowest likelihood**, call it $L_{\\text{min}}$.\n",
    "3. Remove it, and **replace** it with a new point sampled from the prior **subject to** $\\mathcal{L} > L_{\\text{min}}$.\n",
    "4. Keep track of the shrinking prior volume and the associated likelihoods.\n",
    "5. Approximate the 1D integral $\\mathcal{Z}$ using the sequence of $(X_i, \\mathcal{L}_i)$ points.\n",
    "\n",
    "In Python, the `dynesty` library does this automatically:\n",
    "\n",
    "---\n",
    "\n",
    "### **Very important** \n",
    "- The samples produced by a **nested sampling** run are **weighted**—each sample contributes differently to the final result based on its weight, which reflects its posterior probability mass.\n",
    "  \n",
    "  Therefore, you must **always use the samples together with their weights** when computing expectations, visualizing posteriors, or performing any statistical analysis.  \n",
    "\n",
    "- If an MCMC is taking to long, you can interrupt it and use samples you got so far (assuming you're past the burn in period). This is not possible with nested sampling; samples don't make any sense until the algorithm reached the top of the posterior! You have to let it go till the end.\n",
    "\n",
    "- See the correspondent exercise for more comment about the possible plot on dynesty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42ce45",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 13** - Data Mining & Machine Learning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37232c4",
   "metadata": {},
   "source": [
    "- What is machine learning \n",
    "- Features, samples, classes and instances\n",
    "- Scikit-learn\n",
    "- Supervised learning (classification and regression)\n",
    "- Unsupervised learning (clustering, dimensionality reduction) \n",
    "- Model validation:\n",
    "  - Confusion matrix\n",
    "  - Training set / Test set\n",
    "- **Note**: This lecture provided a brief overview of many concepts, which will be explored in more depth later on.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59d82b",
   "metadata": {},
   "source": [
    "### **What is Machine Learning and Its Main Purpose**\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence where computers learn patterns from data to make decisions or predictions without being explicitly programmed. The main goal is to build models that generalize well on new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Terms**\n",
    "\n",
    "- **Features:** The input variables or attributes used to describe each data point (e.g., height, weight).  \n",
    "- **Sample / Instance:** A single data point or observation with its features (e.g., one person's measurements).  \n",
    "- **Classes:** Categories or labels that data points belong to in classification tasks (e.g., cat, dog).  \n",
    "- **Target:** The output or label we want to predict.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Scikit-learn`** \n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning. It provides easy-to-use tools for data preprocessing, modeling, and evaluation.\n",
    "\n",
    "- **Data format:**  \n",
    "  - Input features: a 2D array/matrix of shape $(n\\_samples, n\\_features)$.  Always in this form, **it's very picky**.\n",
    "  - if your x is just 1D, you have to reshape it in ND by using `np.newaxis()`\n",
    "  - Target labels: A 1D array of length $n\\_samples$.\n",
    "\n",
    "\n",
    "#### **Common Scikit-learn Methods**\n",
    "\n",
    "An **estimator** in scikit-learn is any object implementing at least the methods `.fit()` and `.predict()` or `.transform()`. \n",
    "Usually call model.\n",
    "\n",
    "- **`model.fit(X, y)`:** Trains the model on data $X$ with labels $y$.  \n",
    "- **`model.predict(x)`:** Predicts labels for new data $x$.  \n",
    "- **`model.predict_proba(X)`:** Gives the probability estimates for classification classes (if available).  \n",
    "- **`model.score(X, y)`:** Returns the accuracy on data $X$ compared to true labels $y$.  \n",
    "- **`model.transform(X)`:** Applies a transformation to data $X$ (used in dimensionality reduction, feature extraction).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Supervised Learning**\n",
    "\n",
    "In supervised learning, the model learns from labeled data:\n",
    "\n",
    "- **Classification:** Predicting discrete labels. We will use the propriety of a dataset to predict unlabeled data.\n",
    "- **Regression:** Predicting continuous values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Unsupervised Learning**\n",
    "\n",
    "Unsupervised learning deals with unlabeled data:\n",
    "\n",
    "- **Clustering:** Grouping similar data points .  \n",
    "- **Dimensionality Reduction:** Reducing data features while preserving structure , usefull in data visualization.\n",
    "- **Density Estimation** can determine the distribution of the data within the parameter space.\n",
    "\n",
    "The main difference is that no label is provided; the goal is to find hidden patterns or structure.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Model Validation**\n",
    "Determine how well your model will generalize from the training dataset to future unlabeled data.\n",
    "\n",
    "- **Confusion Matrix:** A matrix showing true vs. predicted classes to evaluate classification accuracy and errors.  The element on the diagnal are the one correctly identified, the off-diagonal are confunded.\n",
    "- **Training Set:** Data used to train the model.  \n",
    "- **Test Set:** Separate data used to evaluate model performance on unseen (but labeled) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dbe69",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 14** - Clustering </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcc0cd",
   "metadata": {},
   "source": [
    "- whats are hyperparameter and make example\n",
    "- training  / validation / test\n",
    "- cross validation hyper parameter tuning \n",
    "- K - fold cross validation\n",
    "- clustering\n",
    "- K- mean clustering\n",
    "- mean shift clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b66ea",
   "metadata": {},
   "source": [
    "### **Hyperparameters**\n",
    "\n",
    "Hyperparameters are parameters that are **not learned from the data**, but set **before** the learning process begins. They control the learning process and model structure.\n",
    "They can easily fool us into thinking something wrong about the data.\n",
    "\n",
    "### Examples:\n",
    "- Number of clusters in K-means ($K$)\n",
    "- number of bins in a histogramm\n",
    "- Depth of a decision tree\n",
    "- Bandwidth in Kernel Density Estimation (KDE)\n",
    "\n",
    "These are typically chosen via **validation** methods like **cross-validation** (see below).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Training / Validation / Test Sets**\n",
    "\n",
    "We can think of divide the dataset into:\n",
    "\n",
    "1. **Training set**: Used to **fit** the model.\n",
    "2. **Validation set**: Used to **tune hyperparameters** and select the best model version.\n",
    "3. **Test set**: Used only **at the end** to report the final unbiased performance of the selected model.\n",
    "\n",
    "But we know that less data is bad for ML, and also make the result dipendent on what is inside each set (think at outliars, if they fall into test gave a different risult...)\n",
    "We can solve this problem thanks to Cross Validation (CV)\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Fold Cross Validation**\n",
    "\n",
    "Cross-validation is a technique to **evaluate the generalization ability** of a model by partitioning the data into multiple subsets.\n",
    "\n",
    "### Why it matters:\n",
    "- Helps prevent **overfitting** and **underfitting**\n",
    "- Makes full use of data (especially important with small datasets)\n",
    "- Gives a better estimate of model performance\n",
    "\n",
    "K-Fold is a smarter form of cross-validation:\n",
    "\n",
    "1. Split the Training data into $K$ equal parts (folds).\n",
    "2. For each fold:\n",
    "   - Use $K-1$ folds for training\n",
    "   - Use the remaining fold for validation\n",
    "3. Repeat $K$ times so every point gets to be in the validation set once.\n",
    "4. Extract the best parameter from the $K$ validation results.\n",
    "5. Use the Test data to evaluate the model.\n",
    "\n",
    "We can take it to extreme by taking K = N = number of data, this is called \"**Leave one  out**\" Cross Validation, This drammatically increase the computational cost, but reduce gratly the variance.\n",
    "\n",
    "![](https://scikit-learn.org/stable/_images/grid_search_cross_validation.png)\n",
    "\n",
    "**IMPORTANTE NOTE:** (in my opinion) \n",
    "Before any type of analysis to the data (but it's more important if I divide the dataset in different sub-folder) it' crucial shuffle corrrectly the dataset, that's because you usally have some outliars all nearby thanks to a bad measurment that day, by shuffle them you don't take the risk to have all the outliars in a single folder.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Clustering**\n",
    "\n",
    "Clustering is **unsupervised learning**: grouping similar data points without knowing the labels.  \n",
    "It aims to discover structure in data by finding clusters (dense regions) of similar observations.\n",
    "\n",
    "\n",
    "### **K-Means Clustering**\n",
    "\n",
    "**K-Means** is a simple and popular **centroid-based** clustering algorithm.\n",
    "\n",
    "#### How it works:\n",
    "1. Choose the number of clusters $k$ (a hyperparameter).\n",
    "2. Initialize $k$ **centroids** (usually randomly).\n",
    "3. Assign each data point to the **nearest centroid**.\n",
    "4. Update centroids as the **mean of points assigned** to each cluster.\n",
    "5. Repeat steps 3–4 until convergence (no significant change in centroids or assignments).\n",
    "\n",
    "**Objective:** Minimize the **within-cluster sum of squares** (WCSS):\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "where $C_i$ is the set of points in cluster $i$ and $\\mu_i$ is the centroid of $C_i$. $K$ is the number of cluster. \n",
    "\n",
    "You are minimizing the distance between points and centroid for each cluster\n",
    "\n",
    "#### Strengths:\n",
    "- Efficient and scalable\n",
    "- Easy to implement\n",
    "\n",
    "#### Weaknesses:\n",
    "- Requires choosing $k$\n",
    "- Assumes spherical, equally sized clusters\n",
    "- Sensitive to initialization\n",
    "- Sensitive to outliers\n",
    "\n",
    "\n",
    "\n",
    "### **Mean Shift Clustering**\n",
    "\n",
    "**Mean Shift** is a **non-parametric**, **density-based**, **centroid-shifting** clustering algorithm.\n",
    "\n",
    "#### How it works (Density Gradient Ascent):\n",
    "1. Define a **window (kernel)** around each data point — typically a Gaussian with bandwidth $h$.\n",
    "2. Compute the **mean of data points** within the window.\n",
    "3. Move (shift) the window center toward the **mean**.\n",
    "4. Repeat steps 2–3 until convergence (i.e., the center stops moving).\n",
    "5. Merge points converging to the same center into a **single cluster**.\n",
    "\n",
    "\n",
    "It performs **gradient ascent** on a kernel density estimate (KDE). The mode (maximum) of the KDE becomes the cluster center.\n",
    "\n",
    "\n",
    "#### Strengths:\n",
    "- **Does not require predefining the number of clusters**\n",
    "- Can identify clusters of **arbitrary shape**\n",
    "- **Robust** to outliers\n",
    "- Works well when clusters correspond to **modes in the density**\n",
    "\n",
    "#### Weaknesses:\n",
    "- Computationally expensive (especially on large datasets)\n",
    "- **Bandwidth selection** is critical — too large merges clusters, too small splits them\n",
    "- Does not scale well in high dimensions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702ced8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 15** - Dimensional Reduction I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d3afd",
   "metadata": {},
   "source": [
    "- Curse of Dimensionality\n",
    "- PCA \n",
    "- scree plot\n",
    "- interpreting PCA result\n",
    "- PCA it's linear, it struggle a lot with non linear component\n",
    "- Recostruction of dark area with PCA\n",
    "- overview of non-negative  matrix factorization\n",
    "- overview of ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea595a6",
   "metadata": {},
   "source": [
    "### **Curse of Dimensionality**\n",
    "\n",
    "As the number of features (dimensions) increases:\n",
    "- The volume of space increases exponentially.\n",
    "- Data becomes **sparse**, even if you have a lot of it.\n",
    "- Models struggle to generalize well.\n",
    "- Distance metrics (like Euclidean distance) lose meaning — all points start to look equally distant.\n",
    "\n",
    "Example: If each feature has a 50% chance of matching, the probability that all $n$ match is $0.5^n$. Even with just 4 features, that’s only 6.25%!\n",
    "\n",
    "---\n",
    "### **PCA (Principal Component Analysis)**\n",
    "\n",
    "PCA is a technique to **reduce dimensionality** by projecting data to a new space:\n",
    "- New axes are the directions of **maximum variance** of the original dataset.\n",
    "- These axes (principal components) are **orthogonal**.\n",
    "- Redundant dimensions are **discarded**.\n",
    "- The process is equivalent to **diagonalizing the covariance matrix**.\n",
    "\n",
    "**PCA is a dimensional reduction process because we can generally account for *nearly all* of the variance in the data set with fewer than the original $K$ dimensions.** See \n",
    "\n",
    "#### Steps:\n",
    "1. **Center the data**: Subtract the mean.\n",
    "2. **Scale the data**: Divide by the standard deviation.\n",
    "3. Normalize samples (optional, done for spectral images).\n",
    "4. Compute the **covariance matrix**.\n",
    "5. Compute **eigenvectors/eigenvalues**.\n",
    "6. Sort eigenvectors by decreasing eigenvalue → these are the principal components. (eingvalues reflect the variance)\n",
    "\n",
    "One you have the eigenvectors $e_j(k)$, you can recostruct a true data $x_i(k)$ in the eigenvecture basis as: \n",
    "\n",
    "$$\n",
    "    x_i(k) = \\mu(k) + \\sum_j \\theta_{ij}e_j(k)\n",
    "$$\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/Screenshot 2025-06-27 154332.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "#### PCA Limitations\n",
    "\n",
    "- **Linear**: PCA can’t handle **non-linear structures** in data.\n",
    "- Struggles with **curved manifolds** (e.g., spirals).\n",
    "- We would need all the component for the 100% exlaination\n",
    "- how many component should i keep? Cross validation is the answer ...\n",
    "\n",
    "let's look at the video for a better comprensation of how it work\n",
    "\n",
    "\n",
    "#### **Scree Plot & Explained Variance**\n",
    "\n",
    "A **scree plot** shows eigenvalues (variance explained) by each principal component.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/SCREEPLOT.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "In our exaple the first 2 components explain 96% of the variance:\n",
    "- So you can reduce your data to 2D while keeping most information.\n",
    "- Useful for **visualization** and **noise reduction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dark Area Reconstruction with PCA**\n",
    "\n",
    "You can use PCA to reconstruct **missing or corrupted data** (e.g., missing regions in astronomical images):\n",
    "- Fit PCA on complete data.\n",
    "- Project corrupted sample into the PC space.\n",
    "- Reconstruct using the leading PCs → fill in missing values based on structure learned from the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overview: Non-Negative Matrix Factorization (NMF)**\n",
    "\n",
    "**NMF** is a technique that factorizes a matrix $X$ into the product of two matrices:\n",
    "\n",
    "$$\n",
    "X \\approx WH\n",
    "$$\n",
    "\n",
    "with the important constraint that **all elements of** $W$ and $H$ are **non-negative** (i.e., no negative numbers allowed).\n",
    "\n",
    "This makes the results easier to interpret in many real-world cases, especially when the data naturally can't be negative (like pixel intensities or word counts).\n",
    "\n",
    "**Applications:**\n",
    "- Discovering topics in a collection of documents\n",
    "- Breaking down images into basic components\n",
    "- Analyzing spectral data in astronomy or chemistry\n",
    "\n",
    "Compared to **PCA**, which can use negative values, **NMF tends to produce more interpretable results**, often representing **distinct parts** of the input (like separate topics or objects).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Just Know It Exists: ICA (Independent Component Analysis)**\n",
    "\n",
    "**ICA** is another method for decomposing data, but instead of focusing on variance (like PCA), it looks for **independent** components.\n",
    "\n",
    "That means it tries to separate a complex signal into **underlying sources** that are as statistically **independent** from each other as possible.\n",
    "\n",
    "**Typical use cases:**\n",
    "- **Blind source separation** (e.g., separating different voices recorded by multiple microphones)\n",
    "- Analyzing EEG brain signals\n",
    "- Uncovering independent trends in financial time series\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122589a8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 16** - Dimensional Reduction II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb5440",
   "metadata": {},
   "source": [
    "- manifold learning techniques\n",
    "- Locally Linear Embedding (what is it, scheme of what is doing)\n",
    "- IsoMap (what is it, scheme of what is doing)\n",
    "- t - SNE (overview)\n",
    "- Density Estimation (recup)\n",
    "- non parametric DE ( KDE , Nearest-Neighbor Density Estimation)\n",
    "- Parametric Density Estimation (Gaussian Mixture Models) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1156c",
   "metadata": {},
   "source": [
    "PCA, ICA and NFM are useless in handwritten dataset... they fail, let's have a look at other possible dimensionality reduction, more helpfull in this case.\n",
    "\n",
    "\n",
    "## **Manifold Learning Techniques**\n",
    "\n",
    "Manifold learning methods are **non-linear dimensionality reduction techniques** that assume data lies on a low-dimensional manifold embedded in a high-dimensional space. These techniques aim to **uncover the underlying structure** of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Locally Linear Embedding (LLE)**\n",
    "\n",
    "Locally Linear Embedding (LLE) is a non-linear dimensionality reduction technique that aims to map high-dimensional data to a lower-dimensional space while preserving the local structure of the data.\n",
    "\n",
    "#### What does LLE do?\n",
    "\n",
    "LLE assumes that high-dimensional data points lie on a smooth, lower-dimensional surface (called a manifold) and that, around each point, this surface is approximately flat (i.e., locally linear). LLE tries to capture this local linear structure and reconstruct it in fewer dimensions.\n",
    "\n",
    "LLE works in two main steps:\n",
    "\n",
    "#### Step 1: Capture the Local Geometry\n",
    "\n",
    "- For each data point, identify its $k$ nearest neighbors (based on Euclidean distance).\n",
    "- Compute weights that best express each point as a linear combination of its neighbors.  \n",
    "  In other words, for each point $x_i$:\n",
    "  $$ x_i \\approx \\sum_j w_{ij} x_j $$\n",
    "  where the sum is over the $k$ nearest neighbors of $x_i$.\n",
    "  \n",
    "- These weights reflect how each point depends on its neighbors in the original space. They are stored in a matrix $W$, where most entries are zero (since each point only depends on a few neighbors).\n",
    "\n",
    "#### Step 2: Find a Lower-Dimensional Embedding\n",
    "\n",
    "- Using the same weights $W$, find a set of new points $y_i$ in lower dimension (e.g., 2D or 3D) that preserve the same reconstruction pattern:\n",
    "  $$ y_i \\approx \\sum_j w_{ij} y_j $$\n",
    "- This means the relationships between points in the new space mimic the ones in the original space.\n",
    "\n",
    "\n",
    "#### Why is LLE useful?\n",
    "\n",
    "- It can \"unroll\" curved surfaces like the famous S-curve or Swiss roll, revealing their intrinsic low-dimensional structure.\n",
    "- Unlike linear methods like PCA, LLE can deal with non-linear manifolds.\n",
    "- It is unsupervised and only needs a distance metric and the number of neighbors $k$.\n",
    "\n",
    "---\n",
    "\n",
    "### **IsoMap**\n",
    "\n",
    "IsoMap is a **global non-linear dimensionality reduction** method that preserves geodesic distances between points.\n",
    "This method assumes data lies on a smooth manifold.\n",
    "\n",
    "**Scheme:**\n",
    "\n",
    "1. Construct a neighborhood graph (e.g., $k$-nearest neighbors).\n",
    "2. Compute shortest paths (geodesic distances) between all pairs using Dijkstra .\n",
    "3. Apply classical MDS (Multi-Dimensional Scaling) to the geodesic distance matrix.\n",
    "\n",
    "IsoMap preserves the **intrinsic geometry** of the data better than PCA in non-linear settings.\n",
    "\n",
    "---\n",
    "\n",
    "### **t-SNE** \n",
    "\n",
    "**t-SNE** is a tool that helps you **visualize data** with many features (high-dimensional) in just **2D or 3D**.\n",
    "\n",
    "What it does:\n",
    "\n",
    "- It takes complex data (with lots of numbers/features) and shows it as a simple **2D or 3D plot**.\n",
    "- Points that are **similar** in the original data end up **close together** in the plot.\n",
    "- It’s really good at showing **clusters** or groups in the data.\n",
    "\n",
    "How it works:\n",
    "\n",
    "- Figures out how similar the data points are.\n",
    "- Then tries to **keep those similarities** when showing the data in 2D or 3D.\n",
    "- Uses special math (like **probabilities** and **Student-t distribution**) to do it well.\n",
    "\n",
    "\n",
    "In short : **t-SNE = A smart way to draw complex data in 2D or 3D so you can spot patterns.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table of Dimensionality Reduction Methods**\n",
    "\n",
    "| Method     | Type        | Preserves     | Suitable for Visualization | Parametric | Main Use Case                          |\n",
    "|------------|-------------|----------------|-----------------------------|------------|----------------------------------------|\n",
    "| PCA        | Linear      | Global dist    | Yes                         | Yes        | General reduction, noise filtering     |\n",
    "| NMF        | Linear      | Parts-based    | Limited                     | Yes        | Topic modeling, text data              |\n",
    "| ICA        | Linear      | Indep. sources | No                          | Yes        | Signal separation (e.g., EEG, audio)   |\n",
    "| LLE        | Non-linear  | Local linearity| Yes                         | No         | Manifold learning, visualizing clusters|\n",
    "| IsoMap     | Non-linear  | Geodetics      | Yes                         | No         | Unfolding non-linear structures        |\n",
    "| t-SNE      | Non-linear  | Probability    | Yes                         | No         | Visualization of high-dim. data        |\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/mainfold.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Parametric Density Estimation: Gaussian Mixture Models (GMM)**\n",
    "\n",
    "GMM assumes that the data is generated from a **mixture of several Gaussian distributions**.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- Probability density function:  $p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\sigma_k)$  \n",
    "\n",
    "- where $\\pi_k$ are the mixing coefficients, and $\\mathcal{N}$ is the multivariate normal.\n",
    "\n",
    "- Using lots of kernels (maybe even more than the AIC or BIC score suggests) may make sense if you just want to provide an accurate description of the data (as in density estimation).  \n",
    "\n",
    "- Using fewer kernels makes mixture models more like clustering (see earlier lecture), where the suggestion is still to use many kernels in order to divide the sample into real clusters and \"background\".\n",
    "\n",
    "**Estimation:**\n",
    "\n",
    "- Parameters $(\\pi_k, \\mu_k, \\sigma_k)$ are learned using the **Expectation-Maximization (EM)** algorithm.\n",
    "\n",
    "GMMs are widely used in **clustering**, **anomaly detection**, and **density modeling**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deed4c0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 17** - Regression I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71913440",
   "metadata": {},
   "source": [
    "- regression \n",
    "- bayesian regression\n",
    "- linear regression ( homoschedastic , SciKit.LinearRegression())\n",
    "- polynomial regressione\n",
    "- basis regression\n",
    "- kernel regression / nadara - watson regression\n",
    "- over / under fitting - CrossValidation for the best model\n",
    "-  andamento RMS or BIC Vs degree polinomial fitting\n",
    "- learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c23571",
   "metadata": {},
   "source": [
    "### **Regression (What is it?)**\n",
    "\n",
    "Regression is the **supervised** process that try to find the relation between x and y.\n",
    "\n",
    "That is, for a given $x$, instead of trying to estimate the **full probability distribution function (PDF)** of $y$, we often settle for a **point estimate** — the most likely expected value.\n",
    "\n",
    "Crudely: regression = **curve fitting**: finding the best function that explains the observed data.\n",
    "\n",
    "In contrast with **unsupervised learning** (like clustering), regression **requires labeled data** — pairs of $(x_i, y_i)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Frequentist VS Bayesian Regression** (Extra)\n",
    "\n",
    "In **frequentist regression** (like least squares), we try to find **one best-fit line** through the data.\n",
    "\n",
    "But in **Bayesian regression**, we don’t just pick one line — we look at **many possible lines**, and figure out how likely each one is.\n",
    "\n",
    "How it works:\n",
    "\n",
    "- We **start with a belief** (called a **prior**) about what the model could look like.\n",
    "- Then we **update that belief** using the data we observe (this gives us the **posterior**).\n",
    "- The result is a **range of possible models**, not just one.\n",
    "\n",
    "What makes it special:\n",
    "\n",
    "- It gives **probabilistic predictions** — we get a prediction *and* how uncertain it is.\n",
    "- It’s **regularized by priors**, meaning it avoids overfitting by starting with assumptions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Regression (Homoscedastic)**\n",
    "\n",
    "This models the response $y$ as a **linear function of inputs**:\n",
    "\n",
    "$$\n",
    "y = \\theta_1 x + \\theta_0 + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ is a noise term assumed to have **constant variance** \n",
    "\n",
    "Each data point restricts the set of plausible lines in parameter space $(\\theta_0, \\theta_1)$. As more points are added, the intersection of these constraints narrows.\n",
    "\n",
    "### **Linear Regression (Heteroscedastic)**\n",
    "\n",
    "This still models the response $y$ as a **linear function of inputs**:\n",
    "\n",
    "$$\n",
    "y = \\theta_1 x + \\theta_0 + \\epsilon\n",
    "$$\n",
    "\n",
    "But now, the **variance of the noise** is **not constant** across all data points — this is called **heteroscedasticity**:\n",
    "\n",
    "This means that:\n",
    "\n",
    "- Some data points are more \"reliable\" (lower variance).\n",
    "- Others are noisier and should influence the model **less**.\n",
    "- The model should **give different weights** to different data points when fitting.\n",
    "\n",
    "If the errors are different for each point, it is better to think of the problem in matrix notation:\n",
    "\n",
    "$$\n",
    "Y = M \\theta\n",
    "$$\n",
    "\n",
    "where $Y$ is an $N$-dimensional vector of values $y_i$,\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "\\vdots \\\\\n",
    "y_{N-1}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For the straight line model, $\\theta$ is simply a two-dimensional vector of regression coefficients,\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and $M$ is called the design matrix\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "1 & x_0 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N-1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where the constant in the first column of $M$ captures the zeropoint (i.e. the constant $y$-intercept) in the regression.\n",
    "\n",
    "If we encapsulate uncertainties in the ($N\\times N$) covariance matrix\n",
    "\n",
    "$$C=\\left[\n",
    "        \\begin{array}{cccc}\n",
    "        \\sigma_{0}^2 & 0 & \\cdots & 0 \\\\\n",
    "        \\vdots & \\vdots & \\cdots & \\vdots \\\\\n",
    "        0 & 0 & \\cdots & \\sigma_{N-1}^2 \\\\\n",
    "        \\end{array}\n",
    "    \\right].\n",
    "$$\n",
    "\n",
    "The maximum likelihood solution for the regression can be analytically solved and expressed as\n",
    "\n",
    "$$\\theta = (M^T C^{-1} M)^{-1} (M^T C^{-1} Y),$$\n",
    "\n",
    "which minimizes the sum of squares, and gives uncertainties on $\\theta$ of \n",
    "\n",
    "$$\\Sigma_\\theta = \\left[\n",
    "                    \\begin{array}{cc}\n",
    "                    \\sigma_{\\theta_0}^2 & \\sigma_{\\theta_0\\theta_1} \\\\\n",
    "                    \\sigma_{\\theta_0\\theta_1} & \\sigma_{\\theta_1}^2\n",
    "                    \\end{array}\n",
    "                  \\right]\n",
    "                    = [M^T C^{-1} M]^{-1}.\n",
    "$$\n",
    "\n",
    "### **Multivariativ linear regressione**\n",
    "\n",
    "It's simply as befor, but instead of have only 2 dimension x and y, you can add more variable, such as y = ax + bz + ck + ...\n",
    "Of course a,b,c are achived from the **designed matrix**.\n",
    "\n",
    "The design matrix, $M$, is now :\n",
    "\n",
    "$$\n",
    "  M =\n",
    "  \\begin{bmatrix}\n",
    "  1 & x_0 & z_0 & k_0 & \\cdots \\\\\n",
    "  1 & x_1 & z_1 & k_1 & \\cdots \\\\\n",
    "  \\vdots & \\vdots & \\vdots & \\vdots & \\cdots \\\\\n",
    "  1 & x_{N-1} & z_{N-1} & k_{N-1} & \\cdots\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Polynomial Regression**\n",
    "\n",
    "Extends linear regression by adding polynomial terms:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d + \\epsilon\n",
    "$$\n",
    "\n",
    "This is still **linear in parameters**, just not linear in $x$.\n",
    "\n",
    "- More expressive models\n",
    "- Risk of **overfitting** for large degree $d$\n",
    "\n",
    "In this case the design matrix became:\n",
    "\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 & x_0^3 \\\\\n",
    "1 & x_1 & x_1^2 & x_1^3 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_{N-1} & x_{N-1}^2 & x_{N-1}^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Basis Function Regression**\n",
    "\n",
    "We use arbitrary **basis functions** $\\phi_j(x)$: Polynomial, Gaussian , ln , sin e cos ...\n",
    "\n",
    "By choosing a suitable basis, you can fit almost any shape.\n",
    "\n",
    "**So regression with arbitrary basis terms that may be non-linear in $x$ is still linear regression, since the model is linear in the regression parameters.**\n",
    "\n",
    "It is linear in the parameters, not necessarily in the inputs\n",
    "\n",
    "---\n",
    "\n",
    "### **Kernel Regression / Nadaraya-Watson Estimator**\n",
    "\n",
    "If we instead placed Gaussians at the location of every data point, we get Gaussian Kernel Regression. Or just Kernel Regression more generally since we don't have to have a Gaussian kernel function. It is also called Nadaraya-Watson regression.\n",
    "\n",
    "You don’t fit a fixed model (like a polynomial), instead, you smooth the data by giving more weight to nearby points. This smooths the data without fitting a fixed global model.\n",
    "\n",
    "Of course you will find the perfect banwidth by using Cross Validation\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/kernelregression.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "See the table at the end of lecture 18\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting / Underfitting — Cross-Validation**\n",
    "\n",
    "- **Underfitting**: Model too simple → can't capture patterns\n",
    "- **Overfitting**: Model too complex → memorizes noise\n",
    "\n",
    "Use **cross-validation** to estimate model performance:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "```\n",
    "Cross-validation-Score helps select:\n",
    "- Best model complexity (e.g., polynomial degree)\n",
    "- Regularization parameters\n",
    "\n",
    "---\n",
    "\n",
    "### **RMS or BIC vs Polynomial Degree**\n",
    "\n",
    "More regression coefficients improve the ability of the model to fit all the points (reduced bias), but at the expense of model complexity and variance. Of course we can fit a Nth-degree polynomial to N data points, but that would be foolish. We'll determine the best trade-off between bias and variance through cross-validation.\n",
    "\n",
    "When we increase the complexity of a model, the data points fit the model more and more closely. However, this process does not necessarily result in a better fit to the data. Rather, if the degree is too high, then we are overfitting the data. The model has high variance, meaning that a small change in a training point can change the model dramatically.\n",
    "\n",
    "We can evaluate this using a **training set, a cross-validation set and a test set**.\n",
    "\n",
    "Why do we need both a training set and a cross-validation set?* \n",
    "- The **model parameters, are learned from the training set**,\n",
    "- But the **\"hyperparameters\" (in this case the model degree) are learned from the cross-validation set**. \n",
    "- The test set then provides the best estimate of the error expected for a new set of unlabeled data.\n",
    "\n",
    "Plotting **RMS or BIC vs polynomial degree** for both the CV set and training set can help choose the optimal degree — where adding complexity no longer improves performance.\n",
    "\n",
    "#### Root Mean Squared Error (RMS):\n",
    "\n",
    "$$\n",
    "\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "#### Bayesian Information Criterion (BIC):\n",
    "\n",
    "$$\n",
    "\\text{BIC} = k \\ln(n) - 2 \\ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "- $k$: number of parameters  \n",
    "- $n$: number of observations  \n",
    "- $\\hat{L}$: maximum likelihood  \n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/RMS-poldeg.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### Interpretation of RMS and BIC\n",
    "\n",
    "- For low order, both the training and CV error are high. This is sign of a high-bias model that is underfitting the data.  \n",
    "- For high order, the training error becomes small (by definition), but the CV error is large. This is the sign of a high-variance model that is overfitting the data.  \n",
    "- The BIC has similar results.  \n",
    "- We'd like to minimize the RMS or BIC, and the minimum should be the same.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Curves**\n",
    "\n",
    "Learning curves show **train vs validation error** as the dataset size increases.\n",
    "\n",
    "Key patterns:\n",
    "- **High bias**: both train and val errors are high → increase model complexity\n",
    "- **High variance**: large gap between train and val → add more data or regularize\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv=5)\n",
    "```\n",
    "\n",
    "Plot to diagnose model behavior and data sufficiency. We can see two regimes:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/training.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "- The training and CV errors have converged. This indicates that the model is dominated by bias. Increasing the number of training points is futile. If the error is too high, you instead need a more complex model, not more training data.\n",
    "- The training error is smaller than the CV error. This indicates that the model is dominated by variance. Increasing the number of training points may help to improve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd232ae5",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 18** - Regression II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9c00c",
   "metadata": {},
   "source": [
    "- Regularization\n",
    "- ridge regression\n",
    "- LASSO regularization\n",
    "- difference and similitude ridge / LASSO \n",
    "- Locally linear Regression (LOWESS / LOESS) (overwiev)\n",
    "- Non - linear regression (overwiev)\n",
    "- Gaussian process regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cadf3e",
   "metadata": {},
   "source": [
    "### **Regularization**\n",
    "\n",
    "When we build complex models—like using high-degree polynomials—they might fit the training data *too* well. This is called **overfitting**: the model doesn’t just learn the underlying patterns, but also the noise and random fluctuations in the data. As a result, it performs very well on the training set but poorly on new, unseen data.\n",
    "\n",
    "**Regularization** is a technique to prevent overfitting. It works by adding a **penalty term** to the learning process, which discourages the model from becoming overly complex. This helps the model stay simpler and generalize better to new data.\n",
    "\n",
    "The regularization penalty can target different aspects of the model, such as:\n",
    "- Limiting the **magnitude of coefficients**,\n",
    "- Reducing the **number of active features**,\n",
    "- Enforcing **smoothness** in the learned function.\n",
    "\n",
    "The goal is to avoid models that are too flexible and prone to memorizing data, encouraging instead those that capture the true structure without overfitting.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Ridge regression tackles overfitting by adding a penalty on the *squared size* of the coefficients (parameters). The loss function it tries to minimize becomes:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{ridge}} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p \\theta_j^2\n",
    "$$\n",
    "\n",
    "- The first part measures how well the model fits the data.\n",
    "- The second part (with **$\\lambda$**) **penalizes large coefficients** to prevent overly complex models. Because if the parameters are too high, you can think that the function need to change a lot among point, that's overfitting.\n",
    "- $\\lambda$ in known as regularization parameter\n",
    "\n",
    "Key points:\n",
    "- **Coefficients get smaller** but don’t become exactly zero.\n",
    "- Good when many features contribute but might be correlated.\n",
    "- Keeps all features in the model but controls their impact.\n",
    "\n",
    "---\n",
    "\n",
    "### **LASSO Regression (L1 Regularization)**\n",
    "\n",
    "LASSO adds a penalty based on the *absolute value* of the coefficients:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{lasso}} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p |\\theta_j|\n",
    "$$\n",
    "\n",
    "What makes LASSO special:\n",
    "- It can shrink some coefficients **exactly to zero**, effectively removing those features from the model.\n",
    "- This means LASSO does **feature selection** automatically.\n",
    "- Useful when you expect only a few important features out of many.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ridge vs. LASSO: Similarities and Differences**\n",
    "\n",
    "| Feature                 | Ridge                            | LASSO                               |\n",
    "|-------------------------|---------------------------------|-----------------------------------|\n",
    "| Penalty type            | Squares of coefficients ($L_2$) | Absolute values of coefficients ($L_1$) |\n",
    "| Feature selection       | No                              | Yes (some coefficients become zero) |\n",
    "| Effect on coefficients  | Shrinks smoothly towards zero   | Produces sparse solutions (some zero exactly) |\n",
    "| Best use case           | When many features matter, even if correlated | When only a few features are really important |\n",
    "\n",
    "Setting $\\lambda = 0 $ is mathemathicall identical to no regularizaion. But that's not necessarily true in the scikit-learn implementation: i.e. Ridge and Lasso with lambda =0 might not give the same result of LinearRegression. The regularization algorithms have additional sophistications to improve convergence. \n",
    "\n",
    "**How do we choose $\\lambda$?**\n",
    "We use cross-validation, just as we discussed before. In fact...Scikit-Learn has versions of Ridge and LASSO regression that do this automatically for you-- see RidgeCV and LassoCV.\n",
    "\n",
    "---\n",
    "\n",
    "### **Math and geometry**\n",
    "\n",
    "The nice thing about regularization is that **we don’t have to manually choose which features to keep or reduce**. The model learns that from the data by penalizing overly large or unnecessary parameters.\n",
    "\n",
    "Regularization works by adding a **penalty** to the model’s loss function, usually written like this:\n",
    "\n",
    "$$\n",
    "\\lambda \\sum_{i=1} |\\theta_i|^q\n",
    "$$\n",
    "\n",
    "- Here, $\\theta_i$ are the model’s parameters.\n",
    "- $\\lambda$ controls how strong the penalty is.\n",
    "- The exponent $q$ changes the **type of regularization**:\n",
    "  - If $q = 2$, we get **Ridge regression**.\n",
    "  - If $q = 1$, we get **LASSO regression**.\n",
    "\n",
    "So Ridge and LASSO are really the same idea with different norms—they just penalize parameters in different ways. This difference is what changes the *shape of the constraint region* in parameter space.\n",
    "\n",
    "\n",
    "This constraint defines a region in parameter space that the model’s solution must stay inside:\n",
    "\n",
    "- For RIDGE ($q=2$), this region is a **circle** (or a sphere in higher dimensions).\n",
    "- For LASSO ($q=1$), it becomes a **diamond** (or a polyhedron in higher dimensions).\n",
    "\n",
    "These shapes are important because they affect *where* the best-fit parameters are allowed to land.\n",
    "\n",
    "Look at this diagram:\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/regularization.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "- The contours are ellipses representing the usual (unregularized) loss function.\n",
    "- The circle (Ridge) or diamond (LASSO) represents the constraint.\n",
    "\n",
    "The **solution** is where these two shapes touch. For Ridge, the circle tends to touch the contour *between* axes, so all $\\theta_i$ remain nonzero.  \n",
    "But with LASSO, the diamond often touches the contour **right on an axis**, which makes **some parameters exactly zero**. This is why LASSO is great for feature selection.\n",
    "\n",
    "So, **changing the regularization type changes the geometry**, which changes the behavior of the model in a very concrete way.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Locally Linear Regression (LOWESS / LOESS)** (Overview)\n",
    "\n",
    "LOWESS and LOESS are simple ways to make a smooth curve through data without assuming a fixed formula.\n",
    "\n",
    "How it works:\n",
    "- For each point, it looks at nearby points only.\n",
    "- Gives more importance to points that are closer.\n",
    "- Fits a simple line just to those nearby points.\n",
    "- Does this for every point, making a smooth curve that follows local changes.\n",
    "\n",
    "Why use it?\n",
    "- Very flexible and easy to understand.\n",
    "- Good when the relationship between variables changes in different areas.\n",
    "- Doesn’t force one shape to fit all data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Loca Non-Linear Regression (Overview)**\n",
    "\n",
    "as before, Non-linear regression fits curves or complex shapes to data, not just straight lines.\n",
    "\n",
    "\n",
    "How it works:\n",
    "- Uses trial-and-error methods (like gradient descent) to find the best curve.\n",
    "- It can be tricky to find the best fit and might need good starting guesses.\n",
    "- Useful when data clearly isn’t a straight line.\n",
    "\n",
    "\n",
    "---\n",
    "## Gaussian Process Regression (GPR)\n",
    "\n",
    "**Gaussian Process Regression (GPR)** is a flexible, non-parametric, Bayesian approach to regression. Unlike traditional methods that fit a fixed model (e.g., a line, polynomial, or neural network), GPR treats regression as a **probabilistic inference problem** over functions.\n",
    "\n",
    "\n",
    "#### Key Idea\n",
    "\n",
    "Instead of fitting a single function to data, GPR places a **prior distribution over all possible functions** that could model the data. This prior is a **Gaussian Process (GP)** — a collection of random variables, any finite number of which have a joint Gaussian distribution.\n",
    "\n",
    "When we observe data points, we update our prior using **Bayes' theorem**, obtaining a **posterior distribution over functions** that are consistent with the data.\n",
    "\n",
    "Thus, GPR does **not** produce a single function, but rather a **distribution over plausible functions**. The prediction at a new input is the **mean** of this distribution, and its **variance** gives a measure of uncertainty.\n",
    "\n",
    "\n",
    "#### Intuition\n",
    "\n",
    "1. **Prior**:  \n",
    "   We start by assuming that any smooth function is possible a priori. This is modeled by a multivariate Gaussian distribution over functions, defined by a **kernel function** (or covariance function), such as the RBF (squared exponential).\n",
    "\n",
    "2. **Conditioning**:  \n",
    "   After observing data points, we condition the GP on this data. This restricts the infinite set of possible functions to only those that **pass through (or near)** the observed points, and remain smooth elsewhere.\n",
    "\n",
    "3. **Prediction**:  \n",
    "   To predict the function value at a new input $x$, we compute the **conditional distribution** of $f(x)$ given the observed data. The result is still Gaussian:\n",
    "\n",
    "   $$\n",
    "   p(f(x) \\mid \\vec{x}, \\vec{y}) = \\mathcal{N}(\\mu(x), \\sigma^2(x))\n",
    "   $$\n",
    "\n",
    "   where $\\mu(x)$ is the mean prediction, and $\\sigma^2(x)$ is the predictive variance.\n",
    "\n",
    "\n",
    "#### Mathematical Formulation\n",
    "\n",
    "Let:\n",
    "- $\\vec{x} = [x_1, \\dots, x_N]^T$ be the training inputs (size $N$),\n",
    "- $\\vec{y} = [y_1, \\dots, y_N]^T$ be the corresponding outputs,\n",
    "- $x_*$ be a new test input, this could be extended at M new test point.\n",
    "\n",
    "We assume:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "\\vec{y} \\\\\n",
    "f(x_*)\n",
    "\\end{bmatrix}\n",
    "\\sim \\mathcal{N} \\left(\n",
    "\\vec{0}, \n",
    "\\begin{bmatrix}\n",
    "K_{11} & K_{12} \\\\\n",
    "K_{12}^T & K_{22}\n",
    "\\end{bmatrix}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $K_{11}$ is the $N \\times N$ covariance matrix between training points,\n",
    "- $K_{12}$ is the $N \\times 1$ covariance between training and test point, (of course if I had M points, it will be a mtrim N X M )\n",
    "- $K_{22}$ is the scalar covariance between test point and itself (i.e., $k(x_*, x_*)$). (or a matrrix M X M)\n",
    "\n",
    "Then the **posterior predictive distribution** at $x_*$ is:\n",
    "\n",
    "$$\n",
    "p(f(x_*) \\mid \\vec{x}, \\vec{y}) = \\mathcal{N}(\\mu_*, \\sigma_*^2)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- **Mean**:\n",
    "  $$\n",
    "  \\mu_* = K_{12}^T K_{11}^{-1} \\vec{y}\n",
    "  $$\n",
    "\n",
    "- **Variance**:\n",
    "  $$\n",
    "  \\sigma_*^2 = K_{22} - K_{12}^T K_{11}^{-1} K_{12}\n",
    "  $$\n",
    "\n",
    "This gives both the **best guess** (mean) and a **confidence estimate** (variance) for the prediction.\n",
    "\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- GPR provides **probabilistic predictions**: each predicted point has a mean and an associated uncertainty.\n",
    "- The method is **non-parametric**: complexity grows with the data but there are no fixed parameters to learn.\n",
    "- **Kernel functions** encode assumptions about the function's smoothness and structure. a common choices is the RBF\n",
    "- GPR performs **Bayesian inference over functions**, which gives it flexibility and interpretability, especially when data is sparse.\n",
    "\n",
    "\n",
    "#### Advantages\n",
    "\n",
    "- Provides **uncertainty estimates** naturally.\n",
    "- Works well even with small datasets.\n",
    "- Very interpretable: predictions come with confidence intervals.\n",
    "- Easy to incorporate **prior knowledge** via kernel design.\n",
    "\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Computationally expensive: training involves inverting an $N \\times N$ matrix ($\\mathcal{O}(N^3)$ complexity).\n",
    "- Doesn’t scale well to large datasets without approximations.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/GPR.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "#### **Differences KR, KDE and GPR**\n",
    "\n",
    "\n",
    "| Feature                     | **Kernel Density Estimation (KDE)**             | **Kernel Regression (KR)**                      | **Gaussian Process Regression (GPR)**              |\n",
    "|-----------------------------|--------------------------------------------------|--------------------------------------------------|----------------------------------------------------|\n",
    "| **Task**                    | Density estimation                               | Regression (point estimate)                      | Regression (Bayesian, full posterior)              |\n",
    "| **Output**                  | Probability density function                     | A single predicted value                         | Predictive mean **+ uncertainty (variance)**       |\n",
    "| **Type of model**           | Non-parametric, deterministic                    | Non-parametric, deterministic                    | Non-parametric, probabilistic                      |\n",
    "| **Uses a kernel to...**     | Smooth and estimate data density                 | Weight nearby training targets                   | Define prior covariance between function values    |\n",
    "| **Prediction is...**        | Estimate of density at a given point             | Weighted average of outputs                      | Posterior distribution over functions              |\n",
    "| **Uncertainty estimation**  | ❌ Not provided                                   | ❌ Not provided                                   | ✅ Predictive variance at each test point           |\n",
    "| **Hyperparameter tuning**   | Kernel Bandwidth                                  | Kernel bandwidth                                | Kernel + noise parameters   |\n",
    "| **Mathematical foundation** | Histogram smoothing       | Nadaraya–Watson estimator                        | Bayesian inference over function space             |\n",
    "| **Common use cases**        | Estimate $p(x)$                                  | Estimate $y = f(x)$ given samples                | Estimate $y = f(x)$ + uncertainty                  |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1456ea0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 19** - Classification I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d96d6",
   "metadata": {},
   "source": [
    "- Generative VS Discriminative classification (differences and main concept)\n",
    "- performance of classifiers\n",
    "- Generative classification (discriminant function, bayes classifier, decision boundary)\n",
    "- Naive bayes\n",
    "- gaussian naive bayes\n",
    "- linear e quadratic discriminat analysis\n",
    "-  GMM and bayes classification\n",
    "- K - nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6ceab",
   "metadata": {},
   "source": [
    "### **Generative vs. Discriminative Classification**\n",
    "\n",
    "KDE is a unsupervised form of classification, like clustering, we want to look now at supervised one.\n",
    "There are two different type:\n",
    "\n",
    "- **Generative classification** :\n",
    "    If we find ourselves asking which category is most likely to generate the observed result, then we are using **density estimation** for classification and this is referred to as **generative classification**. Here we have a full model of the density for each class or we have a model which describes how data could be generated from each class. \n",
    "\n",
    "- **Discriminative classification** :\n",
    "    if we don't care about the full distribution, then we are doing something more like clustering, where we don't need to map the distribution, we just need to define boundaries.  Classification that finds the **decision boundary** that separates classes is called **discriminative classification**. \n",
    "\n",
    "For example, in the figure below, to classify a new object, it would suffice to know:\n",
    "1. model 1 is a better fit than model 2 (***generative classification***), or \n",
    "2. that the decision boundary is at $x=1.4$ (***discriminative classification***).\n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance of Classifiers**\n",
    "\n",
    "- **Confusion matrix** (binary case):\n",
    "    - **True Positive** = **correctly identified**  = TP\n",
    "    - **True Negative** = **correctly rejected**  = TN\n",
    "    - **False Positive** = **incorrectly identified** = FP \n",
    "    - **False Negative** = **incorrectly rejected** = FN\n",
    "\n",
    "- **Metrics**:\n",
    "  - **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "    The proportion of all correct predictions (both positive and negative) out of the total number of predictions.\n",
    "\n",
    "  - **Completeness**: $\\frac{TP}{TP + FN}$\n",
    "\n",
    "    Out of all the actual positives, how many did the model correctly identify?\n",
    "\n",
    "  - **Contamination** :  $\\frac{FP}{TP + FP}$\n",
    "\n",
    "    fraction of false inside the true\n",
    "\n",
    "  - **Precision**: $\\frac{TP}{TP + FP}$\n",
    "\n",
    "    Out of all the instances the model predicted as positive, how many were actually positive?\n",
    "     - Precision = 1 - Contamination\n",
    "\n",
    "   - **F1-score**: harmonic mean of precision and completness\n",
    "\n",
    "---\n",
    "\n",
    "### **ROC Curve**\n",
    "\n",
    "The **ROC curve** (Receiver Operating Characteristic) is a fundamental tool to evaluate the performance of a **binary classifier**.\n",
    "\n",
    "It plots the **True Positive Rate** (TPR) against **False Positive Rate** (FPR) \n",
    "\n",
    "\n",
    "####  What the ROC Curve Tells\n",
    "\n",
    "- A classifier that performs perfectly will have a point at the **top-left** corner of the plot: TPR = 1, FPR = 0.\n",
    "- A **random classifier** will produce a diagonal line from (0,0) to (1,1).\n",
    "- The more the ROC curve bows **toward the top-left**, the better the classifier is.\n",
    "- The **Area Under the Curve (AUC)** is a single metric summarizing the model's ability to distinguish classes:\n",
    "  - AUC = 1 → perfect classification  \n",
    "  - AUC = 0.5 → random guessing  \n",
    "  - AUC < 0.5 → worse than random (possibly inverted predictions)\n",
    "\n",
    "\n",
    "You can also plot the **completeness** (how many true events you catch) against **efficiency** (how many of your selected events are actually correct) is useful when:\n",
    "\n",
    "- You care about making **accurate selections**, not just catching everything.\n",
    "- **False positives** are a problem (e.g. limited resources to follow up wrong detections).\n",
    "- You want to find a good **balance between catching as much as possible** and **keeping the results clean**.\n",
    "\n",
    "This plot helps you **visually choose a threshold** that gives you the best trade-off between detecting true cases and avoiding wrong ones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Generative Classification**\n",
    "\n",
    "Generative classification is a probabilistic approach that models **how the data is generated**. The key idea is to estimate the distribution of features for each class and then apply **Bayes’ theorem** to make predictions. This contrasts with discriminative methods, which model the decision boundary directly.\n",
    "\n",
    "Before we get into the generative classification algortithms, we'll first discuss 3 general concepts:\n",
    "\n",
    "1. ####  Discriminant Function\n",
    "\n",
    "We can relate classification to regression: in regression we estimate a function $f(x)$ to predict continuous values.  \n",
    "In classification, we do the same—but $y$ is discrete, e.g., $y \\in \\{0, 1\\}$.\n",
    "\n",
    "So we define a **discriminant function** $g(x)$ that gave us the probability of class membership:\n",
    "$$\n",
    "g(x) \\in (0,1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "2. ####  Bayes Classifier\n",
    "\n",
    "The **Bayes classifier** uses the discriminant function to make decisions and prediction under uncertainty. For binary classification, it works as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } g(x) > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This rule assigns a new input $x$ to the class with the **highest posterior probability**.\n",
    "\n",
    "\n",
    "3. ####  Decision Boundary\n",
    "\n",
    "The **decision boundary** is the set of all $x$ where the classifier is **uncertain**—i.e., where the posterior probabilities of two (or more) classes are equal.\n",
    "\n",
    "For the binary case:\n",
    "$$\n",
    " g(x)= 1/2\n",
    "$$\n",
    "\n",
    "This defines the surface (or line, or point) in the input space where the predicted class changes. In generative models, this boundary results from the underlying class distributions.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Naive Bayes**\n",
    "\n",
    "Naive Bayes is one of the simplest and most effective classification algorithms.  \n",
    "It is called “naive” because it assumes **conditional independence** between features given the class.\n",
    "\n",
    "\n",
    "**Make a pause:** a feature here is a single information for each element of the dataset, for example if you are searching for spam - email, a possible feature is \"how many times compare the world \"free\" in the email ?\".\n",
    "Each feature is indipendent from all the other in this approach.\n",
    "\n",
    "\n",
    "Formally, the model works in this way:\n",
    "$$\n",
    "P(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "where $C_k$ is the $k$-th class, and $x_i$ are the individual features of input $\\mathbf{x}$.\n",
    "\n",
    "This means that once we know the class, the model calculate the probability for each feature to be in the class and then multiply all of them.\n",
    "\n",
    "- Since Naive Bayes is a **supervised learning** method, we assume that we already know the true class labels in the training set.  \n",
    "- We use these to estimate the distributions $P(x_i \\mid C_k)$ and the priors $P(C_k)$,  \n",
    "- and then apply **Bayes’ theorem** to predict the class of new, unseen data.\n",
    "\n",
    "####  Pros:\n",
    "- Simple to implement\n",
    "- Very fast, both in training and prediction\n",
    "- Works surprisingly well even when the independence assumption is violated\n",
    "- Performs well on high-dimensional data\n",
    "\n",
    "---\n",
    "\n",
    "### **Gaussian Naive Bayes**\n",
    "\n",
    "Gaussian Naive Bayes is a variant of Naive Bayes used when the features are **continuous-valued** (i.e., real numbers rather than categories or binary values).\n",
    "\n",
    "Instead of estimating probabilities using histograms or counts, we assume that each feature $x_i$ follows a **Gaussian distribution** within each class $C_k$:\n",
    "$$\n",
    "P(x_i \\mid C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left( -\\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2} \\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\mu_k$ is the **mean** of feature $x_i$ for class $C_k$\n",
    "- $\\sigma_k^2$ is the **variance** of that feature within the class\n",
    "\n",
    "\n",
    "This model keeps the **Naive Bayes assumption** that all features are conditionally independent given the class:\n",
    "$$\n",
    "P(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "Then, we use **Bayes' theorem** to compute the posterior probability of each class and choose the most likely one.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear and Quadratic Discriminant Analysis (LDA / QDA)**\n",
    "\n",
    "Both **Linear Discriminant Analysis (LDA)** and **Quadratic Discriminant Analysis (QDA)** are **generative classification models** that assume each class follows a **Gaussian distribution**.  \n",
    "They differ in how they treat the **covariance matrix**, which controls the shape and orientation of the Gaussian.\n",
    "\n",
    "\n",
    "####  Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Assumes the data for each class is normally distributed:\n",
    "  $$\n",
    "  \\mathbf{x} \\mid C_k \\sim \\mathcal{N}(\\mu_k, \\Sigma)\n",
    "  $$\n",
    "- All classes **share the same covariance matrix** $\\Sigma$\n",
    "- Class-specific means $\\mu_k$ are different\n",
    "- Because of the shared $\\Sigma$, the **decision boundary is linear**\n",
    "\n",
    "This means the model separates the classes with straight lines (or hyperplanes in higher dimensions).\n",
    "67\n",
    "\n",
    "####  Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "- Still assumes Gaussian distributions:\n",
    "  $$\n",
    "  \\mathbf{x} \\mid C_k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)\n",
    "  $$\n",
    "- But now **each class has its own covariance matrix** $\\Sigma_k$\n",
    "- This allows more flexibility in the shape of each class distribution\n",
    "- As a result, the **decision boundaries are quadratic curves**\n",
    "\n",
    "QDA is more powerful than LDA but also requires estimating more parameters, so it needs more data to avoid overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gaussian Mixture Models (GMM) and Bayes Classification**\n",
    "\n",
    "So far, our generative classifiers have relied on fairly **strong assumptions**, such as:\n",
    "- **Conditional independence** of features (Naive Bayes)\n",
    "- **Single Gaussian distribution** per class (LDA/QDA)\n",
    "\n",
    "These models work well, but they may fail when the true data distribution is more complex.\n",
    "\n",
    "\n",
    "####  Bayes Classification with GMM\n",
    "\n",
    "A more **flexible and expressive** approach is to use **Gaussian Mixture Models** to represent the class-conditional distributions.\n",
    "\n",
    "Instead of assuming that each class is modeled by a single Gaussian, we assume it is a **mixture of multiple Gaussians**.\n",
    "\n",
    "This allows us to **model complex, multimodal distributions** for each class.\n",
    "\n",
    "This methode is called GMM Bayes Classifier\n",
    "\n",
    "####  Why use GMMs?\n",
    "\n",
    "- More expressive than a single Gaussian\n",
    "- Can model **non-linear, complex class boundaries**\n",
    "- Especially useful when data exhibits **clusters within each class**\n",
    "\n",
    "#### Considerations\n",
    "\n",
    "- Requires choosing the number of components $K$\n",
    "- Can be computationally expensive\n",
    "- Risk of **overfitting** with too many components\n",
    "- **NOTE:** We can take this to the extreme by having one mixture component at each training point. We also don't have to restrict ourselves to a Gaussian kernel, we can use any kernel that we like. The resulting ***non-parametric*** Bayes classifier is referred to as **Kernel Discriminant Analysis (KDA)**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Nearest Neighbor Classifier** \n",
    "\n",
    "- **Non-parametric** method (it means that it doesn't imply a structur on the data)\n",
    "- Classify new point $\\mathbf{x}$ by majority vote among its $k$ nearest neighbors\n",
    "- A large choice of K decrease the variance in the classification but increase the bias\n",
    "- you can pick the best K by cross - validation, with the intent to reduce the error\n",
    "\n",
    "Pros:\n",
    "- No training needed\n",
    "- Simple\n",
    "\n",
    "Cons:\n",
    "- Expensive at test time\n",
    "- Choice of $k$ and distance metric affects performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a68d1",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 20** - Classification II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e5151",
   "metadata": {},
   "source": [
    "- Discriminative Classification\n",
    "- Logistic regression\n",
    "- Support Vector Machines\n",
    "- kernel method\n",
    "- Decision tree\n",
    "- splitting criteria\n",
    "- ensemble learning\n",
    "- bagging\n",
    "- Random forest\n",
    "- Boosting\n",
    "- what should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47090d0b",
   "metadata": {},
   "source": [
    "### **Discriminative Classification & Advanced Models**\n",
    "\n",
    "Discriminative classifiers do not model how data “came to be” (that's generative classification), but instead **directly learn** the mapping from inputs $x$ to labels $y$.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Logistic Regression**\n",
    "\n",
    "- Logistic Regression models the **posterior probability** directly:\n",
    "  \n",
    "  $$\n",
    "  P(y = 1 \\mid \\mathbf{x}) = \\frac{1}{1 + \\exp(-(\\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbf{x}))} \\text{= called logistic function}\n",
    "  $$\n",
    "\n",
    "- This is equivalent to modeling the **log-odds** as a linear function of the inputs:\n",
    "\n",
    "  $$\n",
    "  \\log\\left(\\frac{P(y=1 \\mid \\mathbf{x})}{P(y=0 \\mid \\mathbf{x})} \\right) = \\beta_0 + \\boldsymbol{\\beta}^\\top \\mathbf{x}\n",
    "  $$\n",
    "\n",
    "- The coefficients $\\beta$ are chosen to **minimize the cross-entropy loss** (also known as negative log-likelihood)\n",
    "- This makes logistic regression a **discriminative model**, since it directly models $ P(y | x)$ without any assumptions on the distribution of x.\n",
    "\n",
    "The only difference between LDA and Logstic Regression is how the regression coefficients are estimated:\n",
    "- **Logistic Regression**:  \n",
    "  Finds the boundary by **minimizing classification error** (via cross-entropy), without making assumptions about the input distribution.\n",
    "\n",
    "- **LDA**:  \n",
    "  Finds the boundary by **fitting a probabilistic model** of the input data for each class (Gaussian), then applying **Bayes' Rule**. The coefficients come from **density estimation**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Support Vector Machines (SVM)**\n",
    "\n",
    "define a hyperplane (a plane in $N-1$ dimensions) that maximizes the distance of the closest point from each class. This distance is the \"margin\". The points that touch the margin (or that are on the wrong side) are called **support vectors**. \n",
    "\n",
    "**What happen if the data have some overlap between the classes?**\n",
    "\n",
    "When this happens, a **hard-margin SVM** (which requires perfect separation) won’t work.  \n",
    "That’s why we use the **soft-margin SVM**, which allows some mistakes but still tries to find the best separating hyperplane.\n",
    "\n",
    "- The model introduces a **slack variables** that allow some points to be:\n",
    "  - Inside the margin\n",
    "  - Or even misclassified\n",
    "- The optimization now balances two goals:\n",
    "  1. **Maximize the margin**\n",
    "  2. **Minimize the total slack** (how many violations we allow)\n",
    "\n",
    "This is controlled by a **regularization parameter** $C$:\n",
    "- Large $C$ → penalize misclassifications more (tighter margin, less tolerant)\n",
    "- Small $C$ → allow more violations (wider margin, more tolerant)\n",
    "\n",
    "\n",
    "**Some important notion:**\n",
    "1) SVM is not scale invariant, is worth rescaling the mean to 0 and variance of 1\n",
    "2) Once the support vectors are determined, changes to the positions or numbers of points beyond the margin will not change the decision boundary\n",
    "3) Strong resilience to outliers\n",
    "4) This is why there is a high completeness compared to the other methods: it does not matter if the background sources is much bigger. It simply determines the best boundary between the small source clump and the large background clump.\n",
    "5) This completeness, however, comes at the cost of a relatively large contamination level.\n",
    "6) all the SVM methode are imported in a library called SVC (support vector classifier) --> ' clf = SVC(kernel='linear') '\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/SVM.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "### **Kernel Method**\n",
    "\n",
    "If the contamination is driven by non-linear effects, it may be worth implementing a **non-linear decision boundary**. We can do this by **kernelization**.\n",
    "\n",
    "' clf = SVC(kernel='rbf') '\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/svm_circ.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "Another possibility is simply to add a new dimension at the dataset (from 2D to 3D for example), i create the information associated at this dimension, so it could be something that clearly separete the point in different cluster, then i can fin the best plane to separete them (SVM).\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Trees**\n",
    "\n",
    "A **decision tree** is similar to the process of classification that you might do by hand: \n",
    "\n",
    "- define some criteria to separate the sample into 2 groups (not necessarily equal),\n",
    "- then take those sub-groups and do it again.  \n",
    "- keep going until you reach a stopping point such as not having a minimum number of objects to split again.  \n",
    "\n",
    "In short, we have done a hierarchical application of decision boundaries.\n",
    "\n",
    "The tree structure is as follows:\n",
    "- top node contains the entire data set\n",
    "- at each branch the data are subdivided into two child nodes \n",
    "- split is based on a predefined decision boundary (usually axis aligned)\n",
    "- splitting repeats, recursively, until we reach a predefined stopping criteria \n",
    "\n",
    "Build a tree by **recursively splitting** on feature thresholds until leaves are pure or other stopping criteria are met.\n",
    "\n",
    "#### Splitting Criteria\n",
    "\n",
    "At each node, choose feature $j$ and threshold $t$ to minimize impurity.\n",
    "The typical process for finding the optimal decision boundary is to perform trial splits along each feature one at a time, within which the value of the feature to split at is also trialed. The feature that allows for the maximum information gain is the one that is split at this level.\n",
    "\n",
    "- **Gini Impurity:**  \n",
    "  - Gini impurity tells us how mixed the classes are in a group of data. It measures how likely it is to make a wrong guess if we randomly pick a label for an item, based on the class distribution in that group\n",
    "  - Gini is 0 when the node is pure (all samples belong to one class). \n",
    "  - The higher the Gini, the more mixed the classes are. computationally faster than entropy and gives similar results.\n",
    "  $$\n",
    "  G = 1 - \\sum_{k=1}^K p_k^2\n",
    "  $$\n",
    "     where $p_k$ is the fraction of samples of class $k$ in the node.\n",
    "  \n",
    "- **Entropy (Information Gain):**  \n",
    "  - Entropy quantifies the amount of uncertainty (or impurity) in a node. \n",
    "  - It comes from information theory. \n",
    "  - Entropy is 0 when the node is pure. \n",
    "  - If entropy drops significantly after a split, it means we’ve made a good separation.\n",
    "  $$\n",
    "  H = -\\sum_{k=1}^K p_k \\log p_k\n",
    "  $$\n",
    "    \n",
    "\n",
    "**Obviously** in constructing a decision treee, if your choice of stopping criteria is too loose, further splitting just ends up adding noise.  So using cross-validation in order to optimize the depth of the tree (and to avoid overfitting) is the best choice.\n",
    "\n",
    "---\n",
    "\n",
    "## **Ensemble Learning**\n",
    "\n",
    "Combine multiple “weak learners” (the method seen before) to form a stronger model.\n",
    "\n",
    "#### **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "- **bootstrap** the samples (with replacement) of size $N$.\n",
    "- Train an independent tree on each sample.\n",
    "- **Aggregate** by majority vote (classification) or average (regression).\n",
    "\n",
    "Reduces **variance** without increasing bias.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $B$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator for $\\hat{y}$, defined by bagging, is then\n",
    "\n",
    "$$\\hat{y} = f(x) = \\frac{1}{B} \\sum_i^B f_i(x).$$\n",
    "\n",
    "**NOTE** : you can put the parameter `n_jobs=-1`; That says to use all the cores of your machine to do the job.  That's one of the benefits of bagging.  It can be made parallel trivially -- one bagging process has nothing to the with the others.  You just average them all together when you are done. \n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "Random forests extend bagging by generating decision trees from the bootstrap samples. In addition to drawing random samples from our training set with replacement, we may also draw random subsets of features for training the individual trees.\n",
    " \n",
    "- In Random Forests, the splitting features on which to generate the tree are selected at random from the full set of features in the data.\n",
    "- The number of features selected per split level is typically the square root of the total number of features, $\\sqrt{D}$. \n",
    "- **The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node.** \n",
    "- The final classification from the random forest is based on the averaging of the classifications of each of the individual decision trees.\n",
    "- As alway use CV to determine the optimal depth of the tree\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/RANDOM_FOREST.webp\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "### **Boosting**\n",
    "\n",
    "**Boosting** is a way to make a strong model by combining **many weak DIFFERENT models** that aren't very good on their own.\n",
    "\n",
    "#### Basic Idea\n",
    "\n",
    "1. Train a simple model.\n",
    "2. See which points it gets wrong.\n",
    "3. Focus more on those hard points next time.\n",
    "4. Repeat this several times.\n",
    "5. At the end, combine all the models to get a better result.\n",
    "\n",
    "Each new model **tries to fix the mistakes** made by the ones before.\n",
    "\n",
    "#### What Makes Boosting Special?\n",
    "\n",
    "- It gives **more importance** to points that are hard to classify.\n",
    "- The final prediction is a **weighted vote** of all the models.\n",
    "- The better a model is, the more say it gets.\n",
    "\n",
    "### **AdaBoost (Adaptive Boosting)**\n",
    "\n",
    "- A popular type of boosting.\n",
    "- After each round, it **boosts** (increases) the weight of the wrong points.\n",
    "- This helps the next model **focus** on the tough stuff.\n",
    "- change the learning rate, where half the learning rate means that weights are boosted half as much for each iteration.\n",
    "\n",
    "#### Downside\n",
    "\n",
    "- Boosting is **slow** because models are built **one after the other**. \n",
    "- Not easy to run in parallel like Random Forests.\n",
    "- Can be sensitive to noisy data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Selection: “What Should I Use?”**\n",
    "\n",
    "| Method                     | Type            | Strengths                                                                 | Weaknesses                                                         |\n",
    "|----------------------------|------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
    "| **Logistic Regression**    | Discriminative   | Fast, interpretable, works well on linearly separable data                | Struggles with nonlinearity                                        |\n",
    "| **SVM (linear / kernel)**  | Discriminative   | Strong margin, effective in high‐dim spaces, kernels for nonlinearity     | Computationally heavy for large $N$                                |\n",
    "| **Decision Tree**          | Discriminative   | Intuitive rules, handles mixed data types                                 | High variance, prone to overfitting                                |\n",
    "| **Random Forest**          | Discriminative   | Excellent off‐the‐shelf performance, handles large feature sets            | Less interpretable, many parameters                                |\n",
    "| **Bagging**                | Discriminative   | Reduces variance of unstable learners                                     | Requires multiple models, less interpretable                       |\n",
    "| **Boosting** | Discriminative | High accuracy, focuses on hard examples                                   | Slower to train, sensitive to noise and overfitting                |\n",
    "| **Naive Bayes**            | Generative       | Very fast, works surprisingly well with many features                     | Assumes feature independence, which is often unrealistic           |\n",
    "| **Gaussian Naive Bayes**   | Generative       | Handles continuous data with a normal distribution assumption             | Poor with non-Gaussian data or correlated features                 |\n",
    "| **LDA (Linear Disc. Analysis)** | Generative | Simple, fast, works well with Gaussian data and equal covariances         | Assumes same covariance matrix across classes                      |\n",
    "| **QDA (Quadratic Disc. Analysis)** | Generative | More flexible than LDA, models class-specific covariance          | Needs more data, prone to overfitting with small samples           |\n",
    "| **GMM + Bayes Classifier** | Generative       | Can model complex, multimodal distributions per class                     | Training (EM algorithm) is slower, sensitive to initialization     |\n",
    "| **K-Nearest Neighbors (KNN)** | Generative| No training needed, simple and intuitive                                  | Slow at test time, sensitive to irrelevant features & scaling      |\n",
    "\n",
    "\n",
    "Naive Bayes and its variants are by far the easiest to compute. Linear support vector machines are more expensive, though several fast algorithms exist. Random forests can be easily parallelized. Boosting helps with challenging classification (but at that point you might want to go all the way to deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb09e8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 21** - Deep Learning I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ad4f8",
   "metadata": {},
   "source": [
    "- Loss function\n",
    "- Gradient Descent\n",
    "- AdaBoost\n",
    "- Neural Networks\n",
    "- In more detail\n",
    "- Backpropagation\n",
    "- About derivative\n",
    "- number of layers\n",
    "- number of neurons\n",
    "- activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ce9f5",
   "metadata": {},
   "source": [
    "### **Loss function**\n",
    "\n",
    "A **loss function** measures how “bad” a model’s prediction is compared to the true value. The smaller the loss, the better your model is doing.\n",
    "\n",
    "It helps guide the **training** of a machine learning model, by telling it *how wrong* its predictions are.\n",
    "\n",
    "\n",
    "#### **Common Loss Functions**\n",
    "\n",
    "#### L2 Loss (Mean Squared Error - MSE)\n",
    "\n",
    "This is typical in **regression problems**:\n",
    "\n",
    "$$\n",
    "L_2 = (y - f(x))^2\n",
    "$$\n",
    "\n",
    "- Squared difference between prediction and actual value.\n",
    "- Penalizes **larger errors more heavily**.\n",
    "- Smooth and easy to differentiate → good for gradient descent.\n",
    "- Assumes Gaussian noise in the data.\n",
    "\n",
    "### L1 Loss (Mean Absolute Error - MAE)\n",
    "\n",
    "Also used in regression:\n",
    "\n",
    "$$\n",
    "L_1 = |y - f(x)|\n",
    "$$\n",
    "\n",
    "- Less sensitive to outliers than L2.\n",
    "- Leads to **sparser** models (used in LASSO).\n",
    "- Less smooth → gradient descent can be trickier.\n",
    "\n",
    "---\n",
    "\n",
    "### **Classification loss function**\n",
    "\n",
    "In **classification**, especially **binary classification**, your true labels are {-1, +1} and your model's prediction is a **score** or **probability**.\n",
    "\n",
    "So we look at:\n",
    "\n",
    "$$\n",
    "y \\cdot f(x)\n",
    "$$\n",
    "\n",
    "- If this is **positive**, your model got the right class ( +1 * +1  or -1 * -1).\n",
    "- If this is **negative**, it got it wrong ( +1 * -1  or -1 * +1).\n",
    "- Larger positive → more confident correct prediction.\n",
    "- More negative → confident wrong prediction.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/LOSS.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f86e0",
   "metadata": {},
   "source": [
    "This function does something reasonable for $y*f(x)\\le1$.  However, look what happens at larger values where we are even more confident that $y*f(x)$ is positive and that our class should be $+1$.  The loss goes **up**.  That's bad.\n",
    "\n",
    "We need a loss function that makes sense for classification.\n",
    "\n",
    "- The first we'll try is the so-called **Zero-One Loss**. It is 1 for $yf(x)<0$ and 0 for $yf(x)>0$; thus the name. You increment the loss function by 1 every time you make a wrong prediction. It is just a count of the total number of mistakes. However, the Zero-One loss is hard to minimize, so instead we can try something that allows the loss to be continuous function in $y*f(x)$.  \n",
    "\n",
    "- The **Hinge Loss**, which looks like $${\\rm max}(0,1-y*f(x)),$$. Here there is no contribution to the loss for values $\\ge 1$, but there is a linearly increasing loss for smaller values. So, it penalizes both wrong predictions and also correct predictions that have low confidence.\n",
    "\n",
    "- A **Logistic Loss** (also called the *log loss* and *cross entropy loss*) function has similar properties as shown in **blue**, but is smoother and has slightly less and less penalty for more and more confident $+1$ predictions.\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/otherLOSS.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient descendent**\n",
    "\n",
    "**Gradient descent** is an optimization algorithm used to find the parameters $\\theta$ that minimize a loss function by iteratively updating them in the direction that reduces the loss, without testing every possible value.\n",
    "\n",
    "There are different ways to do that:\n",
    "- Sometimes we can calculate the best $\\theta$ directly.\n",
    "- Sometimes we try many random values (like in MCMC).\n",
    "- But gradient descent is like standing on a hill and walking downhill until you reach the bottom.\n",
    "\n",
    "We update the parameters using:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "- $\\eta$  is the **learning rate** (step size).\n",
    "- $\\nabla J(\\theta)$  is the slope (gradient) of the cost function.\n",
    "\n",
    "\n",
    "#### Learning rate:\n",
    "\n",
    "- Too small → takes forever to reach the minimum.\n",
    "- Too big → can skip over the minimum or even diverge.\n",
    "\n",
    "#### Other notes:\n",
    "\n",
    "- We start from random values of $\\theta$.\n",
    "- Gradient descent works well when the cost function has only one minimum (like with  L2 loss).\n",
    "- It's also useful when the dataset is too big to load all at once.\n",
    "\n",
    "![boh](https://miro.medium.com/max/1400/0*GaO7X6j3coh3oNwf.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **AdaBoost**\n",
    "\n",
    "Let's see lecture 20\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Network**\n",
    "\n",
    "**Artificial Neural Networks** are a simplified computational architecture based loosely on the real neural networks found in brains.\n",
    "\n",
    "the term \"neural network\" is overused and encompasses a huge variety of deep learning approaches. What we are going to explore is more properly called **multi-layer perceptron**\n",
    "\n",
    "In the image below, \n",
    "- the circles on the **left** represent the **features/attributes** of our input data, $X$, which here is 3 dimensional.  \n",
    "- the circles in the **middle** represent the **neurons**. They take in the information from the input and, based on some criterion decide whether or not to \"fire\". These middle layers are called \"**hidden layers**\".\n",
    "- the collective results of the neurons in the hidden layer produce the **output**, $y$, which is represented by the circles on the **right**, which here is 2 dimensional result.  \n",
    "- the lines connecting the circles represent the synapses.  \n",
    "\n",
    "This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)\n",
    "\n",
    "\n",
    "#### **In more detail**\n",
    "\n",
    "Each **synapse** (connection between neurons) passes a signal from one neuron to the next by performing a simple computation:\n",
    "\n",
    "1. It **multiplies** each input value $x_i$ by a corresponding **weight** $w_i$.\n",
    "2. It **adds** a **bias** term $b$ to the weighted sum.\n",
    "3. The result is:   $z = \\sum_i w_i x_i + b$\n",
    "\n",
    "During training, the network **adjusts the weights $w_i$** and **biases $b$** to minimize the error between the predicted outputs and the actual target values. This process is done using **gradient descent** and **backpropagation**.\n",
    "\n",
    "\n",
    "---\n",
    "### **Backpropagation**\n",
    "\n",
    "After the network makes a prediction, we compare it to the true value to calculate the **error**.\n",
    "\n",
    "**Backpropagation** is the algorithm used to determine **how much each weight contributed to the error**, and in which direction it should change to reduce it.\n",
    "\n",
    "The process involves:\n",
    "- Computing the **gradient** of the loss function with respect to each weight — this tells us how the error changes as each weight changes.\n",
    "- Updating the weights using **gradient descent**, which adjusts them slightly in the direction that reduces the error.\n",
    "\n",
    "This cycle of predicting, measuring error, and updating weights is repeated many times during training, allowing the network to gradually improve its performance.\n",
    "\n",
    "Backpropagation is efficient and far faster than exhaustively testing all possible weight combinations.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **About derivative**\n",
    "\n",
    "As you see, deep learning at the end of the day requires computing a ton of derivatives. Computing numerical derivatives is a tricky business. One could naively approximate\n",
    "\n",
    "$$ f'(x) \\sim \\frac{f(x+ \\Delta x) - f(x)}{\\Delta x} $$\n",
    "\n",
    "for a small value of $\\Delta x$. If you try you'll quickly convince youself that this **is numerically very unstable**.\n",
    "\n",
    "So why is deep learning so successfull and widely adopted? This is largely due to **automatic differentiation**, which is one of the smartest computational idea ever. \n",
    "\n",
    "The key idea is that a computer is a simple machine, even if the software we write can be very complicated. All a computer can do at the chip level are elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). Given a complicated piece of software, one can track the (very long!) lists of elemetary function that are required to reproduce the result. With this list, computing a derivative is just a long but trivial application of the chain rule. In practice, this means that derivates of any oder can be computed accurately to working precision very quickly.\n",
    "\n",
    "The other computational revolution behind the success of deep learning is the exploitation of **Graphic processing units (GPUs)** . CPUs are the standard silicon chips used by all computers, including your laptopt. GPUs are graphic boards designed for expensive video rendering (the market was largely driven by gaming professionals; now it's all AI). It turns out they're extremely good at matrix multiplication, and deep learning has lots of matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## **Parameter of a network**\n",
    "\n",
    "### **Number of layer**\n",
    "No layers are needed for linear regression.  We just connect our input to the output where the synapses are the weights (slopes) and the output neurons add the constant (intecept).\n",
    "\n",
    "So you might start with a single layer, then add more layers and use cross-validation to determine when you are overfitting.\n",
    "\n",
    "If there are lots hidden layers (where \"lots\" is not clearly defined) then we call that a **deep neural network or **deep learning**.\n",
    "\n",
    "### **Number of neutron**\n",
    "\n",
    "The number of neurons in each layer is also a free parameter. \n",
    "\n",
    "- **Typically choose somewhere between twice the number of input nodes and a number between the number of input and output nodes.**\n",
    "\n",
    "- Sometimes the number of neurons in each layer goes down.  But it can also be useful to have the same number in each layer so that there is only one hyperparameter (the number of neurons) and not one per layer, obviously it depend on the problem.\n",
    "\n",
    "- In practice a reasonable approach is to simply **specify many more layers and neurons than you need and perform regularization**. This can be as simple as just stopping the training when the cross-validation error reaches a minimum, which is called **early stopping**.\n",
    "\n",
    "\n",
    "While the number of neurons in the hidden layers are free parameters the number of input and output nodes are constrained by the data and the desired output.  For example, the MNIST digits data requires 784 input neurons (one for each pixel in the 28x28 images) and 10 output neurons (one for each class digit).  \n",
    "\n",
    "\n",
    "### **Vanishing and Exploding Gradients**\n",
    "\n",
    "In the past, training deep neural networks was difficult due to the **vanishing** and **exploding gradient** problems. When gradients become too small or too large during backpropagation, the learning process becomes unstable or extremely slow.\n",
    "One key factor that influences this behavior is the **activation function**, which controls how much \"signal\" a neuron sends to the next layer based on its input.\n",
    "\n",
    "- #### ReLU (Rectified Linear Unit)\n",
    "\n",
    "    $$\n",
    "    \\text{ReLU}(z) = \\max(0, z)\n",
    "    $$\n",
    "\n",
    "    - Helps reduce the vanishing gradient problem by keeping positive gradients intact.\n",
    "    - It's simple and computationally efficient.\n",
    "    - However, for $z < 0$, the gradient becomes 0, which can lead to \"dead neurons\" that never activate.\n",
    "\n",
    "- #### Sigmoid \n",
    "\n",
    "    $$\n",
    "    \\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}\n",
    "    $$\n",
    "\n",
    "    - Maps input to a value between 0 and 1.\n",
    "    - Common in binary classification tasks.\n",
    "    - Suffers from **vanishing gradients** when $z$ is very large or very small (derivatives approach 0).\n",
    "    - Output is **not zero-centered**, which can slow down learning.\n",
    "\n",
    "- #### Tanh\n",
    "\n",
    "    $$\n",
    "    \\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}\n",
    "    $$\n",
    "\n",
    "    - Maps input to a value between **-1 and 1**.\n",
    "    - Like sigmoid, but **zero-centered**, which helps optimization.\n",
    "    - Still suffers from vanishing gradients when $|z|$ is large.\n",
    "\n",
    "\n",
    "**Note**: Different layers in a network can use different activation functions.  \n",
    "In **regression tasks**, it’s common to **omit the activation function in the output layer** so that it can produce any real value.\n",
    "\n",
    "\n",
    "### **Regularization**\n",
    "\n",
    "Neural networks can **overfit**, just like other models. To avoid this, we use **regularization**.\n",
    "\n",
    "- **L1 (LASSO)** and **L2 (Ridge)** regularization add penalties to large weights.\n",
    "- **Dropout** randomly turns off some neurons during training (usually 10–50%) so the network doesn't rely too much on specific ones. After training, all neurons are used again.\n",
    "\n",
    "This makes the model more flexible and prevents it from becoming too dependent on certain parts.\n",
    "\n",
    "\n",
    "### **Batch Normalization**\n",
    "\n",
    "Just like we normalize input data before training, we can also normalize the outputs of hidden layers — this technique is called **batch normalization**.\n",
    "(A batch is simply a small subset of the training data used to compute the gradient and update the model’s weights once during training)\n",
    "\n",
    "It standardizes the activations (outputs) of a layer **within each mini-batch**, ensuring they have:\n",
    "- **Zero mean**\n",
    "- **Unit variance**\n",
    "\n",
    "Batch normalization is typically applied **before the activation function**, and it helps:\n",
    "\n",
    "- **Speed up training**\n",
    "- **Stabilize the learning process**\n",
    "- **Reduce internal covariate shift** (i.e., the change in layer distributions during training)\n",
    "- **Act as a form of regularization**, reducing the need for other techniques like dropout in some cases\n",
    "\n",
    "\n",
    "### **Data Augmentation**\n",
    "\n",
    "Data augmentation is a technique to artificially increase the size and diversity of the training dataset by applying various transformations to the existing data. This helps the model **generalize better** and reduces the risk of overfitting.\n",
    "\n",
    "Examples include:\n",
    "- **Adding extra features** such as polynomial terms (e.g., $x^2 $) to capture non-linear relationships.\n",
    "- For images, applying transformations like **rotation**, **flipping**, **scaling**, or **zooming** to simulate different perspectives and conditions.\n",
    "\n",
    "By exposing the network to these varied examples, data augmentation enables it to learn the essential underlying patterns rather than memorizing specific examples.\n",
    "\n",
    "\n",
    "### **Faster Optimizers**\n",
    "\n",
    "Optimizers are algorithms that determine how the neural network’s weights are updated during training. While basic **stochastic gradient descent (SGD)** updates weights using the gradient of the loss function, advanced optimizers improve upon this by adapting the learning process to speed up convergence and achieve better results.\n",
    "\n",
    "Common optimizers include:\n",
    "\n",
    "- **SGD (Stochastic Gradient Descent):**  \n",
    "  Updates weights using the gradient calculated from a small batch of data. It’s simple and effective but can be slow to converge and sensitive to the learning rate.\n",
    "\n",
    "- **Adam (Adaptive Moment Estimation):**  \n",
    "  Combines the benefits of two other methods (Momentum and RMSProp) by adapting the learning rate for each parameter individually, based on estimates of first and second moments of the gradients. This often leads to faster and more reliable convergence.\n",
    "\n",
    "- **Adagrad:**  \n",
    "  Adjusts the learning rate for each parameter dynamically, giving smaller updates for parameters associated with frequently occurring features, and larger updates for infrequent features. It works well for sparse data but can slow down over time.\n",
    "\n",
    "Using these smarter optimizers can **accelerate training**, **help escape poor local minima**, and often produce models with better performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d4d5e",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 22** - Deep Learning II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b64a7a",
   "metadata": {},
   "source": [
    "- keras\n",
    "- Photo-z with Deep Learning (PyTorch)\n",
    "- Convolutional Neural Networks (CNNs) with Keras\n",
    "- Autoencoders and Variational Autoencoders\n",
    "- Generative Adversarial Networks (GANs)\n",
    "- Large Language Models (LLMs) and Transformers\n",
    "- Extra Topics: Flows and Simulation-Based Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1885d",
   "metadata": {},
   "source": [
    "### **Keras & PyTorch**\n",
    "\n",
    "- Keras is a high-level API (Application Programming Interface) for building and training deep learning models.  \n",
    "    Think of it as similar to Scikit-Learn, but for neural networks.\n",
    "\n",
    "    `keras` simplifies building models on top of `tensorflow`.\n",
    "\n",
    "    It makes creating, training, and testing neural networks much easier.\n",
    "\n",
    "\n",
    "\n",
    "- PyTorch is an open-source deep learning framework developed by Facebook.  \n",
    "    It offers **flexibility and full control** over model building and training, making it popular in research and advanced applications.\n",
    "\n",
    "    Unlike Keras, which is more high-level, PyTorch uses a more **code-first, dynamic approach** — models are defined and updated using standard Python code.\n",
    "\n",
    "    It’s known for its **ease of debugging**, strong GPU support, and tight integration with Python scientific libraries.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Convolutional Layers**\n",
    "\n",
    "Convolutional layers are the core building blocks of **Convolutional Neural Networks (CNNs)**.  \n",
    "The idea behind CNNs is inspired by human visual perception:  \n",
    "> Each neuron in the visual cortex doesn't \"see\" the entire visual field at once — instead, it's sensitive to specific **local patterns**, like vertical or horizontal lines.\n",
    "\n",
    "#### **How it works**\n",
    "\n",
    "A convolutional layer applies small filters (also called **kernels**) — such as $3 \\times 3$ or $5 \\times 5$ — across the input image.  \n",
    "Each filter is designed to detect specific patterns, like edges, textures, or shapes.\n",
    "\n",
    "- The filter **slides** (or **convolves**) across the image.\n",
    "- At each position, it computes a weighted sum of the input pixels under the filter.\n",
    "- The result is a **feature map** — a transformed version of the image that highlights specific patterns.\n",
    "\n",
    "> The key idea is that **the network learns the filters automatically** — it figures out which patterns are useful for solving the task (e.g., classifying an image).\n",
    "\n",
    "#### **Filters in action**\n",
    "\n",
    "Some filters detect **vertical features**:\n",
    "\n",
    "![vertical filter](https://miro.medium.com/max/1338/1*7IEbBib7Yc7qST5IGQoYXA.jpeg)\n",
    "\n",
    "Others detect **horizontal features**:\n",
    "\n",
    "![horizontal filter](https://miro.medium.com/max/1238/1*PSSAaH2pZbl5bK3Ef_zk4A.jpeg)\n",
    "\n",
    "This is conceptually similar to how **polarized light** highlights specific orientations — which can be a helpful analogy to visualize what CNNs do.\n",
    "\n",
    "#### **Local Connectivity**\n",
    "\n",
    "In a convolutional layer, each neuron is connected only to a small region of the previous layer — its **receptive field** — rather than to all neurons (as in a fully connected layer).\n",
    "\n",
    "We **slide the kernel** over the input, and the neuron in the next layer computes its value **only from the local input pixels under the kernel**.\n",
    "\n",
    "![convolution gif](https://miro.medium.com/max/4800/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
    "\n",
    "This local processing enables CNNs to:\n",
    "- Focus on small details\n",
    "- Reuse patterns across the image\n",
    "- Scale to high-dimensional inputs like images\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Pooling Layers**\n",
    "\n",
    "After convolution, we often use **pooling layers** to reduce the size of the feature maps.\n",
    "\n",
    "This helps:\n",
    "- Decrease computation\n",
    "- Reduce overfitting\n",
    "- Keep only the most important information\n",
    "\n",
    "Pooling summarizes nearby pixels into a single value.\n",
    "\n",
    "![CNN Example](https://www.researchgate.net/publication/331986593/figure/fig1/AS:740548759547904@1553571991001/The-classic-structure-of-CNN-it-consists-of-two-modules-Feature-extraction-module-FEM.ppm)\n",
    "\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "**Max pooling** is the most common pooling method.\n",
    "\n",
    "- It looks at a small region (e.g., $2 \\times 2$) of the feature map\n",
    "- It outputs the **maximum value** from that region\n",
    "\n",
    "This keeps the strongest features and throws away the weaker ones.\n",
    "\n",
    "So instead of keeping all pixel values, it just keeps the most \"active\" one from each patch.\n",
    "\n",
    "![Pooling example](https://miro.medium.com/max/1000/1*ydNsGDxMldAiq7b96GDQwg.jpeg)\n",
    "\n",
    "---\n",
    "\n",
    "### **Autoencoders**\n",
    "\n",
    "An encoder in a neural network is designed to transform complex or high-dimensional input data into a compact, meaningful representation — while preserving the most important information.\n",
    "\n",
    "**This is the deep-learning to the problem of dimensionality reduction.**  You can think of an autoencoder as doing PCA with a neural network -- breaking our data down into the only the most important features that we actually need (finding the intrinsic dimensionality). In fact, if the network uses only linear (or no) activation functions and $L2$ cost function, then we have exactly PCA.  \n",
    "\n",
    "![autoencoder example](https://miro.medium.com/max/1400/1*SxwRp9i23OM0Up4sEze1QQ@2x.png)\n",
    "\n",
    "\n",
    "### **Variational Autoencoders (VAEs)**\n",
    "\n",
    "**VAEs** are a special kind of autoencoder that do more than just compress and reconstruct data — they also learn the **distribution** behind the data.\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "Like a normal autoencoder, a VAE has two parts:\n",
    "- An **encoder**: compresses input data into a smaller representation\n",
    "- A **decoder**: reconstructs the data from this representation\n",
    "\n",
    "But instead of learning a single vector as the encoding, the encoder learns two things:\n",
    "- A **mean** vector  $\\mu$ \n",
    "- A **standard deviation** vector $\\sigma$\n",
    "\n",
    "From these, the model samples a latent variable $z$ from a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "This makes the encoding **stochastic**, not deterministic.\n",
    "\n",
    "\n",
    "#### Why this matters:\n",
    "\n",
    "- The decoder learns to generate data from different points in this latent space.\n",
    "- You can **sample** new values of $z$, feed them into the decoder, and get **new, realistic data**.\n",
    "- This is useful for **generating new examples** that are similar to the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Generative Adversarial Networks (GANs)**\n",
    "\n",
    "GANs consist of two networks:\n",
    "- A **generator** that tries to make fake data,\n",
    "- A **discriminator** that tries to tell if the data is real or fake.\n",
    "\n",
    "They are trained together in a sort of game, improving each other over time.  \n",
    "GANs are widely used to create realistic images and simulations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Large Language Models (LLMs) and Transformers**\n",
    "\n",
    "LLMs are deep learning models that can understand and generate human language.  \n",
    "They’re built using **transformers**, a special architecture that captures the relationship between words efficiently.\n",
    "\n",
    "These models are used in applications like translation, chatbots, and text generation.\n",
    "\n",
    "**GPT = generative pretrained transformers**\n",
    "\n",
    "A GPT is a (very!) complicated mathematical function that predicts, given a piece of text, what word should come next. \n",
    "\n",
    "GPT-4 by OpenAI allegedly has **1.76 trillion parameters**. The dataset? It's **the entire internet**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Extra Topics: Flows and Simulation-Based Inference**\n",
    "\n",
    "These are more advanced deep learning tools:\n",
    "- **Normalizing Flows** are used to model complex probability distributions.\n",
    "- **Simulation-Based Inference** is used when traditional statistical methods don’t work well, especially in physics or astronomy, where simulations are easier to get than real data.\n",
    "\n",
    "These methods expand what deep learning can do beyond classic prediction tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
