{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edf1ab5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816324f2",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 2** - Probability & Statistic I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03de5df3",
   "metadata": {},
   "source": [
    "- PDF, CDF, quantile\n",
    "- Real and empirical distributions\n",
    "- Errors: heteroscedastic and homoscedastic\n",
    "- Kolmogorov axioms and probability\n",
    "- Bayes’ theorem\n",
    "- Transformations of random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35648beb",
   "metadata": {},
   "source": [
    "### **Probability Density Function (PDF), Cumulative Distribution Function (CDF), and Quantile**\n",
    "\n",
    "- **PDF (Probability Density Function)**: Describes the probability for a continuous variable to take a specific value. The area under the PDF over an interval gives the probability of the variable falling within that interval.\n",
    "- **CDF (Cumulative Distribution Function)**: It is obtained by integrating the PDF from - infinity up to a certain values X. Gives the probability that a random variable is less than or equal to a certain value. \n",
    "\n",
    "$$\n",
    "H(x) = \\int_{-\\infty}^{x} h(x')\\, dx'\n",
    "$$\n",
    "\n",
    "\n",
    "- **Quantile**: it's the inverse of the CDF. The value below which a certain percentage of observations fall. For example, the 0.25 quantile (or 25th percentile) is the value below which 25% of the data lie.\n",
    "\n",
    "---\n",
    "\n",
    "### **Empirical and Theoretical Distributions**\n",
    "\n",
    "- **Theoretical Distribution**: A probability distribution derived from a known mathematical model (e.g., Normal, Poisson).\n",
    "- **Empirical Distribution**: Based on observed data. It approximates the distribution of a dataset and is typically represented by the empirical CDF or histogram.\n",
    "- Empirical distributions are used when the true distribution is unknown or difficult to model.\n",
    "\n",
    "---\n",
    "\n",
    "### **Homoscedastic and Heteroscedastic Errors**\n",
    "\n",
    "- **Homoscedasticity**: The variance of the errors is constant.\n",
    "- **Heteroscedasticity**: The error variance changes with the data\n",
    "\n",
    "---\n",
    "\n",
    "### **Kolmogorov's Axioms and Probability**\n",
    "\n",
    "Kolmogorov formalized the foundation of probability with three axioms:\n",
    "\n",
    "1. **Non-negativity**: For any event A, the probability is non-negative:  \n",
    "   \\( P(A) >= 0 \\)\n",
    "2. **Normalization**: The probability of the entire sample space is 1:  \n",
    "   \\( P($\\Omega$) = 1 \\)\n",
    "3. **Additivity**: For any two mutually exclusive events A and B:  \n",
    "   \\( P(A $\\cup$ B) = P(A) + P(B) \\)\n",
    "\n",
    "These axioms form the basis of modern probability theory.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes' Theorem**\n",
    "\n",
    "Bayes' Theorem updates the probability of a hypothesis based on new evidence:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "-  P(A|B) : Posterior probability (updated belief)  \n",
    "-  P(B|A) : Likelihood of observing B given A  \n",
    "-  P(A) : Prior probability of A  \n",
    "-  P(B) : Marginal probability of B. We can write the marginal probability of x as:   $ p(x) = \\int p(x,y) dy = \\int p(x|y)p(y)dy$\n",
    "\n",
    "Used in many fields like medicine, machine learning, and decision theory.\n",
    "\n",
    "### Example: COVID Test\n",
    "\n",
    "Suppose:\n",
    "- 1% of the population has COVID:  **P(COVID) = 0.01**\n",
    "- The test is 99% sensitive:  **P(Positive | COVID) = 0.99**\n",
    "- The test is 95% specific:  **P(Negative | No COVID) = 0.95**, so **P(Positive | No COVID) = 0.05**\n",
    "\n",
    "We want to find the probability that someone actually has COVID **given that they tested positive**, i.e., **P(COVID | Positive)**.\n",
    "\n",
    "**Apply Bayes' Theorem:**\n",
    "\n",
    "$$\n",
    "P(\\text{COVID} | \\text{Positive}) = \\frac{P(\\text{Positive} | \\text{COVID}) \\cdot P(\\text{COVID})}{P(\\text{Positive})}\n",
    "$$\n",
    "\n",
    "We compute the denominator using the law of total probability:\n",
    "\n",
    "$$\n",
    "P(\\text{Positive}) = P(\\text{Positive} | \\text{COVID}) \\cdot P(\\text{COVID}) + P(\\text{Positive} | \\text{No COVID}) \\cdot P(\\text{No COVID})\n",
    "$$\n",
    "\n",
    "Plug in the numbers:\n",
    "\n",
    "$$\n",
    "P(\\text{Positive}) = 0.99 \\cdot 0.01 + 0.05 \\cdot 0.99 = 0.0099 + 0.0495 = 0.0594\n",
    "$$\n",
    "\n",
    "Now compute the posterior:\n",
    "\n",
    "$$\n",
    "P(\\text{COVID} | \\text{Positive}) = \\frac{0.99 \\cdot 0.01}{0.0594} \\approx \\frac{0.0099}{0.0594} \\approx 0.1667\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "Even after testing positive, the probability that the person actually has COVID is only **~16.7%**. This highlights the importance of considering base rates (prior probabilities) when interpreting test results.\n",
    "\n",
    "---\n",
    "\n",
    "### **Transformations of Random Variables**\n",
    "\n",
    "Transforming a random variable means applying a function to it to create a new variable. This is useful when we want to study the distribution of a quantity that depends on a known random variable.\n",
    "\n",
    "#### **Basic Idea**\n",
    "\n",
    "Let $X$ be a continuous random variable with known probability density function (PDF) $f_X(x)$, and let $Y = g(X)$ be a transformation of $X$. If the function $g$ is **invertible and differentiable**, then the PDF of $Y$, denoted $f_Y(y)$, is given by:\n",
    "\n",
    "$$\n",
    "f_Y(y) = f_X(g^{-1}(y)) \\cdot \\left| \\frac{d}{dy} g^{-1}(y) \\right|\n",
    "$$\n",
    "\n",
    "This formula adjusts the density according to how the transformation stretches or compresses the space.\n",
    "\n",
    "To find the probability density function of the transformed variable $Y = g(X)$, take the value of the original density function $f_X$ at the inverse of the transformation $g^{-1}(y)$, and multiply it by the **absolute value of the derivative** of that inverse function with respect to $y$.\n",
    "\n",
    "\n",
    "#### **Example: Squaring a Uniform Variable**\n",
    "\n",
    "Let $X \\sim \\text{Uniform}(0, 1)$\n",
    "\n",
    "Now define $Y = X^2$. We want to find the distribution of $Y$.\n",
    "\n",
    "- Invert the transformation: $X = \\sqrt{Y}$\n",
    "- Compute the derivative: $\\frac{d}{dy} \\sqrt{y} = \\frac{1}{2\\sqrt{y}}$\n",
    "\n",
    "Now apply the change-of-variable formula:\n",
    "\n",
    "$$\n",
    "f_Y(y) = f_X(\\sqrt{y}) \\cdot \\left| \\frac{1}{2\\sqrt{y}} \\right| = \\begin{cases}\n",
    "\\frac{1}{2\\sqrt{y}} & \\text{if } 0 \\leq y \\leq 1 \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "The transformed variable $Y = X^2$ has a distribution that is concentrated more near 0 than near 1, even though $X$ was uniform. This shows how transformations can significantly affect the shape of a distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa401bbc",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 3** - Probability & Statistic II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d9b6e4",
   "metadata": {},
   "source": [
    "- Monte Carlo integration (crude / hit-or-miss/ importance sampling)  \n",
    "- Mean, median, and expected value  \n",
    "- Standard deviation, MAD1, variance, MAD2, quantile region, interquantile range, mode  \n",
    "- Skewness  \n",
    "- Kurtosis  \n",
    "- Statistics of the PDF and sample; Bessel’s correction  \n",
    "- Uncertainties of estimators  \n",
    "- PDFs: uniform, Gaussian, log-normal, chi-squared, Poisson   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dd6cd1",
   "metadata": {},
   "source": [
    "### **Monte Carlo Integration**\n",
    "\n",
    "**Monte Carlo methode** uses random generated number to approximate matemathical and physical problem, such as integration.\n",
    "- **Crude Monte Carlo**:  \n",
    "  Estimate the integral $\\int_a^b f(x) \\, dx$ by sampling $x_i \\sim \\mathcal{U}(a, b)$ and computing:  \n",
    "  $$\n",
    "  I \\approx (b - a) \\cdot \\frac{1}{N} \\sum_{i=1}^N f(x_i)\n",
    "  $$\n",
    "- **Hit-or-Miss method**:  \n",
    "  Sample uniformly in a rectangle that encloses the graph of $f(x)$.  \n",
    "  The integral is approximated by the fraction of points that fall below the curve times the area of the rectangle.\n",
    "\n",
    "-  **Importance Sampling**\n",
    "    - Hit or miss and Crude MC, are inefficient if the integrand has some null zone, or even if is really extendended... that's beacuse this 2 methode  use the uniform distribution.\n",
    "    - Instead of sampling from the uniform, sample from a **proposal distribution** $g(x)$ \n",
    "    - Best when $g(x)$ is close to the shape of $f(x)$.\n",
    "    - Reduces variance and computational cost if the $g(x)$ it's well chosen\n",
    "---\n",
    "\n",
    "### **Mean, Median, Expected Value, and Mode**\n",
    "\n",
    "KEEP ATTENTION AT THE DIFFERENT USE OF $\\bar{x}$ AND $\\mu$\n",
    "\n",
    "- **Mean**: Arithmetic average of a dataset. $\\mu = \\mathbb{E}[X]$, Where X will denote an entire dataset.\n",
    "- **Median**: Middle value when data are ordered. Less sensitive to outliers.\n",
    "- **Expected value** $\\mathbb{E}[X]$ : Theoretical mean of a random variable. For continuous variables:  \n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\int x f(x) \\, dx\n",
    "  $$\n",
    "- **Mode**: Most frequent value in a dataset.\n",
    "\n",
    "---\n",
    "\n",
    "### **Standard Deviation, Variance, MAD, Quantiles, and IQR**\n",
    "\n",
    "- **Variance** (2nd-order moment):  \n",
    "  $$\n",
    "  \\sigma^2 = \\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\int_{-\\infty}^{+\\infty}(x - \\mu)^2 f(x) \\, dx\n",
    "  $$\n",
    "  f(x) is the probability density function (PDF) of the continuous random variable x\n",
    "- **Standard Deviation** $\\sigma$: Measures spread around the mean. It is the square root of the variance:\n",
    "  $$\n",
    "  \\sigma = \\sqrt{\\text{Var}(X)} = \\sqrt{\\sigma^2}\n",
    "  $$\n",
    "- **MAD_1 (Mean Absolute Deviation)**:  \n",
    "  $$\n",
    "  \\text{MAD}_1 = \\frac{1}{N} \\sum_{i=1}^N |x_i - \\bar{x}|\n",
    "  $$\n",
    "  Note: this is not differentiable at  x = 0 , so it's sometimes avoided in optimization.\n",
    "- **MAD_2 (Median Absolute Deviation)**:  \n",
    "  $$\n",
    "  \\text{MAD}_2 = \\frac{1}{N} \\sum_{i=1}^N |x_i - M|\n",
    "  $$\n",
    "  where M is the median.\n",
    "- **Quantile region**: Range containing a central portion of the distribution (e.g., 95% interval).\n",
    "- **Interquantile Range (IQR)**:  \n",
    "  $$\n",
    "  \\text{IQR} = Q_{75} - Q_{25}\n",
    "  $$\n",
    "  Contains the central 50% of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Skewness and Kurtosis**\n",
    "\n",
    "- **Skewness**: Measures asymmetry of a distribution (3rd-order moment).  \n",
    "  - Positive skew: tail to the right.  \n",
    "  - Negative skew: tail to the left.  \n",
    "  - Formula:\n",
    "    $$\n",
    "    \\text{Skewness} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{x_i - \\bar{x}}{\\sigma} \\right)^3\n",
    "    $$\n",
    "- **Kurtosis**: Measures how likely extreme values (far from the mean) are (4th-order moment).  \n",
    "  - High kurtosis: heavy tails.  \n",
    "  - Low kurtosis: light tails.  \n",
    "  - Normal distribution has kurtosis = 3.  \n",
    "  - Formula:\n",
    "    $$\n",
    "    \\text{Kurtosis} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( \\frac{x_i - \\bar{x}}{\\sigma} \\right)^4\n",
    "    $$\n",
    "\n",
    "---\n",
    "\n",
    "### **PDFs: Uniform, Gaussian, Log-Normal, Chi-Squared, Poisson**\n",
    "\n",
    "This section summarizes key properties of common probability distributions.\n",
    "\n",
    "\n",
    "####  **Uniform Distribution**  (on $[a, b]$)\n",
    "\n",
    "- **PDF**:  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{b - a}, \\quad \\text{for } x \\in [a, b]\n",
    "  $$\n",
    "- **Mean**: $\\mu = \\frac{a + b}{2}$\n",
    "- **Variance**: $\\sigma^2 = \\frac{(b - a)^2}{12}$\n",
    "- **Skewness**: 0 (symmetric)\n",
    "- **Kurtosis** (excess): $-1.2$\n",
    "\n",
    "\n",
    "\n",
    "####  **Gaussian (Normal) Distribution**  ($X \\sim \\mathcal{N}(\\mu, \\sigma^2)$)\n",
    "\n",
    "- **PDF**:  \n",
    "  $$\n",
    "  f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\, e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}}\n",
    "  $$\n",
    "- **Mean**: $\\mu$\n",
    "- **Variance**: $\\sigma^2$\n",
    "- **Skewness**: 0 (perfectly symmetric)\n",
    "- **Kurtosis** (excess): 0\n",
    "\n",
    "Notes:\n",
    "- The sum (or convolution) of two Gaussian variables is still Gaussian.\n",
    "- Called the \"queen of distributions\" — many natural phenomena approximate a normal distribution due to the Central Limit Theorem.\n",
    "- Probability within:\n",
    "  - $1\\sigma$: ~68%\n",
    "  - $2\\sigma$: ~95%\n",
    "  - $3\\sigma$: ~99.7%\n",
    "\n",
    "\n",
    "#### **Chi-Squared Distribution** ($X \\sim \\chi^2(k)$)\n",
    "\n",
    "- Defined as:  \n",
    "  $$\n",
    "  X = \\sum_{i=1}^k \\left( \\frac{x_i - \\mu}{\\sigma} \\right)^2\n",
    "  $$\n",
    "- **Mean**: $\\mu = k$\n",
    "- **Variance**: $\\sigma^2 = 2k$\n",
    "- **Skewness**: $\\sqrt{8/k}$\n",
    "- **Kurtosis** (excess): $12/k$\n",
    "- **What is $k$?**\n",
    "    - $k$ is the number of **degrees of freedom** (DOF).\n",
    "    - It typically corresponds to the number of **independent standardized variables** being squared and summed.\n",
    "    - For example, in hypothesis testing, $k$ is often the number of data points **minus the number of fitted parameters**.\n",
    "\n",
    "\n",
    "#### **Reduced Chi-Squared ($\\chi^2_{\\text{red}}$)**\n",
    "\n",
    "When evaluating the goodness-of-fit of a model, it's common to compute the **reduced chi-squared**:\n",
    "\n",
    "$$\n",
    "\\chi^2_{\\text{red}} = \\frac{\\chi^2}{k} = \\frac{1}{k} \\sum_{i=1}^N \\left( \\frac{y_i - f(x_i)}{\\sigma_i} \\right)^2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\chi^2$ is the total chi-squared statistic.\n",
    "- $k = N - p$ is the degrees of freedom (number of data points $N$ minus number of fitted parameters $p$).\n",
    "- $f(x_i)$ is the model prediction for data point $i$.\n",
    "- $\\sigma_i$ is the uncertainty on $y_i$.\n",
    "\n",
    "\n",
    "#### **Poisson Distribution** ($X \\sim \\text{Poisson}(\\mu)$)\n",
    "\n",
    "- **PMF**:  \n",
    "  $$\n",
    "  P(k; \\mu) = \\frac{\\mu^k e^{-\\mu}}{k!}, \\quad k = 0, 1, 2, \\dots\n",
    "  $$\n",
    "- **Mean**: $\\mu$\n",
    "- **Variance**: $\\mu$\n",
    "- **Skewness**: $\\frac{1}{\\sqrt{\\mu}}$\n",
    "- **Kurtosis** (excess): $\\frac{1}{\\mu}$\n",
    "\n",
    "Models counts of rare events in fixed intervals (e.g., radioactive decay, emails per hour).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea58c2c4",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 4** - Probability & Statistic III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccaca67",
   "metadata": {},
   "source": [
    "- Central Limit Theorem  \n",
    "- Law of Large Numbers  \n",
    "- Multidimensional PDFs (mean, sigma x and y, covariance, correlation coefficient, principal axes, 2D confidence level)  \n",
    "- Correlation vs causation (Pearson, Spearman, Kendall)  \n",
    "- Rejection sampling  \n",
    "- Inverse sampling  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc59594",
   "metadata": {},
   "source": [
    "### **Central Limit Theorem (CLT)**\n",
    "\n",
    "The CLT states that the sum (or mean) of a large number of independent, identically distributed random variables tends to follow a **normal distribution**, regardless of the original distribution.\n",
    "This theoreme is the faundation for the repeated measurments .\n",
    "\n",
    "---\n",
    "\n",
    "### **Law of Large Numbers (LLN)** (Bernoulli's theoreme)\n",
    "\n",
    "- The LLN states that as the number of observations $N$ increases, the sample mean $\\bar{x}$ converges to the true mean $\\mu$, this is also valid fot the variance:\n",
    "  $$\n",
    "  \\lim_{N \\to \\infty} \\bar{x} = \\mu \\quad, \\quad \\lim_{N \\to \\infty} s = \\sigma\n",
    "  $$\n",
    "- This is a statement about convergence **in probability**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Multidimensional PDFs**\n",
    "\n",
    "- In 2D, the joint distribution can be described by:\n",
    "  - **Mean vector**:  \n",
    "    $$\n",
    "    \\vec{\\mu} = (\\mu_x, \\mu_y)\n",
    "    $$\n",
    "    where, ofcourse, $\\mu_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}x h(x,y) dx dy$\n",
    "  - **Covariance matrix**:  \n",
    "    $$\n",
    "    \\Sigma = \\begin{pmatrix}\n",
    "    \\sigma_x^2 & \\text{cov}(x, y) \\\\\n",
    "    \\text{cov}(y, x) & \\sigma_y^2\n",
    "    \\end{pmatrix}\n",
    "    $$\n",
    "    The two off diagonal values are equal to 0 only if x & y are totaly uncorrelated.\n",
    "    $$\\sigma^2_x = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x)^2 h(x,y) dx dy$$\n",
    "    $$\\sigma_{xy} = Cov(x,y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty}(x-\\mu_x) (y-\\mu_y) h(x,y) dx dy$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"immagini/negative-and-positive-covariance.png\" alt=\"boh\" width=\"600\"/>\n",
    "</p>\n",
    "\n",
    "  - **Correlation coefficient**:  \n",
    "    $$\n",
    "    \\rho = \\frac{\\text{cov}(x, y)}{\\sigma_x \\sigma_y}\n",
    "    $$\n",
    "    Express the percentual of correlation between the 2 variable\n",
    "\n",
    "  - **Principal axes**: determined by the eigenvectors of $\\Sigma$; note that the correlation vanish in this system by definition.\n",
    "  - **2D Confidence Ellipses**: regions where the joint probability is constant, keep attention, for each dimension the number of sigma has a different meaning: $1\\sigma = 39$% in 2 dimension! I can impose 68% for the similitude with 1D, but it's not $1\\sigma$.\n",
    "\n",
    "  Outliers can bias these estimates and that it may be more appropriate to use the median rather than the mean as a more robust estimator for $\\mu_x$ and $\\mu_y$\n",
    "\n",
    "- **Marginalized PDFs**\n",
    "\n",
    "    If you have the joint PDF $h(x, y)$, you can obtain the **marginal distribution** of one variable by **integrating out** the other:\n",
    "\n",
    "    - Marginal PDF of $x$:\n",
    "      $$\n",
    "      f_X(x) = \\int_{-\\infty}^{\\infty} h(x, y) \\, dy\n",
    "      $$\n",
    "\n",
    "    These marginalized PDFs describe the individual behavior of each variable, regardless of the other.\n",
    "---\n",
    "\n",
    "\n",
    "### **Correlation vs Causation**\n",
    "\n",
    "Correlation does not imply causation!\n",
    "Just because the sun burns our skin and also makes us thirsty, it doesn't mean that thirst causes sunburn!\n",
    "\n",
    "- **Pearson's coefficient** ($r$) : Measures linear correlation between 2 different dataset; it's a value between -1 and 1, the 2 are uncorrelated only if r = 0.\n",
    "It has 2 problems:\n",
    "  - it's susceptible at the outliars\n",
    "  - doesn't count the error\n",
    "\n",
    "- **Spearman's coefficient** ($r_s$): Measures monotonic (rank-based) correlation.\n",
    "- **Kendall's coefficient** ($\\tau$): Measures ordinal association between two variables.\n",
    "\n",
    "---\n",
    "\n",
    "### **Rejection Sampling**\n",
    "\n",
    "Rejection sampling is a method to generate random samples from a complex distribution $h(x)$, using a simpler proposal distribution $q(x)$.\n",
    "\n",
    "The procedure works as follows:\n",
    "\n",
    "1) Decide on a straightforward *proposal distribution* $q(x)$ to propose new samples. It should be wide enough to capture the tails of $h(x)$. Usually used a uniform distribution.\n",
    "\n",
    "2) Generate a random sample from $q(x)$, $x_i$.\n",
    "\n",
    "3) Now generate a random sample, $u$, from a uniform distribution in the range $[0,\\mathrm{max}(h(x))]$, where the upper bound should be as large or larger than the maximum density of $h(x)$. (This could be worked out analytically or by histograming the data.)\n",
    "\n",
    "4) If $u\\leq h(x_i)$ accept the point, or else reject it and try again from step 2.\n",
    "\n",
    "The set of accepted $x$ values will follow the target distribution $p(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c65a7b",
   "metadata": {},
   "source": [
    "### **Inverse Transform Sampling**\n",
    "Rejection sampling works, but wouldn't it be awesome if we didn't have to discard *any* points during our sampling? This is the power and simplicity of **inverse transform sampling**.\n",
    "\n",
    "- Used to sample from a distribution $h(x)$ with known CDF $H(x)$ and Quantile.\n",
    "- Steps:\n",
    "  1. Sample $u$ from  ${U}(0, 1)$.\n",
    "  2. Using the quantile function $H^{-1}(x)$, find the value of $x$ below which a fraction $u$ of the distribution is contained.\n",
    "  3. The $x$ value you get is a random sample from $h(x)$\n",
    "\n",
    "\n",
    "Normalizarion here are rellly important.\n",
    "you can retrive the quantile and the CDF by numerically solution if you are not able to do in by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012b4cd6",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 5** - Frequentist Inference I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668bbc14",
   "metadata": {},
   "source": [
    "- Population  \n",
    "- Sample  \n",
    "- Statistics  \n",
    "- Estimators  \n",
    "- Uncertainties and intervals  \n",
    "- Frequentist vs Bayesian  \n",
    "- Maximum Likelihood Estimator (MLE)  \n",
    "- Properties of estimators  \n",
    "- Likelihood  \n",
    "- Chi-squared  \n",
    "- Minimization  \n",
    "- Mean and error of MLE with heteroscedastic and homoscedastic errors  \n",
    "- Non-Gaussian likelihoods "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b71a25",
   "metadata": {},
   "source": [
    "### **Population, Sample, Statistic, Estimators, Uncertainty and Intervals**\n",
    "\n",
    "- A **population** is the full set of data or measurements we are interested in.\n",
    "- A **sample** is a subset of the population, used to infer properties of the whole.\n",
    "- A **statistic** is a function of the sample (e.g. the sample mean $\\bar{x}$).\n",
    "- An **estimator** is a rule or formula to estimate population parameters from the sample.\n",
    "- All estimators have **uncertainties** due to random sampling.\n",
    "- A **confidence interval**, gives a range likely to contain the true value.\n",
    "\n",
    "---\n",
    "\n",
    "### **Frequentist vs Bayesian**\n",
    "\n",
    "- **Frequentist**: Probability is extract from the frequency of events. Parameters are fixed, data are random.\n",
    "Into Frequentist inference we have confidence levels,.\n",
    "- **Bayesian**: Probability expresses belief or uncertainty about what we know. Parameters have distributions while data are fixed. In Bayesian inference we have credible regions derive from posterior distribution of the parameters.\n",
    "\n",
    "\n",
    "- Bayesian statistic it's hold by the **Bayes’ theorem**:\n",
    "  $$\n",
    "  P(\\theta | \\text{data}) = \\frac{P(\\text{data} | \\theta) \\cdot P(\\theta)}{P(\\text{data})}\n",
    "  $$\n",
    "  where $\\theta$ are the parameters.\n",
    "    -  $P(\\theta | \\text{data})$ : Posterior probability (updated belief)  \n",
    "    -  $P(\\text{data} | \\theta)$ : Likelihood of observing data given the parameter  \n",
    "    -  $P(\\theta)$ : Prior probability  \n",
    "    -  $P(data)$ : Marginal probability \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72811421",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### **Maximum Likelihood Estimator (MLE)**\n",
    "\n",
    "- The **MLE** is the value of the parameter $\\theta$ that **maximizes the likelihood** of the observed data:\n",
    "  $$\n",
    "  \\hat{\\theta}_{\\text{MLE}} = \\arg \\max_\\theta \\mathcal{L}(\\theta)\n",
    "  $$\n",
    "- It's usefull in both frequentist e bayesian approach\n",
    "- **Remember**: the **likelihood** is defined as the product of the probabilities (or probability densities) of the observed data, assuming a given model or parameter value.\n",
    "\n",
    "  For independent data points $x_1, x_2, ..., x_N$:\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = \\prod_{i=1}^{N} p(x_i \\mid \\theta)\n",
    "  $$\n",
    "\n",
    "  Where:\n",
    "  - $\\mathcal{L}(\\theta)$ is the likelihood function,\n",
    "  - $p(x_i \\mid \\theta)$ is the probability (or density) of observing $x_i$ given parameter $\\theta$,\n",
    "  - The product assumes all $x_i$ are independent.\n",
    "\n",
    "  Often, we work with the **log-likelihood**:\n",
    "  $$\n",
    "  \\log \\mathcal{L}(\\theta) = \\sum_{i=1}^{N} \\log p(x_i \\mid \\theta)\n",
    "  $$\n",
    "  which is easier to compute and optimize.\n",
    "\n",
    "**Remember how this works:**\n",
    "You have a likelihood function (or log-likelihood).\n",
    "The values $x_i$ are known (observed data), but the parameters $\\theta$ are unknown.\n",
    "You use an algorithm (e.g., grid search, gradient ascent, or scipy.optimize) to:\n",
    "Try many values of $\\theta$, then compute the likelihood (or log-likelihood) for each value, and plot the result over the values of theta. Find the one that maximizes it\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "515b6f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1010101010101\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7ff4d1e67bc0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.stats import poisson\n",
    "\n",
    "poisson_data = poisson.rvs(12, size=5000)\n",
    "\n",
    "# Scan across 1000 possible mu values from 3 to 20.\n",
    "mu_proposed = np.linspace(3,20,100)\n",
    "\n",
    "# compute the ln(L) for each possible mu.\n",
    "lnL_scan = []\n",
    "for mu in mu_proposed:\n",
    "    lnL_temp = poisson.logpmf(poisson_data, mu=mu) # gives you the log prob. density; useful!\n",
    "    lnL_temp = np.sum(lnL_temp) # sum over the log pmf of all data points\n",
    "    lnL_scan.append( lnL_temp )\n",
    "    \n",
    "# convert to numpy array\n",
    "lnL_scan = np.array(lnL_scan)\n",
    "\n",
    "# mu_proposed that maximizes the lnL.\n",
    "plt.plot(mu_proposed,lnL_scan)\n",
    "\n",
    "findmax = mu_proposed[np.argmax(lnL_scan)]\n",
    "print(findmax)\n",
    "plt.axvline(findmax,c='red')\n",
    "plt.xlabel(\"mu\")\n",
    "plt.ylabel(\"log-likelihood\")\n",
    "\n",
    "plt.axvline(12,c='black',ls='dotted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a60ed64",
   "metadata": {},
   "source": [
    "\n",
    "### **Properties of Estimators**\n",
    "\n",
    "- **Unbiasedness**: $\\mathbb{E}[\\hat{\\theta}] = \\theta$\n",
    "- **Consistency**: $\\hat{\\theta} \\to \\theta_{true}$ as $N \\to \\infty$\n",
    "- **Normality** : As $N \\to \\infty$ , the distribution of the estimator approaches a normal distribution.\n",
    "- **Efficiency**: Minimum possible variance (called Cramer -Rao bound)\n",
    "\n",
    "---\n",
    "\n",
    "### **Likelihood, Chi-squared and Minimization**\n",
    "\n",
    "- The **likelihood** $\\mathcal{L}(\\theta)$ is the probability of the data given parameters. $P(data | \\theta)$\n",
    "- Suppose you have data points $(x_i, y_i)$ with known uncertainties $\\sigma_i$, and a model $f(x_i; \\theta)$ depending on parameters $\\theta$.\n",
    "      Each measurement $y_i$ is modeled as:\n",
    "    $$\n",
    "    y_i = f(x_i; \\theta)\n",
    "    $$\n",
    "\n",
    "    So the probability density of observing $y_i$ given $\\theta$ is:\n",
    "    $$\n",
    "    p(y_i \\mid \\theta) = \\frac{1}{\\sqrt{2\\pi \\sigma_i^2}} \\, \\exp\\left( -\\frac{(y_i - f(x_i;\\theta))^2}{2\\sigma_i^2} \\right)\n",
    "    $$\n",
    "\n",
    "    the total likelihood is the product over all data points:\n",
    "  $$\n",
    "  \\mathcal{L}(\\theta) = \\prod_{i=1}^N p(y_i \\mid \\theta)\n",
    "  $$\n",
    "\n",
    "  Taking the log:\n",
    "  $$\n",
    "  \\log \\mathcal{L}(\\theta) = -\\frac{1}{2} \\sum_{i=1}^N \\left[ \\log(2\\pi \\sigma_i^2) + \\frac{(y_i - f(x_i;\\theta))^2}{\\sigma_i^2} \\right]\n",
    "  $$\n",
    "\n",
    "  The first term is constant if $\\sigma_i$ are known.  \n",
    "  So **maximizing the log-likelihood** is equivalent to **minimizing** the second term\n",
    "\n",
    "- the Likelihood will follow the $\\exp(-\\chi^2/2)$\n",
    "- In Gaussian cases, maximizing the log-likelihood is equivalent to **minimizing the chi-squared**:\n",
    "  $$\n",
    "  \\chi^2 = \\sum_{i=1}^N \\left( \\frac{x_i - f(x_i; \\theta)}{\\sigma_i} \\right)^2\n",
    "  $$\n",
    "- Minimizing $\\chi^2$ gives the best-fit parameters.\n",
    "- The MLE method tell us to think the likelihood as a function of the (unknown) model parameters, and by minimizing the $\\chi^2$, we will find the values that maximize the values of the likelihood.\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean and MLE Error: Homoscedastic vs Heteroscedastic**\n",
    "\n",
    "- **Homoscedastic**: All data points have the same uncertainty $\\sigma$. If we minimize the $\\chi^2$ distribution, we will retrived the **sample mean**:\n",
    "    $$\n",
    "    \\bar{x} = \\frac{1}{N} \\sum x_i \\quad \\text{and} \\quad \\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{N}}\n",
    "    $$\n",
    "- **Heteroscedastic**: Uncertainties vary for each data point $\\sigma_i$. Then you will retrive the **weighted mean**:\n",
    "    $$\n",
    "    \\bar{x} = \\frac{\\sum x_i / \\sigma_i^2}{\\sum 1 / \\sigma_i^2}\n",
    "    $$\n",
    "    $$\n",
    "    \\sigma_{\\bar{x}}^2 = \\frac{1}{\\sum 1/\\sigma_i^2}\n",
    "    $$\n",
    "This two formula are extracted from the derivative of the log-Likelihood = 0 , that' because we are searching for a maximum.\n",
    "\n",
    "Our Maximum Likelihood Estimator (MLE) is not perfect — every estimate has an associated **uncertainty** due to the finite sample size.\n",
    "\n",
    "Under general conditions, the MLE becomes **asymptotically normal**, meaning that for large $N$, the likelihood function can be approximated by a **Gaussian** centered at the true parameter value $\\theta_0$.\n",
    "\n",
    "To quantify the uncertainty, we expand the **log-likelihood** around its maximum using a second-order **Taylor expansion**:\n",
    "\n",
    "$$\n",
    "\\log \\mathcal{L}(\\theta) \\approx \\log \\mathcal{L}(\\hat{\\theta}) - \\frac{1}{2} (\\theta - \\hat{\\theta})^2 F(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "Here, $F(\\hat{\\theta})$ is the **Fisher Information Matrix**, defined as the negative second derivative (Hessian) of the log-likelihood:\n",
    "\n",
    "$$\n",
    "F(\\theta) = - \\frac{\\partial^2}{\\partial \\theta^2 } \\log \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "The **covariance matrix** of the estimator $\\hat{\\theta}$ is then given by the **inverse** of the Fisher matrix (the Fisher Information tells you how \"sharp\" (peaked) the likelihood is around its maximum.\n",
    "The inverse of that sharpness gives the spread (uncertainty) — which is exactly the variance (or covariance, in multiple dimensions) of your estimator.):\n",
    "\n",
    "$$\n",
    "\\text{Cov}(\\hat{\\theta}) = F^{-1}(\\hat{\\theta})\n",
    "$$\n",
    "\n",
    "For a **single parameter** $\\theta$, this simplifies to:\n",
    "\n",
    "$$\n",
    "\\sigma_{\\hat{\\theta}} = \\sqrt{\\frac{1}{F(\\hat{\\theta})}}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Gaussian Likelihoods**\n",
    "\n",
    "- When the data doesn’t follow a Gaussian distribution, use the appropriate **likelihood model**, such as : **Poisson**, **Binomial**, **Exponential**, **Log-normal**, etc.\n",
    "- The MLE approach still applies: choose the model, write the likelihood, and maximize it numerically.\n",
    "- In most of the cases, you will find the same result as in the gaussian one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa14af5",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 6** - Frequentist Inference II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608a02e5",
   "metadata": {},
   "source": [
    "- Fit  \n",
    "- Outliers (Huber loss function)  \n",
    "- Goodness of fit  \n",
    "- Reduced chi-squared  \n",
    "- Model misspecification  \n",
    "- Occam’s Razor  \n",
    "- AIC (Akaike Information Criterion)  \n",
    "- Bootstrap  \n",
    "- Jackknife"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe2ed34",
   "metadata": {},
   "source": [
    "### **Fit**\n",
    "\n",
    "- Fitting means adjusting model parameters so that the model best matches the observed data.\n",
    "- Typically done by minimizing a **loss function**, such as the **sum of squared residuals** or **negative log-likelihood**.\n",
    "- The goal is to find the best estimate $\\hat{\\theta}$ that explains the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Outliers and Huber Loss Function**\n",
    "\n",
    "- **Outliers** are data points that deviate significantly from the trend of the rest of the data.\n",
    "- Summing the squares of the residuals ($\\chi^2=\\sum_{i=1}^N (y_i - M(x_i))^2/\\sigma^2$) is sensitive to outliers\n",
    "- How do we deal with outliers? By modifying the likelihood!\n",
    "- The **Huber loss** combines the squared loss for small errors and absolute loss for large errors:\n",
    "\n",
    "$$\n",
    "L_{\\text{Huber}}(t) =\n",
    "\\begin{cases}\n",
    "\\frac{1}{2} t^2 & \\text{if } |t| \\leq c \\\\\n",
    "c |t| - \\frac{1}{2} c^2 & \\text{if } |t| > c\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "- Where $t = \\left| \\frac{y - M(\\theta)}{\\sigma} \\right|$ represents the **standardized residual**, i.e. how far the observed value $y$ is from the model prediction $M(\\theta)$, in units of the known uncertainty $\\sigma$.\n",
    "- $c$ is the **tuning constant** (or confidence threshold), which determines the cutoff point where the loss switches from quadratic to linear. A common value is $c \\approx 1.345$, which gives good balance between efficiency and robustness under normal errors.\n",
    "- This approach makes the fit more **robust** to outliers: small residuals behave like in least squares, but large residuals are penalized less harshly.\n",
    "- Note that by doing this, we are effectively putting **prior information** into the analysis... infact, in a frequentist approach we prefear to re-do the measurments ore simply delete the few outliars.\n",
    "\n",
    "---\n",
    "\n",
    "### **Goodness of Fit : Reduced Chi-squared**\n",
    "\n",
    "- Measures how well the model describes the data. Remember GIGO (Garbage In Garbage Out), if the model is wrong , finding the \"best\" parameter doesn't really mean something ...\n",
    "- A good fit should show residuals randomly scattered around zero.\n",
    "\n",
    "- The **reduced chi-squared** is defined as:\n",
    "\n",
    "  $$\n",
    "  \\chi^2_{\\text{red}} = \\frac{1}{\\nu} \\sum_{i=1}^N \\left( \\frac{y_i - f(x_i)}{\\sigma_i} \\right)^2\n",
    "  $$\n",
    "\n",
    "  where **$\\nu = N - k$** is the number of **degrees of freedom** (data points minus number of parameters).\n",
    "\n",
    "- Interpretation:\n",
    "  - $\\chi^2_{\\text{red}} \\approx 1$: good fit\n",
    "  - $\\chi^2_{\\text{red}} \\gg 1$: underfitting or underestimated errors\n",
    "  - $\\chi^2_{\\text{red}} \\ll 1$: overfitting or overestimated errors\n",
    "- If the model is **wrong** (misspecified), goodness-of-fit measures can be misleading.\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Comparison, Occam’s Razor , AIC and BIC**\n",
    "- You can't do $\\chi^2_{\\text{red}}$ with Huber function, because it's not gaussian! We have to find other possibilities...\n",
    "- When comparing two models with the **same number of parameters**, we can simply compare their **maximum log-likelihood** values:\n",
    "- Larger log-likelihood ⇒ better fit\n",
    "\n",
    "The **Huber loss** clearly performs better (less negative log-likelihood), meaning it fits the data more effectively, especially in the presence of outliers.\n",
    "\n",
    "\n",
    "When models have **different numbers of parameters**, simply comparing likelihoods is not fair: more complex models might fit better **just by chance**. We need to penalize complexity — this is known as the **Occam penalty**.\n",
    "\n",
    "\n",
    "A simple method to compare models with different complexity is the **AIC** (Akaike Information Criterion):\n",
    "  \n",
    "- Lower AIC is better for the explaination of the dataset\n",
    "- It's composed by lot of term, the first one it's the $\\chi^2$, the second and third penalize model complexity\n",
    "- If models fit the data equally well, AIC prefers the one with fewer parameters.\n",
    "\n",
    "\n",
    "\n",
    "The **BIC** (Bayesian Information Criterion) is another way to compare models, especially when they have different numbers of parameters.\n",
    "It’s similar to AIC, but it **penalizes complex models more strongly**, especially when the dataset is large.\n",
    "\n",
    "\n",
    "- Lower **BIC** means a better model.\n",
    "- BIC prefers **simpler models**, especially when N is large.\n",
    "- It’s often used in **Bayesian statistics**, but doesn’t need a full Bayesian analysis, often use in frequentist analysis too.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Bootstrap**\n",
    "\n",
    "- A **resampling method** to estimate uncertainties and confidence intervals.\n",
    "- Keep attention : it create information out of nothing!\n",
    "- Steps:\n",
    "  1. Resample data (with replacement) to create many datasets. \n",
    "    The probability of getting the original dataset it's extreamly low ($N! / N^N$)\n",
    "  2. Fit the model to each resampled dataset.\n",
    "  3. Analyze the distribution of the fitted parameters.\n",
    "- Useful when analytical uncertainty is hard to compute or it's too big (such as when we have few point for a gaussian distribution).\n",
    "\n",
    "---\n",
    "\n",
    "###  **Jackknife Method** \n",
    "\n",
    "The **Jackknife** is a method to estimate the **uncertainty** (standard error) and **bias** of a statistic — like the **mean** or **standard deviation** — using your data.\n",
    "\n",
    "Suppose you have a dataset of $N$ values.\n",
    "\n",
    "1. Leave out **one** data point at a time → you get $N$ new datasets containing (N-1) points.\n",
    "2. Compute your statistic (e.g. mean, std) on each of these.\n",
    "3. From the $N$ results, estimate:\n",
    "   - A **better (bias-corrected)** value of the statistic\n",
    "   - The **uncertainty** on that value\n",
    "\n",
    "\n",
    "Jackknife works **well** when the statistic is:\n",
    "- The **mean**\n",
    "- The **standard deviation**\n",
    "\n",
    "It works **poorly** for:\n",
    "- The **median**\n",
    "- **Quantiles** (e.g. the 25th percentile)\n",
    "\n",
    "These are called **rank-based statistics**, and removing one point at a time doesn’t change them much — so the jackknife underestimates the uncertainty.\n",
    "\n",
    "\n",
    "####  **Jackknife vs Bootstrap**\n",
    "\n",
    "|                | Jackknife                | Bootstrap                |\n",
    "|----------------|--------------------------|--------------------------|\n",
    "| Type           | Leaves out one point     | Resamples with replacement |\n",
    "| Fast?          | ✅ Yes                   | ❌ Slower               |\n",
    "| Repeatable?    | ✅ Always same result     | ❌ Changes each time    |\n",
    "| Works for all stats? | ❌ Not for medians       | ✅ Yes                 |\n",
    "| Confidence intervals | ❌ Approximate         | ✅ Full distribution     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a611a0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 7** - Frequentist Inference III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27f4b80",
   "metadata": {},
   "source": [
    "- Hypothesis testing (p-value)  \n",
    "- Null hypothesis  \n",
    "- Type I and Type II errors  \n",
    "- KS test (Kolmogorov–Smirnov)  \n",
    "- Histograms  \n",
    "- Number of bins (Scott’s & Freedman–Diaconis rules)  \n",
    "- Rug plot  \n",
    "- Kernel Density Estimation (Gaussian and Epanechnikov)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0c6fe",
   "metadata": {},
   "source": [
    "### **Hypothesis Testing and p-value**\n",
    "\n",
    "Hypothesis testing is a fundamental procedure in statistics used to decide whether there is enough evidence in a sample of data to infer that a certain condition holds for the entire population.\n",
    "\n",
    "- **Null Hypothesis ($H_0$):** This is the starting assumption or the default claim about the population. It usually represents the idea that there is **no effect**, **no difference**, or **no relationship** between variables. For example, $H_0$ might state that the mean of a population is equal to a specific value.\n",
    "\n",
    "- **Alternative Hypothesis ($H_1$):** This is the hypothesis you want to test or provide evidence for. It represents a change, effect, or difference from what the null hypothesis states. \n",
    "\n",
    "- **Test Statistic:** To test the hypotheses, a test statistic is computed from the sample data. This statistic measures how far the observed data are from what would be expected if $H_0$ were true. Different tests have different statistics (e.g., t-test, z-test, chi-square test).\n",
    "\n",
    "- **p-value:** The p-value is the probability, assuming the null hypothesis $H_0$ is true, of obtaining a test statistic at least as extreme as the one observed. In other words, it quantifies how likely your data would be if there were actually no effect.\n",
    "\n",
    "  - A **small p-value** indicates that the observed data is unlikely under $H_0$, so we have evidence to reject the null hypothesis.\n",
    "  - A **large p-value** suggests the data is consistent with $H_0$, and we do not reject it.\n",
    "\n",
    "    $$\n",
    "        p_i = \\int_{x_i}^{\\infty} h_0(x)dx = 1 - \\int_{-\\infty}^{x_i}h_0(x)dx = 1- H_0(x_i)\n",
    "    $$\n",
    "\n",
    "- **Significance Level ($\\alpha$):** This is a threshold probability set before the test (commonly 0.05 or 5%). If the p-value is less than $\\alpha$, the result is called statistically significant, and we reject the null hypothesis in favor of the alternative.\n",
    "\n",
    "\n",
    "#### Important notes:\n",
    "\n",
    "- **Failing to reject $H_0$ is not the same as accepting $H_0$.** It means the data do not provide strong enough evidence against $H_0$, but $H_0$ might still be false.\n",
    "- The p-value does **not** measure the probability that $H_0$ is true or false; it only measures data compatibility with $H_0$.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "Suppose you want to test if a coin is fair.  \n",
    "- $H_0$: The coin is fair (probability of heads = 0.5).  \n",
    "- $H_1$: The coin is biased (probability of heads ≠ 0.5).\n",
    "\n",
    "You flip the coin 100 times, get 60 heads, and compute a test statistic. The p-value tells you how likely it is to get 60 or more heads assuming the coin is fair. If the p-value is below your threshold (e.g., 0.05), you reject $H_0$ and conclude the coin is likely biased.\n",
    "\n",
    "\n",
    "--- \n",
    "### **Facts about p-value**\n",
    "1) Not the chance the hypothesis is true:\n",
    "A p-value does not tell you the probability that the null hypothesis is true. Instead, it tells you how likely your results are if the null hypothesis were true.\n",
    "\n",
    "2) Not the chance it's \"just random\":\n",
    "A p-value is not the probability that your results happened by chance alone. It's based on the assumption that the null hypothesis is correct and measures how well your data fit that assumption.\n",
    "\n",
    "3) The 0.05 rule is just a guideline:\n",
    "The 0.05 cutoff for “significance” is a tradition, not a scientific rule. A result just below or above 0.05 should not be seen as automatically meaningful or meaningless.\n",
    "\n",
    "4) Doesn’t tell how big or important an effect is:\n",
    "A small p-value doesn’t mean the effect is big or important. Even tiny effects can be “significant” if the sample is large enough — that’s why we also need to consider effect size.\n",
    "---\n",
    "\n",
    "### **Type I and Type II Errors**\n",
    "\n",
    "**TYPE I ERRORS (false positives, or false alarms)**\n",
    "\n",
    "- The null hypothesis is true, but incorrectly rejected.\n",
    "- False positive probability is dictated by the significance level $\\alpha$. \n",
    "\n",
    "aka *That pixel was just background but I think it's a real source.*\n",
    "\n",
    "**TYPE II ERRORS (false negatives, or false dismissals)**\n",
    "\n",
    "- The null hypothesis is false, but not rejected.\n",
    "- False negatives probability is dictated by a variable called $\\beta$, related to $(1-\\beta)$, called the ***detection probability***.\n",
    "\n",
    "aka *That was a real galaxy but I missed it!*\n",
    "\n",
    "For a sample of size $N$ (containing background noise and sources), the **expected number of spurious sources (Type I / false positives)** is \n",
    "\n",
    "$$ n_\\mathrm{spurious} = N(1-a)\\alpha = N(1-a)\\int_{x_c}^\\infty h_B(x)dx$$ \n",
    "\n",
    "and the **expected number of missed sources (Type II / false negatives)** is\n",
    "\n",
    "$$ n_\\mathrm{missed} = Na\\beta = Na\\int_0^{x_c}h_S(x)dx.$$\n",
    "\n",
    "The **total number of classified sources** (that is number of instances where we reject the null hypothesis) is\n",
    "\n",
    "$$ n_\\mathrm{source} = Na - n_\\mathrm{missed} + n_\\mathrm{spurious} = N[(1-\\beta)a + (1-a)\\alpha].$$\n",
    "\n",
    "The **sample completeness** (or **detection probability**) is defined as\n",
    "\n",
    "$$ \\eta = \\frac{Na - n_\\mathrm{missed}}{Na} = 1-\\int_0^{x_c}h_S(x)dx = 1-\\beta$$\n",
    "\n",
    "Finally, the **sample contamination** is\n",
    "\n",
    "$$ \\epsilon = \\frac{n_\\mathrm{spurious}}{n_\\mathrm{source}}$$\n",
    "\n",
    "where $(1-\\epsilon)$ is sometimes called the **classification efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kolmogorov–Smirnov (KS) Test**\n",
    "\n",
    "- we'd like to compare two different sample and understand if they were taken from the same distribution\n",
    "- KS is a non-parametric test to compare a sample with a reference distribution, or two samples.\n",
    "- Measures the maximum distance between the empirical CDF $\\rightarrow D = max|F_1 - F_2|$\n",
    "- Outputs a statistic $D$ and a p-value.\n",
    "- amazingly D, does not dipend on the underlying distribution we care about\n",
    "- Useful to test goodness-of-fit.\n",
    "\n",
    "---\n",
    "\n",
    "### **Histograms and Number of Bins**\n",
    "\n",
    "Choosing the number of bins affects the histogram shape. The bin's width it's a hyper-parameter that has to be tune for correctly extracting the true statistics:\n",
    "\n",
    "- **Scott’s Rule:**  \n",
    "$$\n",
    "\\text{bin width} = \\Delta_b =\\frac{3.5 \\times \\sigma}{N^{1/3}}\n",
    "$$\n",
    "That's a grat rule only if we know sigma of the distribution... often it's not usable\n",
    "\n",
    "- **Freedman-Diaconis Rule:**  \n",
    "$$\n",
    "\\text{bin width} = \\Delta_b =  \\frac{2 \\times IQR}{N^{1/3}} = \\frac{2.7 \\times \\sigma_G}{N^{1/3}}\n",
    "$$\n",
    "where $IQR$ = interquartile range between 75% and 25% and $N$ = number of data points.\n",
    "\n",
    "- By making histogram you are losing some information depending on the width of the bin you are choosing; It's possible to define the bin height uncertainty by a simple rule: \n",
    "$$\n",
    "  \\sigma_k = \\frac{\\sqrt{n_k}}{\\Delta_b \\cdot N}\n",
    "$$\n",
    "where: N is the total number of data, $n_k$ it's the numer of count in the k-bin and $\\Delta$ is the bin width\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Rug Plot**\n",
    "\n",
    "- A simple plot showing individual data points as small vertical lines (ticks) along an axis.\n",
    "- Useful to visualize the distribution of data points on top of other plots (like histograms or density plots).\n",
    "\n",
    "---\n",
    "\n",
    "### **Kernel Density Estimation (KDE)**\n",
    "\n",
    "- The core idea it's not to usa a Dirac - delta in each point, but rather a distribution.\n",
    "- All this distribution (kernel) are summed up to produce the PDF.\n",
    "- Any distribution could be use:\n",
    "\n",
    "#### Common kernels:\n",
    "\n",
    "- **Gaussian kernel:**  \n",
    "$$\n",
    "K(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}\n",
    "$$\n",
    "\n",
    "- **Epanechnikov kernel:**  \n",
    "  $$\n",
    "  K(x) = \\frac{3}{4} (1 - x^2) \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$\n",
    "  parabolic with a fix support, more localized then the gaussian\n",
    "\n",
    "- **Linear (triangular) kernel:**  \n",
    "  $$\n",
    "  K(x) = 1 - |x| \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$  \n",
    "  Linearly decreasing weights as distance from the reference point increases. **Less smooth** than Gaussian.\n",
    "\n",
    "\n",
    "- **Uniform (or tophat) kernel:**  \n",
    "  $$\n",
    "  K(x) = \\frac{1}{2} \\quad \\text{for } |x| \\leq 1, \\quad 0 \\text{ otherwise}\n",
    "  $$  \n",
    "  Assigns equal weight to all points in a fixed window. Simple but **can produce less accurate estimates** due to lack of smoothness.\n",
    "\n",
    "\n",
    "KDE bandwidth controls the smoothness (similar to bin width for histograms). That's an hyper parameter that has to be tune fine before the analysis thanks to Cross Validation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8008fa4",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 8** - Bayesian Inference I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef596f",
   "metadata": {},
   "source": [
    "- Bayes recap  \n",
    "- Bayesian method  \n",
    "- Prior  \n",
    "- 3 Bayesian principles  \n",
    "- Credibility regions  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a805e709",
   "metadata": {},
   "source": [
    "\n",
    "### **Bayes Recap – Principles and Rules**\n",
    "\n",
    "Bayes' theorem allows us to **update our belief** about a hypothesis or a parameter after observing new data. The core formula is:\n",
    "\n",
    "$P(\\theta| D) = \\dfrac{P(D | \\theta) \\cdot P(\\theta)}{P(D)}$\n",
    "\n",
    "Where:\n",
    "- $\\theta$:  parameters values\n",
    "- $D$: observed data\n",
    "- $P(\\theta)$: **prior** – initial belief before seeing the data\n",
    "- $P(D | \\theta)$: **likelihood** – probability of observing $D$ assuming $\\theta$ is true\n",
    "- $P(\\theta | D)$: **posterior** – updated belief after seeing the data\n",
    "- $P(D)$: normalization constant (also called **evidence**)\n",
    "\n",
    "\n",
    "$$\n",
    "  \\text{Posterior probability} = \\frac{\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}}\n",
    "$$\n",
    "---\n",
    "\n",
    "### **Bayesian Method** \n",
    "\n",
    "1. **Define the problem** – Choose the model and the parameter $\\theta$ to estimate.\n",
    "2. **Assign the prior** $P(\\theta)$ – Express your knowledge or assumptions about $\\theta$ before the data.\n",
    "3. **Define the likelihood** $P(D | \\theta)$ – Describe how the data is generated from $\\theta$.\n",
    "4. **Compute the posterior** $P(\\theta | D)$ – Using Bayes’ theorem.\n",
    "5. **Estimate the parameter** – Use the posterior to get a point estimate (e.g. MAP, mean, median).\n",
    "6. **Quantify uncertainty** – Through credibility intervals, variance, etc.\n",
    "\n",
    "---\n",
    "\n",
    "### **Prior Distributions** \n",
    "\n",
    "In Bayesian statistics, the prior distribution represents what we believe about a parameter before observing any data. Choosing a prior is a crucial step because it influences the final result (the posterior).\n",
    "\n",
    "There are two main types of priors:\n",
    "\n",
    "1. **Informative Prior**\n",
    "\n",
    "    - A prior that incorporates specific, pre-existing knowledge or beliefs about the parameter.\n",
    "\n",
    "    - When to use it: When you have reliable background information, from previous experiments, expert opinion, or strong theoretical expectations.\n",
    "\n",
    "Example:\n",
    "Suppose you’re estimating the probability that a coin is biased towards heads.\n",
    "If previous tests suggest it lands heads ~70% of the time, you could use a Beta(7, 3) prior (centered around 0.7), which reflects your prior belief.\n",
    "\n",
    "2. **Uninformative (or non-informative) Prior**\n",
    "\n",
    "    - A prior that is intentionally vague or flat, expressing no strong belief about the parameter before seeing the data.\n",
    "\n",
    "    - When to use it: When you want the data to speak for itself and avoid influencing the result with prior assumptions.\n",
    "\n",
    "Example:\n",
    "For the same coin-flip scenario, if you have no idea about the bias, you might use a uniform prior over [0, 1], this assumes all probabilities are equally likely.\n",
    "\n",
    "\n",
    "\n",
    "When no strong prior information is available, there are three main principles to guide the choice:\n",
    "\n",
    "- **Principle of indifference**: Assign equal probabilities when there is no reason to prefer one value over another. \n",
    "- **Invariance principle**: The prior should remain consistent under reparameterization.\n",
    "- **Maximum entropy**: Among all distributions compatible with known constraints, choose the one with the highest entropy (least informative).\n",
    "\n",
    "If you choose the wrong prior, it's your fault (GIGO), only an anormus amount of data could correct your initial belif, in this case you can say that are **\"data dominated\"**, otherwise you are **\"prior dominated\"**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayesian Credible Regions**\n",
    "\n",
    "In the **frequentist paradigm**, the meaning of the confidence interval $\\mu_0 \\pm \\sigma_{\\mu}$ is \n",
    "the interval that would contain the true $\\mu$ (from which the data were drawn) in $68\\%$ (or X\\%) cases\n",
    "of a large number of imaginary repeated experiments (each with a different N values of $\\{x_i\\}$). \n",
    "\n",
    "However, the meaning of the so-called **Bayesian credible region** is fundamentally different: it is the interval that **I believe** contains the true $\\mu$  with a probability of $68%$ (or $X\\%$) after I've collected my data (my dear, one and only dataset; no imaginary repetitions). This credible region is the \n",
    "relevant quantity in the context of scientific measurements. \n",
    "\n",
    "There are several important features of a Bayesian posterior distribution:\n",
    "- They represent how well we believe a parameter is constrained within a certain range\n",
    "- We often quote the posterior maximum (**Maximum A Posteriori (MAP)**).\n",
    "- We also often quote the posterior expectation value (i.e. mean) $\\bar{\\theta} = \\int \\theta\\, p(\\theta|D)d\\theta$, or most often median (recall: robust estimator).\n",
    "- **The credible regions are not unique**. We can compute them in two different ways\n",
    "    1. We can integrate downwards from the MAP to enclose $X\\%$ (\"highest probability density interval\"), or\n",
    "    2. We can integrate inwards from each tail by $X/2\\%$ (\"equal-tailed interval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714fef0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 9** - Bayesian Inference II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252d661e",
   "metadata": {},
   "source": [
    "- Odds ratios  \n",
    "- Bayes factors  \n",
    "- Frequentist vs Bayesian  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c917c07",
   "metadata": {},
   "source": [
    "\n",
    "Model comparison and hypothesis testing in Bayesian inference are enormously different from classical/frequentist statistics. We don't have p-values here. **In Bayesian inference, we probabilistically rank models based on how well they explain the data under our prior knowledge.** \n",
    "\n",
    "### **Odds Ratio $(\\mathcal{O})$**\n",
    "\n",
    "- The **Odds Ratio** is the ratio of the posterior probabilities of two competing models $M_1$ and $M_2$, given the data D and the information at my disposition I:\n",
    "\n",
    "  $$\n",
    "    O_{21} = \\frac{p(M_2|D,I)}{p(M_1|D,I)} = \\frac{p(D\\,|\\,M_2,I)\\,p(M_2\\,|\\,I)}{p(D\\,|\\,M_1,I)\\,p(M_1\\,|\\,I)} \\equiv B_{21} \\, \\frac{p(M_2\\,|\\,I)}{p(M_1\\,|\\,I)}\n",
    "  $$\n",
    "\n",
    "- It combines the **Bayes Factor** and the **prior odds** to update our belief in which model is more likely after seeing the data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayes Factor $(\\mathcal{B})$**\n",
    "\n",
    "- The **Bayes Factor** is the ratio of marginal likelihoods (evidences) of the two models:\n",
    "\n",
    "  $$\n",
    "  B_{12} = \\frac{P(D | M_1, I)}{P(D | M_2, I)} = \\frac{\\mathcal{Z}_1}{\\mathcal{Z}_2}\n",
    "  $$\n",
    "\n",
    "- $\\mathcal{Z}$ is call evidence\n",
    "- It measures how well each model explains the observed data, **independent of prior model probabilities**.\n",
    "\n",
    "- Interpretation (Jeffreys scale):\n",
    "\n",
    "  | $B_{10}$ Value        | Strength of Evidence for $M_1$ |\n",
    "  |------------------------|-------------------------------|\n",
    "  | $<1$                   | Evidence against $M_1$        |\n",
    "  | $1 - 3$                | Weak                          |\n",
    "  | $3 - 10$               | Moderate                      |\n",
    "  | $10 - 100$             | Strong                        |\n",
    "  | $>100$                 | Decisive                      |\n",
    "\n",
    "---\n",
    "\n",
    "### **Frequentist vs Bayesian Approach**\n",
    "\n",
    "|              | **Frequentist**                                                 | **Bayesian**                                                        |\n",
    "|----------------------|------------------------------------------------------------------|----------------------------------------------------------------------|\n",
    "| **Parameters**        | Treated as **fixed but unknown** values                         | Treated as **random variables** with probability distributions       |\n",
    "| **Probability**       | Interpreted as **long-run frequency** of events                 | Interpreted as **degree of belief** or subjective certainty          |\n",
    "| **Data**              | Considered as **repeatable random samples** from a population   | Considered as **fixed once observed**; inference is updated via Bayes’ rule |\n",
    "| **Use of Prior**      | **Not used**; all inference is based on data                    | **Essential**; priors express beliefs before seeing the data         |\n",
    "| **Inference**         | Based on **sampling distribution** and repeated hypothetical experiments | Based on **posterior distribution**, combining prior and data        |\n",
    "| **Model Comparison**  | Uses **p-values, confidence intervals, likelihood ratios**      | Uses **Bayes factors, posterior probabilities, credible intervals**  |\n",
    "| **Interpretation**    | Results apply to **what would happen in repeated experiments**  | Results apply to **the current data and model assumptions**          |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e56e7",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 10** - Bayesian Inference III </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cfc965",
   "metadata": {},
   "source": [
    "- Monte Carlo  \n",
    "- Markov chains (detailed balance)  \n",
    "- MCMC (Markov Chain Monte Carlo)  \n",
    "- Metropolis–Hastings algorithm  \n",
    "- Corner plot  \n",
    "- Trace plot  \n",
    "- Burn-in  \n",
    "- Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf1f519",
   "metadata": {},
   "source": [
    "###  **Monte Carlo Methods**\n",
    "\n",
    "- **Monte Carlo methods** are computational algorithms that rely on repeated **random sampling** to estimate numerical results.\n",
    "- Widely used in physics, statistics, and Bayesian inference.\n",
    "- Particularly helpful when analytical solutions are difficult or impossible.\n",
    "- However, in high-dimensional spaces, standard Monte Carlo can become highly **inefficient**, which motivates the use of improved techniques like MCMC.\n",
    "\n",
    "---\n",
    "\n",
    "###  **Markov Chains**\n",
    "\n",
    "A **Markov chain** is a sequence of random variables (states) where the probability of moving to the next state depends **only on the current state**, not on the path taken to get there. This is called the **Markov property**, or **memorylessness**.\n",
    "\n",
    "Mathematically, if you're in state $x$ now, the probability of moving to state $x'$ is:\n",
    "$\n",
    "P(x | x')\n",
    "$\n",
    "\n",
    "#### **Stationary Distribution**\n",
    "\n",
    "A **stationary distribution** $\\pi(x)$ is a probability distribution over the states that **remains unchanged** as the Markov chain evolves.\n",
    "\n",
    "It satisfies the condition:\n",
    "$$\n",
    "\\sum_{x} \\pi(x) \\cdot P(x \\to x') = \\pi(x') \\quad \\text{for every } x'\n",
    "$$\n",
    "\n",
    "This means that the probability of being in state $x'$ at the next step is equal to the sum of the probabilities of arriving at $x'$ from all other states, taking into account:\n",
    "- How likely it was to be in each of those states ($\\pi(x)$)\n",
    " \n",
    "- And the probability of transitioning from $x$ to $x'$ ($P(x \\to x')$).\n",
    "\n",
    "---\n",
    "\n",
    "### **Detailed Balance Condition**\n",
    "\n",
    "A **sufficient condition** for a Markov chain to have a stationary distribution $\\pi(x)$ is the **detailed balance condition**:\n",
    "\n",
    "$$\n",
    "\\pi(x) \\cdot P(x \\to x') = \\pi(x') \\cdot P(x' \\to x)\n",
    "$$\n",
    "\n",
    "This condition says that the **flow of probability** from state $x$ to $x'$ is the same as from $x'$ to $x$.  \n",
    "It implies **reversibility** of the chain and ensures that $\\pi(x)$ is a stationary distribution.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Markov Chain Monte Carlo (MCMC)**\n",
    "\n",
    "- **MCMC** combines the idea of Monte Carlo sampling with Markov chains to draw samples from complex distributions.\n",
    "- The key idea is to construct a Markov chain whose stationary distribution is the **target distribution** $\\pi(x)$ (typically the posterior in Bayesian inference).\n",
    "- Even if $\\pi(x)$ is only known **up to a normalization constant**, MCMC methods can still be used to sample from it.\n",
    "\n",
    "\n",
    "####  Why Use MCMC?\n",
    "\n",
    "In Bayesian inference, we are often interested in the **posterior distribution**, but computing it explicitly is hard.  \n",
    "MCMC allows us to **approximate** this distribution by generating samples from it, rather than calculating it directly.\n",
    "\n",
    "This makes MCMC a powerful and flexible tool for inference in complex models.\n",
    "\n",
    "---\n",
    "\n",
    "### **Metropolis-Hastings Algorithm**\n",
    "\n",
    "- A widely used MCMC algorithm.\n",
    "- Generates a sequence of samples that approximates $\\pi(x)$.\n",
    "\n",
    "#### Steps:\n",
    "\n",
    "1. Start from an initial point $x$.\n",
    "2. Propose a new point $x'$ from a **proposal distribution** $T(x'|x)$.\n",
    "3. Compute the **acceptance probability** = $\\alpha$\n",
    "    $$\n",
    "    \\alpha = \\frac{\\pi(x') \\cdot T(x|x')}{\\pi(x) \\cdot T(x'|x)} \n",
    "    $$  \n",
    "\n",
    "    If the proposal distribution is **symmetric**, i.e. $T(x'|x) = T(x|x')$, the acceptance ratio simplifies to:\n",
    "\n",
    "    $$\n",
    "    \\alpha =  \\frac{\\pi(x')}{\\pi(x)} \n",
    "    $$\n",
    "4. Draw a uniform random number between 0 and 1 ... Accept $x'$ with probability $\\alpha$, otherwise stay at $x$.\n",
    "5. Repeat the process to create a Markov chain.\n",
    "\n",
    "\n",
    "#### How Can We Compute $\\alpha$ If We Don't Know $\\pi(x)$?\n",
    "\n",
    "In Bayesian inference, the target distribution $\\pi(x)$ is typically the **posterior**:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{p(x) \\cdot p(\\text{data} \\mid x)}{p(\\text{data})}\n",
    "$$\n",
    "Let’s plug in the expression for $\\pi(x)$:\n",
    "\n",
    "$$\n",
    "\\frac{\\pi(x')}{\\pi(x)} = \\frac{p(x') \\cdot p(\\text{data} \\mid x')}{p(x) \\cdot p(\\text{data} \\mid x)}\n",
    "$$\n",
    "\n",
    "Notice:  \n",
    "The term $p(\\text{data})$ appears in both numerator and denominator — **so it cancels out**!\n",
    "\n",
    "This means you only need:\n",
    "\n",
    "- The **prior** $p(x)$  \n",
    "- The **likelihood** $p(\\text{data} \\mid x)$\n",
    "\n",
    "Both of which are known or chosen by the modeler.\n",
    "\n",
    "\n",
    "KEEP ATTENTION: MA gives you a sample dataset from the posterior PDF, but not the PDF itself! you have to run some density estimation methode (KDE) for achive the correct $\\pi(x)$\n",
    "\n",
    "---\n",
    "\n",
    "### **Burn-in and Plot**\n",
    "\n",
    "- The early steps of MCMC may not represent the target distribution well.\n",
    "- The **burn-in period** refers to the initial segment of the chain that is discarded.\n",
    "- A **trace plot** shows the sampled values over steps — used to check convergence.\n",
    "- A **corner plot** (also called pair plot) is used to visualize multidimensional posterior distributions, it Shows histograms of each parameter.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Correlation Length**\n",
    "\n",
    "In an MCMC simulation, **correlation length** (also called **autocorrelation time**) refers to how many steps it takes for samples in the chain to become approximately **independent** from each other.\n",
    "\n",
    "- If samples are highly correlated, the chain is moving slowly through the space, and many steps are needed to obtain independent samples.\n",
    "- The **effective number of samples** is smaller than the total number of steps taken.\n",
    "\n",
    "This is why understanding and **reducing correlation** is important to improve sampling efficiency.\n",
    "\n",
    "We often define the correlation length $\\tau$ such that:\n",
    "\n",
    "$$\n",
    "\\text{Effective samples} \\approx \\frac{N_\\text{total}}{\\tau}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step Size Tuning**\n",
    "\n",
    "The **step size** controls how far the chain jumps between states. Choosing it well is crucial:\n",
    "\n",
    "- If the step is **too small**: the chain moves slowly, samples are highly correlated → inefficient exploration.\n",
    "- If the step is **too large**: the chain proposes states far from the current one, and many of them get **rejected** → again, inefficient.\n",
    "\n",
    "the **Goal** is balance **acceptance rate** and **decorrelation**.\n",
    "\n",
    "Typical strategy:\n",
    "- Tune the step size to achieve a **moderate acceptance rate** (e.g. ~20–40% for Metropolis-Hastings).\n",
    "- Monitor the **autocorrelation** or use **diagnostic plots** (e.g. trace plot) to check if the chain is mixing well.\n",
    "\n",
    "There is no universal best step size: it depends on the shape of the target distribution and the algorithm used.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59df8be",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 11** - Bayesian Inference IV </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed5d0e0",
   "metadata": {},
   "source": [
    "- Thinning  \n",
    "- Adaptive Metropolis  \n",
    "- Single Component Adaptive Metropolis  \n",
    "- Hamiltonian Monte Carlo  \n",
    "- Emcee  \n",
    "- Gibbs sampling  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfc1488",
   "metadata": {},
   "source": [
    "### **Thinning**\n",
    "\n",
    "**Definition**: Thinning is the practice of keeping only every *k*-th sample from an MCMC chain (e.g., every 10th sample).\n",
    "\n",
    "- The goal of MCMC is to approximate the **posterior distribution** by generating samples from it.\n",
    "- However, successive samples from the MCMC are usually **autocorrelated**.\n",
    "- Thinning attempts to reduce this autocorrelation by discarding intermediate samples.\n",
    "- **Note**: Modern practice often recommends storing all samples and addressing autocorrelation during post-processing, since thinning can discard useful information and reduce effective sample size unnecessarily.\n",
    "\n",
    "---\n",
    "\n",
    "### **Adaptive Metropolis**\n",
    "\n",
    "**Definition**: An MCMC method that adapts the proposal distribution $T$ based on the history of the chain.\n",
    "\n",
    "- In the **Metropolis-Hastings** algorithm, choosing a good proposal distribution is crucial.\n",
    "- Adaptive Metropolis (AM) automatically tunes the **covariance matrix** of the proposal distribution as the chain progresses.\n",
    "- This allows better exploration of the posterior, especially in high-dimensional or correlated parameter spaces.\n",
    "- This method doesn't use only the last point, but it use the entire chain, our chain is no longer markovian.\n",
    "- To fix this, we often let the algorithm \"learn\" during an initial phase (called the tuning stage), where it adapts the proposal. After that, we stop adapting and keep the proposal fixed — from that moment on, the chain becomes Markovian again and gives valid Bayesian results.\n",
    "---\n",
    "\n",
    "### **Single Component Adaptive Metropolis (SCAM)**\n",
    "\n",
    "**Definition**: A variant of Adaptive Metropolis where only one parameter (component) is updated at a time.\n",
    "\n",
    "- Standard MCMC or AM methods like Metropolis-Hastings suffer of low rate in high-dimensional spaces.\n",
    "- SCAM is especially useful when parameters have **different scales or conditional dependencies**.\n",
    "- At each iteration, only one dimension of the parameter vector is updated, often using an adaptive univariate proposal.\n",
    "- This can be more efficient than updating all parameters jointly, especially in the presence of strong correlations.\n",
    "- The adaptation improves sampling efficiency over time.\n",
    "\n",
    "---\n",
    "\n",
    "### **Other method**\n",
    "\n",
    "**Hamilton Monte Carlo**: An MCMC algorithm that uses concepts from physics (Hamiltonian dynamics) to make informed proposals in parameter space, improving efficiency in exploring complex posterior distributions.\n",
    "\n",
    "\n",
    "**Differential Evolution** : A population-based optimization algorithm that can be adapted for MCMC by evolving a set of candidate solutions using differences between randomly selected members of the population to guide proposals.\n",
    "\n",
    "---\n",
    "\n",
    "### **`emcee`**\n",
    "\n",
    "- It's a full python package.\n",
    "- Emcee is designed to efficiently sample from **complex, anisotropic posterior distributions**.\n",
    "- It uses multiple parallel \"**walkers**\" that share information and adapt proposals to the geometry of the target distribution.\n",
    "- The process need a starting guess, we don't need to be too precise, the chain will eventualy converge to the true value\n",
    "- Need also the **number of step** for the chain, the **burn-in region** \n",
    "- has some specific method to discard the **auto correlation lenght**\n",
    "\n",
    "---\n",
    "\n",
    "### **Gibbs Sampling**\n",
    "\n",
    "**Gibbs sampling** is a type of Markov Chain Monte Carlo (MCMC) algorithm that avoids the need for rejection — **every proposed sample is accepted**. It's especially efficient when the conditional distributions of the parameters are known and easy to sample from.\n",
    "\n",
    "\n",
    "#### **How It Works**\n",
    "\n",
    "1. **Initialize** the sampler at some starting point in parameter space.\n",
    "2. **Iterate over each parameter** in turn.\n",
    "3. For each parameter:\n",
    "   - Keep all other parameters fixed.\n",
    "   - Sample a new value from the **conditional posterior distribution** (THAT YOU HAVE TO KNOW) of that parameter.\n",
    "4. Repeat this process for **many iterations (Gibbs steps)** to build your Markov chain.\n",
    "\n",
    "\n",
    "####  **Key Features**\n",
    "\n",
    "-  Every sample is **automatically accepted** (no rejection step).\n",
    "-  Very **fast and efficient**, especially in high-dimensional spaces.\n",
    "-  **Short burn-in period** — reaches equilibrium quickly.\n",
    "-  **Limitation**: Requires knowledge of the **conditional distributions** of all parameters.\n",
    "\n",
    "\n",
    "Use it when:\n",
    "- You know how to compute and sample from the **conditional posterior distributions**.\n",
    "- You need a **simple, efficient MCMC** method.\n",
    "\n",
    "---\n",
    "\n",
    "### **Conjugate Prior**\n",
    "\n",
    "**Definition**:  \n",
    "In Bayesian statistics, a **conjugate prior** is a prior distribution that, when combined with a particular **likelihood function**, results in a **posterior distribution** that is in the same family as the prior.\n",
    "\n",
    "- It simplifies calculations.\n",
    "- The posterior has a known and tractable form.\n",
    "- Useful for analytical solutions and understanding posterior updates.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8444528",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 12** - Bayesian Inference V </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6fa6731",
   "metadata": {},
   "source": [
    "- Savage–Dickey ratio  \n",
    "- Nested Sampling – Dynesty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba39c60a",
   "metadata": {},
   "source": [
    "### **Savage – Dickey Density Ratio**\n",
    "\n",
    "It's a shortcut to compare two models when one is a **special case** of the other.\n",
    "\n",
    "Let’s say:\n",
    "\n",
    "- $M_1$: the **simple model**, where a parameter $A = 0$ (e.g., \"no signal\", A stay for Amplitude)\n",
    "- $M_2$: the **full model**, where $A$ can be anything (e.g., \"signal allowed\")\n",
    "\n",
    "Then, instead of computing evidence for both models, we use this trick:\n",
    "\n",
    "$$\n",
    "\\mathcal{B} = \\frac{p(A = 0)}{p(A = 0 \\mid \\text{data})}\n",
    "$$\n",
    "\n",
    "This is the **Bayes factor** between $M_2$ and $M_1$.\n",
    "\n",
    "#### What Does It Mean?\n",
    "\n",
    "- $p(A = 0)$ is how much we believed in $A = 0$ **before seeing the data**.\n",
    "- $p(A = 0 \\mid \\text{data})$ is how much we believe in $A = 0$ **after seeing the data**.\n",
    "\n",
    "If the data makes $A = 0$ **less likely**, then $M_1$ is disfavored.\n",
    "\n",
    "\n",
    "#### How Do We Use It?\n",
    "\n",
    "1. Run MCMC on the full model $M_2$.\n",
    "2. From the samples, estimate $p(A = 0 \\mid \\text{data})$.\n",
    "   - For example, use a histogram or KDE on the samples of $A$.\n",
    "3. Calculate $\\mathcal{B}$ using the ratio.\n",
    "\n",
    "\n",
    "#### Why Is This Useful?\n",
    "\n",
    "- You don’t need to compute full evidences $\\mathcal{Z}_1$ and $\\mathcal{Z}_2$.\n",
    "- Fast and simple, especially when comparing null hypotheses.\n",
    "\n",
    "\n",
    "| Concept                 | Description                                                                                                                                                                                                                                                                                                                                                                                       |\n",
    "| ----------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **Bayes Factor**        | A **general method** to compare two models (\\$M\\_1\\$ and \\$M\\_2\\$) by taking the ratio of their **evidences**:  $\\mathcal{B}_{21} = \\frac{P(\\text{data} \\mid M_2)}{P(\\text{data} \\mid M_1)}$ It tells you how much more the data supports one model over the other. Requires full integration over parameter space.                                                                               |\n",
    "| **Savage–Dickey Ratio** | A **special case** of the Bayes factor, used **only** when: <br>– \\$M\\_1\\$ is a **special case** of \\$M\\_2\\$ (e.g. \\$A=0\\$) <br>– You can compute the **prior** and **posterior density** at a single point (e.g. \\$A=0\\$). <br><br>Then:  $\\mathcal{B}_{21} = \\frac{p(A=0)}{p(A=0 \\mid \\text{data})}$ It's a shortcut that **gives the Bayes factor** without needing to compute full evidences. |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Nested Sampling**\n",
    "\n",
    "Nested Sampling is a method to **compute the evidence** $\\mathcal{Z}$ in Bayesian inference:\n",
    "\n",
    "$$\n",
    "\\mathcal{Z} = \\int \\mathcal{L}(\\theta) \\, \\pi(\\theta) \\, d\\theta\n",
    "$$\n",
    "\n",
    "This is crucial for **comparing models**, since:\n",
    "\n",
    "$$\n",
    "\\text{Bayes Factor} = \\frac{\\mathcal{Z}_2}{\\mathcal{Z}_1}\n",
    "$$\n",
    "\n",
    "But computing $\\mathcal{Z}$ is hard — especially in high dimensions. That’s where Nested Sampling comes in.\n",
    "\n",
    "\n",
    "#### How It Works\n",
    "\n",
    "1. Start with $N$ random points from the **prior**.\n",
    "2. Find the one with the **lowest likelihood**, call it $L_{\\text{min}}$.\n",
    "3. Remove it, and **replace** it with a new point sampled from the prior **subject to** $\\mathcal{L} > L_{\\text{min}}$.\n",
    "4. Keep track of the shrinking prior volume and the associated likelihoods.\n",
    "5. Approximate the 1D integral $\\mathcal{Z}$ using the sequence of $(X_i, \\mathcal{L}_i)$ points.\n",
    "\n",
    "In Python, the `dynesty` library does this automatically:\n",
    "\n",
    "---\n",
    "\n",
    "### **Very important** \n",
    "- Samples that come out of a nested sampling runs are **weighted**. The results of nested sampling and the samples and their weights *together*. Do not use samples by themselves, it doesn't make sense.\n",
    "- If an MCMC is taking to long, you can interrupt it and use samples you got so far (assuming you're past the burn in period). This is not possible with nested sampling; samples don't make any sense until the algorithm reached the top of the posterior! You have to let it go till the end.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd42ce45",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 13** - Data Mining & Machine Learning </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37232c4",
   "metadata": {},
   "source": [
    "- cos'è il machine learning e scopo principale\n",
    "- features / sample / classes / istanze\n",
    "- sci-kit cos'è e come vuole i dati\n",
    "- seguenti metodi di sci-kit ( model.fit, model.predict , model.predict_proba, model.score ,model.trasform)\n",
    "- sci - kit estimator object \n",
    "- supervisionato (classificazione e regressione) esempio netflix\n",
    "- KNN velocemente\n",
    "- non supervisionato, cosa cambia da prima (clustering, dimensionality reduction), spiegazione semplice e veloce di ciascuno\n",
    "- PCA velocemente\n",
    "- isomap \n",
    "- clustering k-means velocemente\n",
    "- model validation ( confusion matrix,  training set / test set , )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc59d82b",
   "metadata": {},
   "source": [
    "### **What is Machine Learning and Its Main Purpose**\n",
    "\n",
    "Machine Learning (ML) is a branch of artificial intelligence where computers learn patterns from data to make decisions or predictions without being explicitly programmed. The main goal is to build models that generalize well on new, unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Terms**\n",
    "\n",
    "- **Features:** The input variables or attributes used to describe each data point (e.g., height, weight).  \n",
    "- **Sample / Instance:** A single data point or observation with its features (e.g., one person's measurements).  \n",
    "- **Classes:** Categories or labels that data points belong to in classification tasks (e.g., cat, dog).  \n",
    "- **Target:** The output or label we want to predict.\n",
    "\n",
    "---\n",
    "\n",
    "### **`Scikit-learn`** \n",
    "\n",
    "Scikit-learn is a popular Python library for machine learning. It provides easy-to-use tools for data preprocessing, modeling, and evaluation.\n",
    "\n",
    "- **Data format:**  \n",
    "  - Input features: a 2D array/matrix of shape $(n\\_samples, n\\_features)$.  Always in this form, it's very picky.\n",
    "  - if your x is just 1D, you have to reshape it in ND by using `np.newaxis()`\n",
    "  - Target labels: A 1D array of length $n\\_samples$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Scikit-learn Methods**\n",
    "\n",
    "An **estimator** in scikit-learn is any object implementing at least the methods `.fit()` and `.predict()` or `.transform()`. \n",
    "Usually call model.\n",
    "\n",
    "- **`model.fit(X, y)`:** Trains the model on data $X$ with labels $y$.  \n",
    "- **`model.predict(X)`:** Predicts labels for new data $X$.  \n",
    "- **`model.predict_proba(X)`:** Gives the probability estimates for classification classes (if available).  \n",
    "- **`model.score(X, y)`:** Returns the accuracy on data $X$ compared to true labels $y$.  \n",
    "- **`model.transform(X)`:** Applies a transformation to data $X$ (used in dimensionality reduction, feature extraction).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Supervised Learning**\n",
    "\n",
    "In supervised learning, the model learns from labeled data:\n",
    "\n",
    "- **Classification:** Predicting discrete labels. We will use the propriety of a dataset to predict unlabeled data.\n",
    "- **Regression:** Predicting continuous values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Unsupervised Learning**\n",
    "\n",
    "Unsupervised learning deals with unlabeled data:\n",
    "\n",
    "- **Clustering:** Grouping similar data points .  \n",
    "- **Dimensionality Reduction:** Reducing data features while preserving structure , usefull in data visualization.\n",
    "- **Density Estimation** can determine the distribution of the data within the parameter space.\n",
    "\n",
    "The main difference is that no label is provided; the goal is to find hidden patterns or structure.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Model Validation**\n",
    "Determine how well your model will generalize from the training dataset to future unlabeled data.\n",
    "\n",
    "- **Confusion Matrix:** A matrix showing true vs. predicted classes to evaluate classification accuracy and errors.  The element on the diagnal are the one correctly identified, the off-diagonal are confunded.\n",
    "- **Training Set:** Data used to train the model.  \n",
    "- **Test Set:** Separate data used to evaluate model performance on unseen (but labeled) data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626dbe69",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 14** - Clustering </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbcc0cd",
   "metadata": {},
   "source": [
    "- whats are hyperparameter and make example\n",
    "- cross validation hyper parameter tuning\n",
    "- training  / validation / test \n",
    "- K - fold cross validation\n",
    "- mix con mcmc se massimo si trova tra due punti\n",
    "- clustering, non sappiamo come faccia , ma funziona\n",
    "- mean shift clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b66ea",
   "metadata": {},
   "source": [
    "### **Hyperparameters**\n",
    "\n",
    "Hyperparameters are parameters that are **not learned from the data**, but set **before** the learning process begins. They control the learning process and model structure.\n",
    "They can easily fool us into thinking something wrong about the data.\n",
    "\n",
    "### Examples:\n",
    "- Number of clusters in K-means ($K$)\n",
    "- number of bins in a histogramm\n",
    "- Depth of a decision tree\n",
    "- Bandwidth in Kernel Density Estimation (KDE)\n",
    "\n",
    "These are typically chosen via **validation** methods like **cross-validation** (see below).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Training / Validation / Test Sets**\n",
    "\n",
    "We can think of divide the datasei into:\n",
    "\n",
    "1. **Training set**: Used to **fit** the model.\n",
    "2. **Validation set**: Used to **tune hyperparameters** and select the best model version.\n",
    "3. **Test set**: Used only **at the end** to report the final unbiased performance of the selected model.\n",
    "\n",
    "But we know that less data is bad for ML, and also make the result dipendent on what is inside each set (think at outliars, if they fall into test gave a different risult...)\n",
    "We can solve this problem thanks to Cross Validation (CV)\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Fold Cross Validation**\n",
    "\n",
    "Cross-validation is a technique to **evaluate the generalization ability** of a model by partitioning the data into multiple subsets.\n",
    "\n",
    "### Why it matters:\n",
    "- Helps prevent **overfitting** and **underfitting**\n",
    "- Makes full use of data (especially important with small datasets)\n",
    "- Gives a better estimate of model performance\n",
    "\n",
    "K-Fold is a smarter form of cross-validation:\n",
    "\n",
    "1. Split the Training data into $K$ equal parts (folds).\n",
    "2. For each fold:\n",
    "   - Use $K-1$ folds for training\n",
    "   - Use the remaining fold for validation\n",
    "3. Repeat $K$ times so every point gets to be in the validation set once.\n",
    "4. Extract the best parameter from the $K$ validation.\n",
    "5. Use the Test data to evaluate the model.\n",
    "\n",
    "We can take it to extreme by taking K = N = number of data, this is called \"Leave one  out\" Cross Validation, This drammatically increase the computational cost, but reduce gratly the variance.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## **Clustering**\n",
    "\n",
    "Clustering is **unsupervised learning**: grouping similar data points without knowing the labels.  \n",
    "It aims to discover structure in data by finding clusters (dense regions) of similar observations.\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Means Clustering**\n",
    "\n",
    "**K-Means** is a simple and popular **centroid-based** clustering algorithm.\n",
    "\n",
    "#### How it works:\n",
    "1. Choose the number of clusters $k$ (a hyperparameter).\n",
    "2. Initialize $k$ **centroids** (usually randomly).\n",
    "3. Assign each data point to the **nearest centroid**.\n",
    "4. Update centroids as the **mean of points assigned** to each cluster.\n",
    "5. Repeat steps 3–4 until convergence (no significant change in centroids or assignments).\n",
    "\n",
    "**Objective:** Minimize the **within-cluster sum of squares** (WCSS):\n",
    "\n",
    "$$\n",
    "\\text{WCSS} = \\sum_{i=1}^{k} \\sum_{x \\in C_i} \\|x - \\mu_i\\|^2\n",
    "$$\n",
    "\n",
    "where $C_i$ is the set of points in cluster $i$ and $\\mu_i$ is the centroid of $C_i$. $K$ is the number of cluster. You are minimizing the distance between points and centroid for each cluster\n",
    "\n",
    "#### Strengths:\n",
    "- Efficient and scalable\n",
    "- Easy to implement\n",
    "\n",
    "#### Weaknesses:\n",
    "- Requires choosing $k$\n",
    "- Assumes spherical, equally sized clusters\n",
    "- Sensitive to initialization\n",
    "- Sensitive to outliers\n",
    "\n",
    "---\n",
    "\n",
    "### **Mean Shift Clustering**\n",
    "\n",
    "**Mean Shift** is a **non-parametric**, **density-based**, **centroid-shifting** clustering algorithm.\n",
    "\n",
    "#### How it works (Density Gradient Ascent):\n",
    "1. Define a **window (kernel)** around each data point — typically a Gaussian with bandwidth $h$.\n",
    "2. Compute the **mean of data points** within the window.\n",
    "3. Move (shift) the window center toward the **mean**.\n",
    "4. Repeat steps 2–3 until convergence (i.e., the center stops moving).\n",
    "5. Merge points converging to the same center into a **single cluster**.\n",
    "\n",
    "\n",
    "It performs **gradient ascent** on a kernel density estimate (KDE). The mode (maximum) of the KDE becomes the cluster center.\n",
    "\n",
    "\n",
    "#### Strengths:\n",
    "- **Does not require predefining the number of clusters**\n",
    "- Can identify clusters of **arbitrary shape**\n",
    "- **Robust** to outliers\n",
    "- Works well when clusters correspond to **modes in the density**\n",
    "\n",
    "#### Weaknesses:\n",
    "- Computationally expensive (especially on large datasets)\n",
    "- **Bandwidth selection** is critical — too large merges clusters, too small splits them\n",
    "- Does not scale well in high dimensions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e702ced8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 15** - Dimensional Reduction I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d3afd",
   "metadata": {},
   "source": [
    "- Curse of Dimensionality\n",
    "- PCA (apply a trasform to the data such that the new axes are aligned with the maximal variance of the data, ortogonalization in more dimension, fewer then the original dimension: some are discard, at the end of the game it's a diagonalization)\n",
    "- data preparization for PCA: subtract the mean, divide by the variance , normalize eachsample\n",
    "- in spectral imaging from galaxxy, every peak grow on a background , that's not noise, it's physisc, but i can remove it thanks to PCA\n",
    "- scree plot, first 2 comoponent exlain 96% of the variance in our example\n",
    "- interpreting PCA result\n",
    "- PCA it's linear, it struggle a lot with non linear component\n",
    "- Recostruction of dark area with PCA\n",
    "- overview of non-negative  matrix factorization\n",
    "- overvie (just know it exist) of ICA "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea595a6",
   "metadata": {},
   "source": [
    "### **Curse of Dimensionality**\n",
    "\n",
    "As the number of features (dimensions) increases:\n",
    "- The volume of space increases exponentially.\n",
    "- Data becomes **sparse**, even if you have a lot of it.\n",
    "- Models struggle to generalize well.\n",
    "- Distance metrics (like Euclidean distance) lose meaning — all points start to look equally distant.\n",
    "\n",
    "Example: If each feature has a 50% chance of matching, the probability that all $n$ match is $0.5^n$. Even with just 4 features, that’s only 6.25%!\n",
    "\n",
    "---\n",
    "### **PCA (Principal Component Analysis)**\n",
    "\n",
    "PCA is a technique to **reduce dimensionality** by projecting data to a new space:\n",
    "- New axes are the directions of **maximum variance**.\n",
    "- These axes (principal components) are **orthogonal**.\n",
    "- Redundant dimensions are **discarded**.\n",
    "- The process is equivalent to **diagonalizing the covariance matrix**.\n",
    "\n",
    "#### Steps:\n",
    "1. **Center the data**: Subtract the mean.\n",
    "2. **Scale the data**: Divide by the standard deviation.\n",
    "3. **Normalize samples** (optional, done for spectral images).\n",
    "4. Compute the **covariance matrix**.\n",
    "5. Compute **eigenvectors/eigenvalues**.\n",
    "6. Sort eigenvectors by decreasing eigenvalue → these are the principal components. (eingvalues reflect the variance)\n",
    "\n",
    "One you have the eigenvectors $e_j(k)$, you can recostruct a true data $x_i(k)$ in the eigenvecture basis as: \n",
    "\n",
    "$$\n",
    "    x_i(k) = \\mu(k) + \\sum_j \\theta_{ij}e_j(k)\n",
    "$$\n",
    "\n",
    "#### PCA Limitations\n",
    "\n",
    "- **Linear**: PCA can’t handle **non-linear structures** in data.\n",
    "- Struggles with **curved manifolds** (e.g., spirals).\n",
    "- We would need all the component for the 100% exlaination\n",
    "- how many component should i keep? Cross validation is the answer ...\n",
    "\n",
    "let's look at the video for a better comprensation of how it work\n",
    "\n",
    "---\n",
    "\n",
    "### **Scree Plot & Explained Variance**\n",
    "\n",
    "A **scree plot** shows eigenvalues (variance explained) by each principal component.\n",
    "\n",
    "In our exaple the first 2 components explain 96% of the variance:\n",
    "- So you can reduce your data to 2D while keeping most information.\n",
    "- Useful for **visualization** and **noise reduction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Dark Area Reconstruction with PCA**\n",
    "\n",
    "You can use PCA to reconstruct **missing or corrupted data** (e.g., missing regions in astronomical images):\n",
    "- Fit PCA on complete data.\n",
    "- Project corrupted sample into the PC space.\n",
    "- Reconstruct using the leading PCs → fill in missing values based on structure learned from the rest.\n",
    "\n",
    "---\n",
    "\n",
    "### **Overview: Non-Negative Matrix Factorization (NMF)**\n",
    "\n",
    "**NMF** is a technique that factorizes a matrix $X$ into the product of two matrices:\n",
    "\n",
    "$$\n",
    "X \\approx WH\n",
    "$$\n",
    "\n",
    "with the important constraint that **all elements of** $W$ and $H$ are **non-negative** (i.e., no negative numbers allowed).\n",
    "\n",
    "This makes the results easier to interpret in many real-world cases, especially when the data naturally can't be negative (like pixel intensities or word counts).\n",
    "\n",
    "**Applications:**\n",
    "- Discovering topics in a collection of documents\n",
    "- Breaking down images into basic components\n",
    "- Analyzing spectral data in astronomy or chemistry\n",
    "\n",
    "Compared to **PCA**, which can use negative values, **NMF tends to produce more interpretable results**, often representing **distinct parts** of the input (like separate topics or objects).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Just Know It Exists: ICA (Independent Component Analysis)**\n",
    "\n",
    "**ICA** is another method for decomposing data, but instead of focusing on variance (like PCA), it looks for **independent** components.\n",
    "\n",
    "That means it tries to separate a complex signal into **underlying sources** that are as statistically **independent** from each other as possible.\n",
    "\n",
    "**Typical use cases:**\n",
    "- **Blind source separation** (e.g., separating different voices recorded by multiple microphones)\n",
    "- Analyzing EEG brain signals\n",
    "- Uncovering independent trends in financial time series\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122589a8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 16** - Dimensional Reduction II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb5440",
   "metadata": {},
   "source": [
    "- random forest\n",
    "- manifold learning techniques\n",
    "- Locally Linear Embedding (what is it, scheme of what is doing)\n",
    "- IsoMap (what is it, scheme of what is doing)\n",
    "- t - SNE (overview)\n",
    "- Density Estimation (recup)\n",
    "- non parametric DE ( KDE , Nearest-Neighbor Density Estimation)\n",
    "- Parametric Density Estimation (Gaussian Mixture Models) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d1156c",
   "metadata": {},
   "source": [
    "PCA, ICA and NFM are useless in handwritten dataset... they fail, let's have a look at other possible dimensionality reduction, more helpfull in this case.\n",
    "\n",
    "### **Random Forest**\n",
    "\n",
    "**Random Forest** is a machine learning method that helps make **predictions** — like: Is this email spam? What’s the price of this house?\n",
    "\n",
    "- It builds **many decision trees**.\n",
    "- Each tree gives its own answer.\n",
    "- Then it **combines the answers**:\n",
    "  - For classification (like spam/not spam): it chooses the **most common** answer (majority vote).\n",
    "  - For regression (like price): it takes the **average**.\n",
    "\n",
    "How the trees are built:\n",
    "\n",
    "- Each tree is trained on a **random sample** of the data called Bootstrap.\n",
    "- Each tree looks at only a **random set of features** when making decisions.\n",
    "\n",
    "**Random Forest = Many random trees working together to make smart predictions!**\n",
    "\n",
    "**Key Idea**: Combine many weak learners (trees) into a strong learner.\n",
    "\n",
    "---\n",
    "\n",
    "## **Manifold Learning Techniques**\n",
    "\n",
    "Manifold learning methods are **non-linear dimensionality reduction techniques** that assume data lies on a low-dimensional manifold embedded in a high-dimensional space. These techniques aim to **uncover the underlying structure** of the data.\n",
    "\n",
    "\n",
    "### **Locally Linear Embedding (LLE)**\n",
    "\n",
    "LLE is a **non-linear dimensionality reduction** algorithm that preserves local neighborhoods geometry around each point.\n",
    "\n",
    "**What is it?**\n",
    "\n",
    "- Assumes each data point and its neighbors lie on a locally linear patch of the manifold.\n",
    "- Computes weights that best reconstruct each point from its neighbors.\n",
    "- Finds low-dimensional embeddings that best preserve these local relationships.\n",
    "\n",
    "**Scheme:**\n",
    "\n",
    "1. For each point, identify $k$ nearest neighbors.\n",
    "2. Compute weights $w_{ij}$ to reconstruct point $x_i$ **from** its neighbors:  \n",
    "   $x_i \\approx \\sum_j w_{ij} x_j$ where j are the neighbour points\n",
    "3. Find low-dimensional representations $y_i$ that minimize the distance between $x_i$ and new space point $y_i$ \n",
    "\n",
    "\n",
    "### **IsoMap**\n",
    "\n",
    "IsoMap is a **global non-linear dimensionality reduction** method that preserves geodesic distances between points.\n",
    "This method assumes data lies on a smooth manifold.\n",
    "\n",
    "**Scheme:**\n",
    "\n",
    "1. Construct a neighborhood graph (e.g., $k$-nearest neighbors).\n",
    "2. Compute shortest paths (geodesic distances) between all pairs using Dijkstra .\n",
    "3. Apply classical MDS (Multi-Dimensional Scaling) to the geodesic distance matrix.\n",
    "\n",
    "IsoMap preserves the **intrinsic geometry** of the data better than PCA in non-linear settings.\n",
    "\n",
    "\n",
    "### **t-SNE** \n",
    "\n",
    "**t-SNE** is a tool that helps you **visualize data** with many features (high-dimensional) in just **2D or 3D**.\n",
    "\n",
    "What it does:\n",
    "\n",
    "- It takes complex data (with lots of numbers/features) and shows it as a simple **2D or 3D plot**.\n",
    "- Points that are **similar** in the original data end up **close together** in the plot.\n",
    "- It’s really good at showing **clusters** or groups in the data.\n",
    "\n",
    "How it works:\n",
    "\n",
    "- Figures out how similar the data points are.\n",
    "- Then tries to **keep those similarities** when showing the data in 2D or 3D.\n",
    "- Uses special math (like **probabilities** and **Student-t distribution**) to do it well.\n",
    "\n",
    "\n",
    "In short : **t-SNE = A smart way to draw complex data in 2D or 3D so you can spot patterns.**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary Table of Dimensionality Reduction Methods**\n",
    "\n",
    "| Method     | Type        | Preserves     | Suitable for Visualization | Parametric | Main Use Case                          |\n",
    "|------------|-------------|----------------|-----------------------------|------------|----------------------------------------|\n",
    "| PCA        | Linear      | Global dist    | Yes                         | Yes        | General reduction, noise filtering     |\n",
    "| NMF        | Linear      | Parts-based    | Limited                     | Yes        | Topic modeling, text data              |\n",
    "| ICA        | Linear      | Indep. sources | No                          | Yes        | Signal separation (e.g., EEG, audio)   |\n",
    "| LLE        | Non-linear  | Local linearity| Yes                         | No         | Manifold learning, visualizing clusters|\n",
    "| IsoMap     | Non-linear  | Geodetics      | Yes                         | No         | Unfolding non-linear structures        |\n",
    "| t-SNE      | Non-linear  | Probability    | Yes                         | No         | Visualization of high-dim. data        |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Density Estimation (Recap)**\n",
    "\n",
    "Density estimation aims to model the **probability distribution** of a dataset based on observed data.\n",
    "\n",
    "Two broad classes:\n",
    "\n",
    "1. **Parametric**: Assumes a specific distribution (e.g., Gaussian).\n",
    "2. **Non-Parametric**: Makes fewer assumptions; adapts to data complexity.\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Parametric Density Estimation**\n",
    "\n",
    "**Kernel Density Estimation (KDE)**\n",
    "\n",
    "- Places a kernel (e.g., Gaussian) at each data point.\n",
    "- Estimates the density at a point $x$ \n",
    "- The bandwidth controls the smoothness of the resulting density.\n",
    "We can think of it by replacing each point with a probability cloud\n",
    "\n",
    "**Nearest-Neighbor Density Estimation**\n",
    "\n",
    "- Estimates density based on the volume $V_k$ containing the $k$ nearest neighbors of $x$.\n",
    "- Formula:  \n",
    "  $\\hat{f}(x) = \\frac{k}{n V_k}$\n",
    "\n",
    "- Adapts well to **local variations** in data density.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parametric Density Estimation: Gaussian Mixture Models (GMM)**\n",
    "\n",
    "GMM assumes that the data is generated from a **mixture of several Gaussian distributions**.\n",
    "\n",
    "**Definition:**\n",
    "\n",
    "- Probability density function:  \n",
    "  $p(x) = \\sum_{k=1}^K \\pi_k \\mathcal{N}(x \\mid \\mu_k, \\Sigma_k)$  \n",
    "  where $\\pi_k$ are the mixing coefficients, and $\\mathcal{N}$ is the multivariate normal.\n",
    "\n",
    "**Estimation:**\n",
    "\n",
    "- Parameters $(\\pi_k, \\mu_k, \\Sigma_k)$ are learned using the **Expectation-Maximization (EM)** algorithm.\n",
    "\n",
    "GMMs are widely used in **clustering**, **anomaly detection**, and **density modeling**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deed4c0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 17** - Regression I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71913440",
   "metadata": {},
   "source": [
    "- regression \n",
    "- bayesian regression\n",
    "- linear regression ( homoschedastic , SciKit.LinearRegression())\n",
    "- polynomial regressione\n",
    "- basis regression\n",
    "- kernel regression / nadara - watson regression\n",
    "- over / under fitting - CrossValidation for the best model\n",
    "-  andamento RMS or BIC Vs degree polinomial fitting\n",
    "- learning curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c23571",
   "metadata": {},
   "source": [
    "### **Regression (What is it?)**\n",
    "\n",
    "Regression is the **supervised** process that try to find the relation between x and y.\n",
    "\n",
    "That is, for a given $x$, instead of trying to estimate the **full probability distribution function (PDF)** of $y$, we often settle for a **point estimate** — the most likely expected value.\n",
    "\n",
    "Crudely: regression = **curve fitting**: finding the best function that explains the observed data.\n",
    "\n",
    "In contrast with **unsupervised learning** (like clustering), regression **requires labeled data** — pairs of $(x_i, y_i)$.\n",
    "\n",
    "---\n",
    "\n",
    "### **Bayesian Regression**\n",
    "\n",
    "In **regular regression** (like least squares), we try to find **one best-fit line** through the data.\n",
    "\n",
    "But in **Bayesian regression**, we don’t just pick one line — we look at **many possible lines**, and figure out how likely each one is.\n",
    "\n",
    "How it works (in simple terms):\n",
    "\n",
    "- We **start with a belief** (called a **prior**) about what the model could look like.\n",
    "- Then we **update that belief** using the data we observe (this gives us the **posterior**).\n",
    "- The result is a **range of possible models**, not just one.\n",
    "\n",
    "What makes it special:\n",
    "\n",
    "- It gives **probabilistic predictions** — we get a prediction *and* how uncertain it is.\n",
    "- It’s **regularized by priors**, meaning it avoids overfitting by starting with assumptions.\n",
    "- It’s great when we want to **include uncertainty** in our results.\n",
    "\n",
    "\n",
    "When to use it:\n",
    "\n",
    "- When you care about **uncertainty** in predictions\n",
    "- When data is **limited** or **noisy**\n",
    "- When you want to **combine prior knowledge** with data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear Regression (Homoscedastic)**\n",
    "\n",
    "This models the response $y$ as a **linear function of inputs**:\n",
    "\n",
    "$$\n",
    "y = \\theta_1 x + \\theta_0 + \\epsilon\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ is a noise term assumed to have **constant variance** (homoscedasticity):\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "Each data point restricts the set of plausible lines in parameter space $(\\theta_0, \\theta_1)$. As more points are added, the intersection of these constraints narrows.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "\n",
    "### **Linear Regression (Heteroscedastic)**\n",
    "\n",
    "This still models the response $y$ as a **linear function of inputs**:\n",
    "\n",
    "$$\n",
    "y = \\theta_1 x + \\theta_0 + \\epsilon\n",
    "$$\n",
    "\n",
    "But now, the **variance of the noise** is **not constant** across all data points — this is called **heteroscedasticity**:\n",
    "\n",
    "$$\n",
    "\\epsilon \\sim \\mathcal{N}(0, \\sigma^2(x))\n",
    "$$\n",
    "\n",
    "This means that:\n",
    "\n",
    "- Some data points are more \"reliable\" (lower variance).\n",
    "- Others are noisier and should influence the model **less**.\n",
    "- The model should **give different weights** to different data points when fitting.\n",
    "\n",
    "If the errors are different for each point, it is better to think of the problem in matrix notation:\n",
    "\n",
    "$$\n",
    "Y = M \\theta\n",
    "$$\n",
    "\n",
    "where $Y$ is an $N$-dimensional vector of values $y_i$,\n",
    "\n",
    "$$\n",
    "Y = \n",
    "\\begin{bmatrix}\n",
    "y_0 \\\\\n",
    "\\vdots \\\\\n",
    "y_{N-1}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "For the straight line model, $\\theta$ is simply a two-dimensional vector of regression coefficients,\n",
    "\n",
    "$$\n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_0 \\\\\n",
    "\\theta_1\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "and $M$ is called the design matrix\n",
    "\n",
    "$$\n",
    "M =\n",
    "\\begin{bmatrix}\n",
    "1 & x_0 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_{N-1}\n",
    "\\end{bmatrix},\n",
    "$$\n",
    "\n",
    "where the constant in the first column of $M$ captures the zeropoint (i.e. the constant $y$-intercept) in the regression.\n",
    "\n",
    "\n",
    "### **Multivariative**\n",
    "\n",
    "It's simply as befor, but instead of have only 2 dimension x and y, you can add more variable, such as y = ax + bz + ck + ...\n",
    "Of course a,b,c ... are achived from the **designed matrix**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Polynomial Regression**\n",
    "\n",
    "Extends linear regression by adding polynomial terms:\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x + w_2 x^2 + \\dots + w_d x^d + \\epsilon\n",
    "$$\n",
    "\n",
    "This is still **linear in parameters**, just not linear in $x$.\n",
    "\n",
    "- More expressive models\n",
    "- Risk of **overfitting** for large degree $d$\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "model = make_pipeline(PolynomialFeatures(degree=5), LinearRegression())\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "In this case the design matrix became:\n",
    "\n",
    "$$\n",
    "M = \\begin{pmatrix}\n",
    "1 & x_0 & x_0^2 & x_0^3 \\\\\n",
    "1 & x_1 & x_1^2 & x_1^3 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "1 & x_N & x_N^2 & x_N^3\n",
    "\\end{pmatrix}.\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Basis Function Regression**\n",
    "\n",
    "We use arbitrary **basis functions** $\\phi_j(x)$:\n",
    "\n",
    "Examples:\n",
    "- Polynomial: $\\phi_j(x) = x^j$\n",
    "- Gaussian: $\\phi_j(x) = \\exp\\left(-\\frac{(x - \\mu_j)^2}{2\\sigma^2}\\right)$\n",
    "- Fourier: $\\phi_j(x) = \\cos(jx), \\sin(jx)$\n",
    "\n",
    "By choosing a suitable basis, you can fit almost any shape.\n",
    "\n",
    "---\n",
    "\n",
    "### **Kernel Regression / Nadaraya-Watson Estimator**\n",
    "\n",
    "In the case of Gaussian Basis Regression, Gaussians are evenly spaced over the range of interest. If we instead placed Gaussians at the location of every data point, we get Gaussian Kernel Regression instead. Or just Kernel Regression more generally since we don't have to have a Gaussian kernel function. It is also called Nadaraya-Watson regression.\n",
    "\n",
    "This smooths the data without fitting a fixed global model.\n",
    "\n",
    "Of course you will find the perfect banwidth by using Cross Validation\n",
    "\n",
    "#### **Kernel Regression vs Kernel Density Estimation (KDE)**\n",
    "\n",
    "| Feature                     | Kernel Regression (Nadaraya-Watson)             | Kernel Density Estimation (KDE)               |\n",
    "|----------------------------|--------------------------------------------------|------------------------------------------------|\n",
    "| 🎯 **Goal**                | Predict $y$ for a given $x$                     | Estimate the **density** of the data          |\n",
    "| 📈 **Input**               | Pairs $(x_i, y_i)$                              | Single variable $x_i$                         |\n",
    "| 📤 **Output**              | Smoothed estimate of $y$ as function of $x$     | Probability density function over $x$         |\n",
    "| 📦 **Formula**             | Weighted average of $y_i$'s                     | Weighted sum of kernels centered at $x_i$     |\n",
    "| 📍 **Kernel center**       | Centered at **each $x_i$**                     | Also centered at **each $x_i$**               |\n",
    "| 🔧 **Kernel function**     | Usually Gaussian or other symmetric functions   | Same (Gaussian, Epanechnikov, etc.)           |\n",
    "| 📌 **Bandwidth**           | Controls smoothing (chosen via cross-validation)| Controls smoothing (can use rules or CV)      |\n",
    "| 🔁 **Used for**            | Non-parametric regression                       | Density estimation / plotting distributions   |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Overfitting / Underfitting — Cross-Validation**\n",
    "\n",
    "- **Underfitting**: Model too simple → can't capture patterns\n",
    "- **Overfitting**: Model too complex → memorizes noise\n",
    "\n",
    "Use **cross-validation** to estimate model performance:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(\"Mean score:\", np.mean(scores))\n",
    "```\n",
    "\n",
    "Cross-validation-Score helps select:\n",
    "- Best model complexity (e.g., polynomial degree)\n",
    "- Regularization parameters\n",
    "\n",
    "---\n",
    "\n",
    "### **RMS or BIC vs Polynomial Degree**\n",
    "\n",
    "More regression coefficients improve the ability of the model to fit all the points (reduced bias), but at the expense of model complexity and variance. Of course we can fit a Nth-degree polynomial to N data points, but that would be foolish. We'll determine the best trade-off between bias and variance through cross-validation.\n",
    "\n",
    "When we increase the complexity of a model, the data points fit the model more and more closely. However, this process does not necessarily result in a better fit to the data. Rather, if the degree is too high, then we are overfitting the data. The model has high variance, meaning that a small change in a training point can change the model dramatically.\n",
    "\n",
    "We can evaluate this using a training set, a cross-validation set and a test set.\n",
    "\n",
    "Plotting **RMS or BIC vs polynomial degree** for both the CV set and training set can help choose the optimal degree — where adding complexity no longer improves performance.\n",
    "\n",
    "#### Root Mean Squared Error (RMS):\n",
    "\n",
    "$$\n",
    "\\text{RMS} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2}\n",
    "$$\n",
    "\n",
    "#### Bayesian Information Criterion (BIC):\n",
    "\n",
    "$$\n",
    "\\text{BIC} = k \\ln(n) - 2 \\ln(\\hat{L})\n",
    "$$\n",
    "\n",
    "- $k$: number of parameters  \n",
    "- $n$: number of observations  \n",
    "- $\\hat{L}$: maximum likelihood  \n",
    "\n",
    "#### Interpretation of RMS and BIC\n",
    "\n",
    "For low order, both the training and CV error are high. This is sign of a high-bias model that is underfitting the data.  \n",
    "For high order, the training error becomes small (by definition), but the CV error is large. This is the sign of a high-variance model that is overfitting the data.  \n",
    "The BICs give similar results.  \n",
    "We'd like to minimize the RMS or BIC, and the minimum should be the same.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Learning Curves**\n",
    "\n",
    "Learning curves show **train vs validation error** as the dataset size increases.\n",
    "\n",
    "Key patterns:\n",
    "- **High bias**: both train and val errors are high → increase model complexity\n",
    "- **High variance**: large gap between train and val → add more data or regularize\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, val_scores = learning_curve(model, X, y, cv=5)\n",
    "```\n",
    "\n",
    "Plot to diagnose model behavior and data sufficiency. We can see two regimes:\n",
    "\n",
    "- The training and CV errors have converged. This indicates that the model is dominated by bias. Increasing the number of training points is futile. If the error is too high, you instead need a more complex model, not more training data.\n",
    "- The training error is smaller than the CV error. This indicates that the model is dominated by variance. Increasing the number of training points may help to improve the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd232ae5",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 18** - Regression II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d9c00c",
   "metadata": {},
   "source": [
    "- Regularization\n",
    "- ridge regression\n",
    "- LASSO regularization\n",
    "- difference and similitude ridge / LASSO \n",
    "- Locally linear Regression (LOWESS / LOESS) (overwiev)\n",
    "- Non - linear regression (overwiev)\n",
    "- Gaussian process regression\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cadf3e",
   "metadata": {},
   "source": [
    "### **Regularization**\n",
    "\n",
    "When we make models more complex—like using very high-degree polynomials—they can start to fit the training data *too* well. This is called **overfitting**. It means the model learns not only the true pattern but also the noise in the data. As a result, the model does great on the data it has seen but performs badly on new, unseen data.\n",
    "\n",
    "**Regularization** helps prevent overfitting by adding a penalty that discourages the model from becoming too complex. This penalty keeps the model simpler and helps it generalize better to new data by balancing two things:\n",
    "- **Bias** (how much the model assumptions simplify the real data)\n",
    "- **Variance** (how much the model changes when trained on different data samples)\n",
    "\n",
    "\n",
    "Regularization is something extra we add during fitting to avoid overfitting.\n",
    "- Fitting = learning the best parameters from data.\n",
    "- Regularization = gently forcing the model to stay simple during fitting\n",
    "---\n",
    "\n",
    "### **Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Ridge regression tackles overfitting by adding a penalty on the *squared size* of the coefficients (parameters). The loss function it tries to minimize becomes:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{ridge}} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p \\theta_j^2\n",
    "$$\n",
    "\n",
    "- The first part measures how well the model fits the data.\n",
    "- The second part (with $\\lambda$) penalizes large coefficients to prevent overly complex models. Because if the parameters are too high, you can think that the function need to change a lot among point, that's overfitting.\n",
    "- $\\lambda$ in known as regularization parameter\n",
    "\n",
    "Key points:\n",
    "- **Coefficients get smaller** but don’t become exactly zero.\n",
    "- Good when many features contribute but might be correlated.\n",
    "- Keeps all features in the model but controls their impact.\n",
    "\n",
    "---\n",
    "\n",
    "### **LASSO Regression (L1 Regularization)**\n",
    "\n",
    "LASSO adds a penalty based on the *absolute value* of the coefficients:\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{\\text{lasso}} = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^p |\\theta_j|\n",
    "$$\n",
    "\n",
    "What makes LASSO special:\n",
    "- It can shrink some coefficients **exactly to zero**, effectively removing those features from the model.\n",
    "- This means LASSO does **feature selection** automatically.\n",
    "- Useful when you expect only a few important features out of many.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ridge vs. LASSO: Similarities and Differences**\n",
    "\n",
    "| Feature                 | Ridge                            | LASSO                               |\n",
    "|-------------------------|---------------------------------|-----------------------------------|\n",
    "| Penalty type            | Squares of coefficients ($L_2$) | Absolute values of coefficients ($L_1$) |\n",
    "| Feature selection       | No                              | Yes (some coefficients become zero) |\n",
    "| Effect on coefficients  | Shrinks smoothly towards zero   | Produces sparse solutions (some zero exactly) |\n",
    "| Best use case           | When many features matter, even if correlated | When only a few features are really important |\n",
    "\n",
    "The difference between Ridge and LASSO is just the shape of the constraint region. For LASSO, the shape is such that some of the parameters may end up being 0, which is super beneficial.\n",
    "\n",
    "Setting $\\lambda = 0 $ is mathemathicall identical to no regularizaion. But that's not necessarily true in the scikit-learn implementation: i.e. Ridge and Lasso with lambda =0 might not give the same result of LinearRegression. The regularization algorithms have additional sophistications to improve convergence. \n",
    "\n",
    "**How do we choose $\\lambda$?**\n",
    "We use cross-validation, just as we discussed before. In fact...Scikit-Learn has versions of Ridge and LASSO regression that do this automatically for you-- see RidgeCV and LassoCV.\n",
    "\n",
    "---\n",
    "\n",
    "### **Locally Linear Regression (LOWESS / LOESS)**\n",
    "\n",
    "LOWESS and LOESS are simple ways to make a smooth curve through data without assuming a fixed formula.\n",
    "\n",
    "How it works:\n",
    "- For each point, it looks at nearby points only.\n",
    "- Gives more importance to points that are closer.\n",
    "- Fits a simple line just to those nearby points.\n",
    "- Does this for every point, making a smooth curve that follows local changes.\n",
    "\n",
    "Why use it?\n",
    "- Very flexible and easy to understand.\n",
    "- Good when the relationship between variables changes in different areas.\n",
    "- Doesn’t force one shape to fit all data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Non-Linear Regression (Overview)**\n",
    "\n",
    "Non-linear regression fits curves or complex shapes to data, not just straight lines.\n",
    "\n",
    "Examples:\n",
    "- S-shaped growth curves\n",
    "- Exponential growth or decay\n",
    "- Neural networks (many layers of curves)\n",
    "\n",
    "How it works:\n",
    "- Uses trial-and-error methods (like gradient descent) to find the best curve.\n",
    "- It can be tricky to find the best fit and might need good starting guesses.\n",
    "- Useful when data clearly isn’t a straight line.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Gaussian Process Regression (GPR)**\n",
    "\n",
    "Gaussian Process Regression is a powerful **non-parametric** regression method. Unlike traditional regression techniques that assume a specific functional form (like a straight line or a polynomial), GPR assumes that the data come from a **distribution over functions**.\n",
    "\n",
    "**What is a Gaussian Process?**\n",
    "\n",
    "A **Gaussian Process (GP)** is a collection of random variables, any finite number of which have a **joint Gaussian distribution**.\n",
    "\n",
    "Think of a GP not as a single curve, but as a *distribution* over all possible smooth curves that could fit your data. When you observe some data points, you can narrow down this distribution and make predictions with uncertainty included.\n",
    "\n",
    "**GPR in Simple Words**\n",
    "\n",
    "- You give GPR some data: $x_i$, $y_i$ (inputs and outputs).\n",
    "- GPR looks at these data and says: \"what are all the *smooth* functions that could have produced this?\"\n",
    "- Then, for a new input $x^*$, it doesn't just give a single $y^*$ value but a **probability distribution** for what $y^*$ could be.\n",
    "\n",
    "**Mathematical Form**\n",
    "\n",
    "Let’s say we want to predict values of $y$ from inputs $x$. In GPR, we assume:\n",
    "\n",
    "$$\n",
    "y(x) \\sim \\mathcal{GP}(m(x), k(x, x'))\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $m(x)$ is the **mean function** (usually taken as zero: $m(x) = 0$).\n",
    "- $k(x, x')$ is the **kernel** or **covariance function**, defining how correlated the outputs are depending on their inputs.\n",
    "\n",
    "A popular choice for the kernel is the **Radial Basis Function (RBF)**:\n",
    "\n",
    "$$\n",
    "k(x, x') = \\sigma_f^2 \\exp\\left( -\\frac{(x - x')^2}{2 \\ell^2} \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\sigma_f^2$ controls the variance (how \"high\" the function goes),\n",
    "- $\\ell$ is the length scale (how \"wiggly\" the function is).\n",
    "\n",
    "**What GPR Gives You**\n",
    "\n",
    "When you input some new $x^*$ values, GPR gives you:\n",
    "- The **mean** prediction $\\mu(x^*)$\n",
    "- The **variance** $\\sigma^2(x^*)$\n",
    "\n",
    "This means you get **error bars** on every prediction!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1456ea0",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 19** - Classification I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4d96d6",
   "metadata": {},
   "source": [
    "- Generative VS Discriminative classification (differences and main concept)\n",
    "- performance of classifiers\n",
    "- Generative classification (discriminant function, bayes classifier, decision boundary)\n",
    "- Naive bayes\n",
    "- gaussian naive bayes\n",
    "- linear e quadratic discriminat analysis\n",
    "-  GMM and bayes classification\n",
    "- K - nearest neighbor classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b6ceab",
   "metadata": {},
   "source": [
    "### **Generative vs. Discriminative Classification**\n",
    "\n",
    "KDE is a unsupervised form of classification, we want to look now at supervised one.\n",
    "There are two different type:\n",
    "\n",
    "- **Generative classification** :\n",
    "    If we find ourselves asking which category is most likely to generate the observed result, then we are using using **density estimation** for classification and this is referred to as **generative classification**. Here we have a full model of the density for each class or we have a model which describes how data could be generated from each class. \n",
    "\n",
    "- **Discriminative classification** :\n",
    "    if we don't care about the full distribution, then we are doing something more like clustering, where we don't need to map the distribution, we just need to define boundaries.  Classification that finds the **decision boundary** that separates classes is called **discriminative classification**. \n",
    "\n",
    "For example, in the figure below, to classify a new object, it would suffice to know:\n",
    "1. model 1 is a better fit than model 2 (***generative classification***), or \n",
    "2. that the decision boundary is at $x=1.4$ (***discriminative classification***).\n",
    "\n",
    "![Ivezic, Figure 9.1](http://www.astroml.org/_images/fig_bayes_DB_1.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **Performance of Classifiers**\n",
    "\n",
    "- **Confusion matrix** (binary case):\n",
    "    - **True Positive** = **correctly identified**  = TP\n",
    "    - **True Negative** = **correctly rejected**  = TN\n",
    "    - **False Positive** = **incorrectly identified** = FP \n",
    "    - **False Negative** = **incorrectly rejected** = FN\n",
    "\n",
    "- **Metrics**:\n",
    "  - **Accuracy**: $\\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "\n",
    "    The proportion of all correct predictions (both positive and negative) out of the total number of predictions.\n",
    "\n",
    "  - **Completeness**: $\\frac{TP}{TP + FN}$\n",
    "\n",
    "    Out of all the actual positives, how many did the model correctly identify?\n",
    "\n",
    "  - **Contamination** :  $\\frac{FP}{TP + FP}$\n",
    "\n",
    "    fraction of false inside the true\n",
    "\n",
    "  - **Precision**: $\\frac{TP}{TP + FP}$\n",
    "\n",
    "    Out of all the instances the model predicted as positive, how many were actually positive?\n",
    "     - Precision = 1 - Contamination\n",
    "\n",
    "   - **F1-score**: harmonic mean of precision and completness\n",
    "\n",
    "---\n",
    "\n",
    "### **Generative Classification**\n",
    "\n",
    "Generative classification is a probabilistic approach that models **how the data is generated**. The key idea is to estimate the distribution of features for each class and then apply **Bayes’ theorem** to make predictions. This contrasts with discriminative methods, which model the decision boundary directly.\n",
    "\n",
    "####  Discriminant Function\n",
    "\n",
    "We can relate classification to regression: in regression we estimate a function $f(y \\mid x)$ to predict continuous values.  \n",
    "In classification, we do the same—but $y$ is discrete, e.g., $y \\in \\{0, 1\\}$.\n",
    "\n",
    "So we define a **discriminant function** $g(x)$ that estimates the probability of class membership:\n",
    "$$\n",
    "g(x) = p(y = 1 \\mid x)\n",
    "$$\n",
    "\n",
    "This function returns a probability, and we classify based on whether that probability exceeds a threshold (typically $0.5$).\n",
    "\n",
    "\n",
    "\n",
    "####  Bayes Classifier\n",
    "\n",
    "The **Bayes classifier** uses the discriminant function to make optimal decisions under uncertainty. For binary classification, it works as follows:\n",
    "\n",
    "$$\n",
    "\\hat{y} =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } g(x) > \\frac{1}{2} \\\\\n",
    "0 & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "This rule assigns a new input $x$ to the class with the **highest posterior probability**.\n",
    "\n",
    "It can also be extended to multi-class classification by choosing the class with the highest $p(y = k \\mid x)$ for all $k$.\n",
    "\n",
    "\n",
    "\n",
    "####  Decision Boundary\n",
    "\n",
    "The **decision boundary** is the set of all $x$ where the classifier is **uncertain**—i.e., where the posterior probabilities of two (or more) classes are equal.\n",
    "\n",
    "For the binary case:\n",
    "$$\n",
    "p(y=1 \\mid x) = p(y=0 \\mid x)\n",
    "$$\n",
    "\n",
    "This defines the surface (or line, or point) in the input space where the predicted class changes. In generative models, this boundary results from the underlying class distributions.\n",
    "\n",
    "---\n",
    "\n",
    "### **ROC Curve**\n",
    "\n",
    "The **ROC curve** (Receiver Operating Characteristic) is a fundamental tool to evaluate the performance of a **binary classifier**.\n",
    "\n",
    "It plots:\n",
    "\n",
    "- **True Positive Rate** (TPR) — also called **sensitivity** :  $ \\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}} $\n",
    "\n",
    "against\n",
    "\n",
    "- **False Positive Rate** (FPR) :   $ \\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}} $\n",
    "\n",
    "\n",
    "####  What the ROC Curve Tells\n",
    "\n",
    "- A classifier that performs perfectly will have a point at the **top-left** corner of the plot: TPR = 1, FPR = 0.\n",
    "- A **random classifier** will produce a diagonal line from (0,0) to (1,1).\n",
    "- The more the ROC curve bows **toward the top-left**, the better the classifier is.\n",
    "- The **Area Under the Curve (AUC)** is a single metric summarizing the model's ability to distinguish classes:\n",
    "  - AUC = 1 → perfect classification  \n",
    "  - AUC = 0.5 → random guessing  \n",
    "  - AUC < 0.5 → worse than random (possibly inverted predictions)\n",
    "\n",
    "   Note that you **choose** the completeness and efficiency that you want by choosing a **threshold (decision boundary)**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Naive Bayes**\n",
    "\n",
    "Naive Bayes is one of the simplest and most effective classification algorithms.  \n",
    "It is called “naive” because it assumes **conditional independence** between features given the class.\n",
    "\n",
    "\n",
    "**Make a pause:** a feature here is a single information for each element of the dataset, for example if you are searching for spam - email, a possible feature is \"how many times compare the world \"free\" in the email ?\".\n",
    "Each feature is indipendent from all the other in this approach.\n",
    "\n",
    "\n",
    "Formally, the model works in this way:\n",
    "$$\n",
    "P(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "where $C_k$ is the $k$-th class, and $x_i$ are the individual features of input $\\mathbf{x}$.\n",
    "\n",
    "This means that once we know the class, the model calculate the probability for each feature to be in the class and then multiply all of them.\n",
    "\n",
    "- Since Naive Bayes is a **supervised learning** method, we assume that we already know the true class labels in the training set.  \n",
    "- We use these to estimate the distributions $P(x_i \\mid C_k)$ and the priors $P(C_k)$,  \n",
    "- and then apply **Bayes’ theorem** to predict the class of new, unseen data.\n",
    "\n",
    "####  Pros:\n",
    "- Simple to implement\n",
    "- Very fast, both in training and prediction\n",
    "- Works surprisingly well even when the independence assumption is violated\n",
    "- Performs well on high-dimensional data\n",
    "\n",
    "---\n",
    "\n",
    "### **Gaussian Naive Bayes**\n",
    "\n",
    "Gaussian Naive Bayes is a variant of Naive Bayes used when the features are **continuous-valued** (i.e., real numbers rather than categories or binary values).\n",
    "\n",
    "Instead of estimating probabilities using histograms or counts, we assume that each feature $x_i$ follows a **Gaussian distribution** within each class $C_k$:\n",
    "$$\n",
    "P(x_i \\mid C_k) = \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp\\left( -\\frac{(x_i - \\mu_k)^2}{2\\sigma_k^2} \\right)\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $\\mu_k$ is the **mean** of feature $x_i$ for class $C_k$\n",
    "- $\\sigma_k^2$ is the **variance** of that feature within the class\n",
    "\n",
    "\n",
    "This model keeps the **Naive Bayes assumption** that all features are conditionally independent given the class:\n",
    "$$\n",
    "P(\\mathbf{x} \\mid C_k) = \\prod_{i=1}^{n} P(x_i \\mid C_k)\n",
    "$$\n",
    "\n",
    "Then, we use **Bayes' theorem** to compute the posterior probability of each class and choose the most likely one.\n",
    "\n",
    "---\n",
    "\n",
    "### **Linear and Quadratic Discriminant Analysis (LDA / QDA)**\n",
    "\n",
    "Both **Linear Discriminant Analysis (LDA)** and **Quadratic Discriminant Analysis (QDA)** are **generative classification models** that assume each class follows a **Gaussian distribution**.  \n",
    "They differ in how they treat the **covariance matrix**, which controls the shape and orientation of the Gaussian.\n",
    "\n",
    "\n",
    "####  Linear Discriminant Analysis (LDA)\n",
    "\n",
    "- Assumes the data for each class is normally distributed:\n",
    "  $$\n",
    "  \\mathbf{x} \\mid C_k \\sim \\mathcal{N}(\\mu_k, \\Sigma)\n",
    "  $$\n",
    "- All classes **share the same covariance matrix** $\\Sigma$\n",
    "- Class-specific means $\\mu_k$ are different\n",
    "- Because of the shared $\\Sigma$, the **decision boundary is linear**\n",
    "\n",
    "This means the model separates the classes with straight lines (or hyperplanes in higher dimensions).\n",
    "67\n",
    "\n",
    "####  Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "- Still assumes Gaussian distributions:\n",
    "  $$\n",
    "  \\mathbf{x} \\mid C_k \\sim \\mathcal{N}(\\mu_k, \\Sigma_k)\n",
    "  $$\n",
    "- But now **each class has its own covariance matrix** $\\Sigma_k$\n",
    "- This allows more flexibility in the shape of each class distribution\n",
    "- As a result, the **decision boundaries are quadratic curves**\n",
    "\n",
    "QDA is more powerful than LDA but also requires estimating more parameters, so it needs more data to avoid overfitting.\n",
    "\n",
    "\n",
    "####  Summary\n",
    "\n",
    "| Property          | LDA                                | QDA                                 |\n",
    "|------------------|-------------------------------------|--------------------------------------|\n",
    "| Covariance       | Shared $\\Sigma$                     | Class-specific $\\Sigma_k$            |\n",
    "| Decision Surface | Linear                              | Quadratic                            |\n",
    "| Flexibility      | Less (simpler model)                | More (can fit complex boundaries)    |\n",
    "| Data Requirement | Lower (fewer parameters)            | Higher (more parameters to estimate) |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Gaussian Mixture Models (GMM) and Bayes Classification**\n",
    "\n",
    "So far, our generative classifiers have relied on fairly **strong assumptions**, such as:\n",
    "- **Conditional independence** of features (Naive Bayes)\n",
    "- **Single Gaussian distribution** per class (LDA/QDA)\n",
    "\n",
    "These models work well, but they may fail when the true data distribution is more complex.\n",
    "\n",
    "\n",
    "####  Bayes Classification with GMM\n",
    "\n",
    "A more **flexible and expressive** approach is to use **Gaussian Mixture Models** to represent the class-conditional distributions.\n",
    "\n",
    "Instead of assuming that each class is modeled by a single Gaussian, we assume it is a **mixture of multiple Gaussians**.\n",
    "\n",
    "This allows us to **model complex, multimodal distributions** for each class.\n",
    "\n",
    "This methode is called GMM Bayes Classifier\n",
    "\n",
    "####  Why use GMMs?\n",
    "\n",
    "- More expressive than a single Gaussian\n",
    "- Can model **non-linear, complex class boundaries**\n",
    "- Especially useful when data exhibits **clusters within each class**\n",
    "\n",
    "#### Considerations\n",
    "\n",
    "- Requires choosing the number of components $K$\n",
    "- Can be computationally expensive\n",
    "- Risk of **overfitting** with too many components\n",
    "- **NOTE:** We can take this to the extreme by having one mixture component at each training point. We also don't have to restrict ourselves to a Gaussian kernel, we can use any kernel that we like. The resulting ***non-parametric*** Bayes classifier is referred to as **Kernel Discriminant Analysis (KDA)**.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **K-Nearest Neighbor Classifier** \n",
    "\n",
    "- **Non-parametric** method\n",
    "- Classify new point $\\mathbf{x}$ by majority vote among its $k$ nearest neighbors\n",
    "- A large choice of K decrease the variance in the classification but increase the bias\n",
    "- you can pick the best K by cross - validation, with the intent to reduce the error\n",
    "\n",
    "Pros:\n",
    "- No training needed\n",
    "- Simple\n",
    "\n",
    "Cons:\n",
    "- Expensive at test time\n",
    "- Choice of $k$ and distance metric affects performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936a68d1",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 20** - Classification II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e5151",
   "metadata": {},
   "source": [
    "- Discriminative Classification\n",
    "- Logistic regression\n",
    "- Support Vector Machines\n",
    "- kernel method\n",
    "- Decision tree\n",
    "- splitting criteria\n",
    "- ensemble learning\n",
    "- bagging\n",
    "- Random forest\n",
    "- Boosting\n",
    "- what should I use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47090d0b",
   "metadata": {},
   "source": [
    "### **Discriminative Classification & Advanced Models**\n",
    "\n",
    "Discriminative classifiers do not model how data “came to be” (that's generative classification), but instead **directly learn** the mapping from inputs $x$ to labels $y$.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Logistic Regression**\n",
    "\n",
    "predict $P(y=1 \\mid x)$ by fitting a linear model in the log-odds space.\n",
    "\n",
    "The only difference between LDA and Logstic Regression is how the regression coefficients are estimated. In LDA they are chosen to minimize density estimation error, whereas in Logistic Regression they are chosen to minimize classifcation error.\n",
    "\n",
    "---\n",
    "\n",
    "### **Support Vector Machines (SVM)**\n",
    "\n",
    "define a hyperplane (a plane in $N-1$ dimensions) that maximizes the distance of the closest point from each class. This distance is the \"margin\". The points that touch the margin (or that are on the wrong side) are called **support vectors**. \n",
    "\n",
    "**What happen if the data have some overlap between the classes?**\n",
    "\n",
    "When this happens, a **hard-margin SVM** (which requires perfect separation) won’t work.  \n",
    "That’s why we use the **soft-margin SVM**, which allows some mistakes but still tries to find the best separating hyperplane.\n",
    "\n",
    "- The model introduces a **slack variables** that allow some points to be:\n",
    "  - Inside the margin\n",
    "  - Or even misclassified\n",
    "- The optimization now balances two goals:\n",
    "  1. **Maximize the margin**\n",
    "  2. **Minimize the total slack** (how many violations we allow)\n",
    "\n",
    "This is controlled by a **regularization parameter** $C$:\n",
    "- Large $C$ → penalize misclassifications more (tighter margin, less tolerant)\n",
    "- Small $C$ → allow more violations (wider margin, more tolerant)\n",
    "\n",
    "The soft-margin SVM still tries to separate the classes as cleanly as possible,  \n",
    "but it **accepts some overlap** to achieve a better **generalization** on unseen data.\n",
    "\n",
    "**Some important notion:**\n",
    "1) SVM is not scale invariant, is worth rescaling the mean to 0 and variance of 1\n",
    "2) Once the support vectors are determined, changes to the positions or numbers of points beyond the margin will not change the decision boundary\n",
    "3) Strong resilience to outliers\n",
    "4) This is why there is a high completeness compared to the other methods: it does not matter if the background sources is much bigger. It simply determines the best boundary between the small source clump and the large background clump.\n",
    "5) This completeness, however, comes at the cost of a relatively large contamination level.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Kernel Method**\n",
    "\n",
    "If the contamination is driven by non-linear effects, it may be worth implementing a **non-linear decision boundary**. We can do this by ***kernelization**.\n",
    "\n",
    "A first possibility is simply to aad a new dimension at the dataset (from 2D to 3D for example), i create the information associated at this dimension, so it could be something that clearly separete the point in different cluster, then i can fin the best plane to separete them.\n",
    "\n",
    "---\n",
    "\n",
    "### **Decision Trees**\n",
    "\n",
    "A **decision tree** is similar to the process of classification that you might do by hand: \n",
    "\n",
    "- define some criteria to separate the sample into 2 groups (not necessarily equal),\n",
    "- then take those sub-groups and do it again.  \n",
    "- keep going until you reach a stopping point such as not having a minimum number of objects to split again.  \n",
    "\n",
    "In short, we have done a hierarchical application of decision boundaries.\n",
    "\n",
    "The tree structure is as follows:\n",
    "- top node contains the entire data set\n",
    "- at each branch the data are subdivided into two child nodes \n",
    "- split is based on a predefined decision boundary (usually axis aligned)\n",
    "- splitting repeats, recursively, until we reach a predefined stopping criteria \n",
    "\n",
    "Build a tree by **recursively splitting** on feature thresholds until leaves are pure or other stopping criteria are met.\n",
    "\n",
    "#### Splitting Criteria\n",
    "\n",
    "At each node, choose feature $j$ and threshold $t$ to minimize weighted impurity.\n",
    "The typical process for finding the optimal decision boundary is to perform trial splits along each feature one at a time, within which the value of the feature to split at is also trialed. The feature that allows for the maximum information gain is the one that is split at this level.\n",
    "\n",
    "- **Gini Impurity:**  \n",
    "  $$\n",
    "  G = 1 - \\sum_{k=1}^K p_k^2\n",
    "  $$\n",
    "  It essentially estimates the probability of incorrect classification by choosing both a point and (separately) a class randomly from the data.\n",
    "- **Entropy (Information Gain):**  \n",
    "  $$\n",
    "  H = -\\sum_{k=1}^K p_k \\log p_k\n",
    "  $$\n",
    "  where $p_k$ is the fraction of samples of class $k$ in the node.  \n",
    "\n",
    "Obviously in constructing a decision treee, if your choice of stopping criteria is too loose, further splitting just ends up adding noise.  So using cross-validation in order to optimize the depth of the tree (and to avoid overfitting) is the best choice.\n",
    "\n",
    "---\n",
    "\n",
    "### **Ensemble Learning**\n",
    "\n",
    "Combine multiple “weak learners” (the method seen before) to form a stronger model.\n",
    "\n",
    "#### **Bagging (Bootstrap Aggregating)**\n",
    "\n",
    "- **bootstrap** the samples (with replacement) of size $N$.\n",
    "- Train an independent tree on each sample.\n",
    "- **Aggregate** by majority vote (classification) or average (regression).\n",
    "\n",
    "Reduces **variance** without increasing bias.\n",
    "\n",
    "For a sample of $N$ points in a training set, bagging generates $B$ equally sized bootstrap samples from which to estimate the function $f_i(x)$. The final estimator for $\\hat{y}$, defined by bagging, is then\n",
    "\n",
    "$$\\hat{y} = f(x) = \\frac{1}{B} \\sum_i^B f_i(x).$$\n",
    "\n",
    "**NOTE** : you can put the parameter `n_jobs=-1`; That says to use all the cores of your machine to do the job.  That's one of the benefits of bagging.  It can be made parallel trivially -- one bagging process has nothing to the with the others.  You just average them all together when you are done. \n",
    "\n",
    "#### **Random Forest**\n",
    "\n",
    "Random forests extend bagging by generating decision trees from the bootstrap samples. In addition to drawing random samples from our training set with replacement, we may also draw random subsets of features for training the individual trees.\n",
    " \n",
    "- In Random Forests, the splitting features on which to generate the tree are selected at random from the full set of features in the data.\n",
    "- The number of features selected per split level is typically the square root of the total number of features, $\\sqrt{D}$. \n",
    "- The fundamental difference is that in Random forests, only a subset of features are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike in bagging where all features are considered for splitting a node. \n",
    "- The final classification from the random forest is based on the averaging of the classifications of each of the individual decision trees.\n",
    "- As alway use CV to determine the optimal depth of the tree\n",
    "\n",
    "---\n",
    "### **Boosting**\n",
    "\n",
    "**Boosting** is a way to make a strong model by combining **many weak models** that aren't very good on their own.\n",
    "\n",
    "#### Basic Idea\n",
    "\n",
    "1. Train a simple model.\n",
    "2. See which points it gets wrong.\n",
    "3. Focus more on those hard points next time.\n",
    "4. Repeat this several times.\n",
    "5. At the end, combine all the models to get a better result.\n",
    "\n",
    "Each new model **tries to fix the mistakes** made by the ones before.\n",
    "\n",
    "#### What Makes Boosting Special?\n",
    "\n",
    "- It gives **more importance** to points that are hard to classify.\n",
    "- The final prediction is a **weighted vote** of all the models.\n",
    "- The better a model is, the more say it gets.\n",
    "\n",
    "#### AdaBoost (Adaptive Boosting)\n",
    "\n",
    "- A popular type of boosting.\n",
    "- After each round, it **boosts** (increases) the weight of the wrong points.\n",
    "- This helps the next model **focus** on the tough stuff.\n",
    "- change the learning rate, where half the learning rate means that weights are boosted half as much for each iteration.\n",
    "\n",
    "#### Downside\n",
    "\n",
    "- Boosting is **slow** because models are built **one after the other**.\n",
    "- Can be sensitive to noisy data.\n",
    "- Not easy to run in parallel like Random Forests.\n",
    "\n",
    "---\n",
    "\n",
    "## **Model Selection: “What Should I Use?”**\n",
    "\n",
    "| Method                     | Type            | Strengths                                                                 | Weaknesses                                                         |\n",
    "|----------------------------|------------------|---------------------------------------------------------------------------|--------------------------------------------------------------------|\n",
    "| **Logistic Regression**    | Discriminative   | Fast, interpretable, works well on linearly separable data                | Struggles with nonlinearity                                        |\n",
    "| **SVM (linear / kernel)**  | Discriminative   | Strong margin, effective in high‐dim spaces, kernels for nonlinearity     | Computationally heavy for large $N$                                |\n",
    "| **Decision Tree**          | Discriminative   | Intuitive rules, handles mixed data types                                 | High variance, prone to overfitting                                |\n",
    "| **Random Forest**          | Discriminative   | Excellent off‐the‐shelf performance, handles large feature sets            | Less interpretable, many parameters                                |\n",
    "| **Bagging**                | Discriminative   | Reduces variance of unstable learners                                     | Requires multiple models, less interpretable                       |\n",
    "| **Boosting** | Discriminative | High accuracy, focuses on hard examples                                   | Slower to train, sensitive to noise and overfitting                |\n",
    "| **Naive Bayes**            | Generative       | Very fast, works surprisingly well with many features                     | Assumes feature independence, which is often unrealistic           |\n",
    "| **Gaussian Naive Bayes**   | Generative       | Handles continuous data with a normal distribution assumption             | Poor with non-Gaussian data or correlated features                 |\n",
    "| **LDA (Linear Disc. Analysis)** | Generative | Simple, fast, works well with Gaussian data and equal covariances         | Assumes same covariance matrix across classes                      |\n",
    "| **QDA (Quadratic Disc. Analysis)** | Generative | More flexible than LDA, models class-specific covariance          | Needs more data, prone to overfitting with small samples           |\n",
    "| **GMM + Bayes Classifier** | Generative       | Can model complex, multimodal distributions per class                     | Training (EM algorithm) is slower, sensitive to initialization     |\n",
    "| **K-Nearest Neighbors (KNN)** | Generative| No training needed, simple and intuitive                                  | Slow at test time, sensitive to irrelevant features & scaling      |\n",
    "\n",
    "\n",
    "Naive Bayes and its variants are by far the easiest to compute. Linear support vector machines are more expensive, though several fast algorithms exist. Random forests can be easily parallelized. Boosting helps with challenging classification (but at that point you might want to go all the way to deep learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bb09e8",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 21** - Deep Learning I </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ad4f8",
   "metadata": {},
   "source": [
    "- Loss function\n",
    "- Gradient Descent\n",
    "- AdaBoost\n",
    "- Neural Networks\n",
    "- In more detail\n",
    "- Backpropagation\n",
    "- About derivative\n",
    "- number of layers\n",
    "- number of neurons\n",
    "- activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ce9f5",
   "metadata": {},
   "source": [
    "### **Loss function**\n",
    "\n",
    "A **loss function** measures how “bad” a model’s prediction is compared to the true value. The smaller the loss, the better your model is doing.\n",
    "\n",
    "It helps guide the **training** of a machine learning model, by telling it *how wrong* its predictions are, so it can improve.\n",
    "\n",
    "\n",
    "#### **Common Loss Functions**\n",
    "\n",
    "#### L2 Loss (Mean Squared Error - MSE)\n",
    "\n",
    "This is typical in **regression problems**:\n",
    "\n",
    "$$\n",
    "L_2 = (y - f(x))^2\n",
    "$$\n",
    "\n",
    "- Squared difference between prediction and actual value.\n",
    "- Penalizes **larger errors more heavily**.\n",
    "- Smooth and easy to differentiate → good for gradient descent.\n",
    "- Assumes Gaussian noise in the data.\n",
    "\n",
    "### L1 Loss (Mean Absolute Error - MAE)\n",
    "\n",
    "Also used in regression:\n",
    "\n",
    "$$\n",
    "L_1 = |y - f(x)|\n",
    "$$\n",
    "\n",
    "- Less sensitive to outliers than L2.\n",
    "- Leads to **sparser** models (used in LASSO).\n",
    "- Less smooth → gradient descent can be trickier.\n",
    "\n",
    "\n",
    "#### Classification loss function\n",
    "\n",
    "In **classification**, especially **binary classification**, your true labels are {-1, +1} and your model's prediction is a **score** or **probability**.\n",
    "\n",
    "So we look at:\n",
    "\n",
    "$$\n",
    "y \\cdot f(x)\n",
    "$$\n",
    "\n",
    "- If this is **positive**, your model got the right class ( +1 * +1  or -1 * -1).\n",
    "- If this is **negative**, it got it wrong ( +1 * -1  or -1 * +1).\n",
    "- Larger positive → more confident correct prediction.\n",
    "- More negative → confident wrong prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f86e0",
   "metadata": {},
   "source": [
    "This function does something reasonable for $y*f(x)\\le1$.  However, look what happens at larger values where we are even more confident that $y*f(x)$ is positive and that our class should be $+1$.  The loss goes **up**.  That's bad.\n",
    "\n",
    "We need a loss function that makes sense for classification.\n",
    "\n",
    "- The first we'll try is the so-called **Zero-One Loss**. It is 1 for $yf(x)<0$ and 0 for $yf(x)>0$; thus the name. You increment the loss function by 1 every time you make a wrong prediction. It is just a count of the total number of mistakes. However, the Zero-One loss is hard to minimize, so instead we can try something that allows the loss to be continuous function in $y*f(x)$.  \n",
    "\n",
    "- The **Hinge Loss**, which looks like $${\\rm max}(0,1-y*f(x)),$$. Here there is no contribution to the loss for values $\\ge 1$, but there is a linearly increasing loss for smaller values. So, it penalizes both wrong predictions and also correct predictions that have low confidence.\n",
    "\n",
    "- A **Logistic Loss** (also called the *log loss* and *cross entropy loss*) function has similar properties as shown in **blue**, but is smoother and has slightly less and less penalty for more and more confident $+1$ predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Gradient descendent**\n",
    "\n",
    "**Gradient descent** is a method to find the best parameters $\\theta$ that minimize a loss function.\n",
    "\n",
    "In this course, we've been trying to find $\\theta$ that gives the best fit to our training data — without overfitting.\n",
    "\n",
    "There are different ways to do that:\n",
    "- Sometimes we can calculate the best $\\theta$ directly.\n",
    "- Sometimes we try many random values (like in MCMC).\n",
    "- But gradient descent is like standing on a hill and walking downhill until you reach the bottom.\n",
    "\n",
    "We update the parameters using:\n",
    "\n",
    "$$\n",
    "\\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\nabla J(\\theta)\n",
    "$$\n",
    "\n",
    "- $\\eta$  is the **learning rate** (step size).\n",
    "- $\\nabla J(\\theta)$  is the slope (gradient) of the cost function.\n",
    "\n",
    "\n",
    "### Learning rate:\n",
    "\n",
    "- Too small → takes forever to reach the minimum.\n",
    "- Too big → can skip over the minimum or even diverge.\n",
    "\n",
    "### Other notes:\n",
    "\n",
    "- We start from random values of $\\theta$.\n",
    "- Gradient descent works well when the cost function has only one minimum (like with  L2 loss).\n",
    "- It's also useful when the dataset is too big to load all at once.\n",
    "\n",
    "![boh](https://miro.medium.com/max/1400/0*GaO7X6j3coh3oNwf.png)\n",
    "\n",
    "---\n",
    "\n",
    "### **AdaBoost**\n",
    "\n",
    "Let's see lecture 20\n",
    "\n",
    "---\n",
    "\n",
    "### **Neural Network**\n",
    "\n",
    "**Artificial Neural Networks** are a simplified computational architecture based loosely on the real neural networks found in brains.\n",
    "\n",
    "the term \"neural network\" is overused and encompasses a huge variety of deep learning approaches. What we are going to explore is more properly called **multi-layer perceptron**\n",
    "\n",
    "In the image below, \n",
    "- the circles on the **left** represent the **features/attributes** of our input data, $X$, which here is 3 dimensional.  \n",
    "- the circles in the **middle** represent the **neurons**. They take in the information from the input and, based on some criterion decide whether or not to \"fire\". These middle layers are called \"**hidden layers**\".\n",
    "- the collective results of the neurons in the hidden layer produce the **output**, $y$, which is represented by the circles on the **right**, which here is 2 dimensional result.  \n",
    "- the lines connecting the circles represent the synapses.  \n",
    "\n",
    "This is a simple example with just one layer of neurons; however, there can be many layers of neurons.\n",
    "\n",
    "![Cartoon of Neural Network](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e4/Artificial_neural_network.svg/500px-Artificial_neural_network.svg.png)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **In more detail**\n",
    "\n",
    "A **synapse** multiplies the input \\( x \\) by a **weight** \\( w \\), adds a **bias** \\( b \\), and sends the result to the neuron:\n",
    "\n",
    "$$\n",
    "z = \\sum_i w x_i + b\n",
    "$$\n",
    "\n",
    "We then apply an **activation function**, like the sigmoid:\n",
    "\n",
    "$$\n",
    "a = \\frac{1}{1 + e^{-z}}\n",
    "$$\n",
    "\n",
    "The network learns the best weights and biases to model the target values.\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpropagation**\n",
    "\n",
    "Once the network makes a prediction, we compare it to the true value and calculate the error.\n",
    "\n",
    "**Backpropagation** is the method used to figure out which weights caused the error, and by how much.\n",
    "\n",
    "We do this by:\n",
    "- Computing the **gradient** (the direction and size of change needed) for each weight.\n",
    "- Updating the weights using **gradient descent**, which moves them in the direction that reduces the error.\n",
    "\n",
    "This process is repeated many times during training. It's much faster than trying all possible combinations of weights.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **About derivative**\n",
    "\n",
    "As you see, deep learning at the end of the day requires computing a ton of derivatives. Computing numerical derivatives is a tricky business. One could naively approximate\n",
    "\n",
    "$$ f'(x) \\sim \\frac{f(x+ \\Delta x) - f(x)}{\\Delta x} $$\n",
    "\n",
    "for a small value of $\\Delta x$. If you try you'll quickly convince youself that this is numerically very unstable.\n",
    "\n",
    "So why is deep learning so successfull and widely adopted? This is largely due to **automatic differentiation**, which is in my opinion one of the smartest computational idea ever. \n",
    "\n",
    "The key idea is that a computer is a simple machine, even if the software we write can be very complicated. All a computer can do at the chip level are elementary arithmetic operations (addition, subtraction, multiplication, division, etc.) and elementary functions (exp, log, sin, cos, etc.). Given a complicated piece of software, one can track the (very long!) lists of elemetary function that are required to reproduce the result. With this list, computing a derivative is just a long but trivial application of the chain rule. In practice, this means that derivates of any oder can be computed accurately to working precision very quickly.\n",
    "\n",
    "In python, **JAX** is a very powerful add-on to numpy with automatic differentiation capabilities (written by folks at Google).\n",
    "\n",
    "The other computational revolution behind the success of deep learning is the exploitation of **Graphic processing units (GPUs)** . CPUs are the standard silicon chips used by all computers, including your laptopt. GPUs are graphic boards designed for expensive video rendering (the market was largely driven  by gaming professionals; now it's all AI). It turns out they're extremely good at matrix multiplication, and deep learning has lots of matrices.\n",
    "\n",
    "---\n",
    "\n",
    "### **Parameter of a network**\n",
    "\n",
    "### Number of layer\n",
    "No layers are needed for linear regression.  We just connect our input to the output where the synapses are the weights (slopes) and the output neurons add the constant (intecept).\n",
    "\n",
    "So you might start with a single layer, then add more layers and use cross-validation to determine when you are overfitting.\n",
    "\n",
    "\n",
    "### Number of neutron\n",
    "\n",
    "The number of neurons in each layer is also a free parameter. \n",
    "\n",
    "- **Typically choose somewhere between twice the number of input nodes and a number between the number of input and output nodes.**\n",
    "\n",
    "\n",
    "- If there are lots hidden layers (where \"lots\" is not clearly defined) then we call that a **deep neural network or **deep learning**.\n",
    "\n",
    "- Sometimes the number of neurons in each layer goes down.  But it can also be useful to have the same number in each layer so that there is only one hyperparameter (the number of neurons) and not one per layer, obviously it depend on the problem.\n",
    "\n",
    "\n",
    "- In practice a reasonable approach is to simply **specify many more layers and neurons than you need and perform regularization**. This can be as simple as just stopping the training when the cross-validation error reaches a minimum, which appropriately (for once) is called **early stopping**.\n",
    "\n",
    "\n",
    "While the number of neurons in the hidden layers are free parameters the number of input and output nodes are constrained by the data and the desired output.  For example, the MNIST digits data requires 784 input neurons (one for each pixel in the 28x28 images) and 10 output neurons (one for each class [digit]).  \n",
    "\n",
    "\n",
    "### Vanishing and Exploding Gradients\n",
    "\n",
    "In the past, training deep neural networks was hard because of the **vanishing** and **exploding gradient** problems — the gradients became too small or too large, making learning unstable.\n",
    "\n",
    "**ReLU Activation**\n",
    "\n",
    "A solution came with the **ReLU (Rectified Linear Unit)** activation function:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(z) = \\max(0, z)\n",
    "$$\n",
    "\n",
    "- It helps fix the vanishing gradient issue.\n",
    "- It's simple and fast.\n",
    "- But it can \"kill\" some neurons when $ z < 0 $ (gradient is 0).\n",
    "\n",
    "\n",
    "**Other Activations possibilities**\n",
    "\n",
    "Since 2015, better versions of ReLU have been introduced:\n",
    "- **Leaky ReLU**\n",
    "- **Randomized Leaky ReLU**\n",
    "- **ELU**, **SELU**\n",
    "\n",
    "\n",
    "Note: You can use different activation functions in different layers.  \n",
    "In regression problems, we often **don’t use any activation** in the final layer to allow any output value.\n",
    "\n",
    "\n",
    "### Activation function\n",
    "The **Activation function** controls how much \"signal\" it takes for a neuron to \"fire\".\n",
    "\n",
    "\n",
    "### Regularization\n",
    "\n",
    "Neural networks can **overfit**, just like other models. To avoid this, we use **regularization**.\n",
    "\n",
    "- **L1 (LASSO)** and **L2 (Ridge)** regularization add penalties to large weights.\n",
    "- **Dropout** randomly turns off some neurons during training (usually 10–50%) so the network doesn't rely too much on specific ones. After training, all neurons are used again.\n",
    "\n",
    "This makes the model more flexible and prevents it from becoming too dependent on certain parts.\n",
    "\n",
    "\n",
    "### Batch Normalization\n",
    "\n",
    "Like we normalize input data, we can also normalize the output of hidden layers — this is called **batch normalization**.\n",
    "\n",
    "It helps make training:\n",
    "- **Faster**\n",
    "- **More stable**\n",
    "\n",
    "It’s done just before applying the activation function.\n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "We can improve training by transforming the data. This is called **data augmentation**.\n",
    "\n",
    "Examples:\n",
    "- Add extra features like $x^2$\n",
    "- For images: rotate, flip, or zoom to create more variety\n",
    "\n",
    "This helps the network learn the key patterns better.\n",
    "\n",
    "\n",
    "### Faster Optimizers\n",
    "\n",
    "To train the network, we can use smarter update methods called **optimizers**.\n",
    "\n",
    "Some common ones:\n",
    "- `'adam'`\n",
    "- `'sgd'` (stochastic gradient descent)\n",
    "- `'adagrad'`\n",
    "\n",
    "These can speed up learning and improve results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25d4d5e",
   "metadata": {},
   "source": [
    "## <span style=\"color:red\"> **LECTURE 22** - Deep Learning II </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b64a7a",
   "metadata": {},
   "source": [
    "- keras\n",
    "- Photo-z with Deep Learning (PyTorch)\n",
    "- Convolutional Neural Networks (CNNs) with Keras\n",
    "- Autoencoders and Variational Autoencoders\n",
    "- Generative Adversarial Networks (GANs)\n",
    "- Large Language Models (LLMs) and Transformers\n",
    "- Extra Topics: Flows and Simulation-Based Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a1885d",
   "metadata": {},
   "source": [
    "### **Keras**\n",
    "\n",
    "Keras is a high-level API (Application Programming Interface) for building and training deep learning models.  \n",
    "Think of it as similar to Scikit-Learn, but for neural networks:\n",
    "\n",
    "- If `numpy` is used for low-level array operations,\n",
    "- and `tensorflow` for deep learning internals,\n",
    "- then `keras` simplifies building models on top of `tensorflow`.\n",
    "\n",
    "It makes creating, training, and testing neural networks much easier.\n",
    "\n",
    "---\n",
    "\n",
    "### **Photo-z with Deep Learning (PyTorch)**\n",
    "\n",
    "In astronomy, one common task is estimating the redshift of a galaxy based on its color data.  \n",
    "This is called **photo-z** (photometric redshift).\n",
    "\n",
    "Using `pytorch`, we can build a deep neural network to learn this task from real SDSS (Sloan Digital Sky Survey) galaxy data.  \n",
    "It shows how deep learning can be applied to real scientific problems, even on a personal laptop.\n",
    "\n",
    "---\n",
    "\n",
    "### **Convolutional Neural Networks (CNNs) with Keras**\n",
    "\n",
    "CNNs are a special kind of neural network especially good at working with images.  \n",
    "With Keras, we can build CNNs quickly and train them to recognize patterns or classify data in images.\n",
    "\n",
    "This is helpful for many tasks, especially in image-based datasets like galaxy images.\n",
    "\n",
    "---\n",
    "\n",
    "### **Convolutional Layers**\n",
    "\n",
    "Convolutional layers are the core building blocks of **Convolutional Neural Networks (CNNs)**.\n",
    "\n",
    "They apply small filters (like $3 \\times 3$ or $5 \\times 5$) across the input image to detect **local patterns**, like edges, textures, orizontal , vertical or shapes.\n",
    "\n",
    "Each filter slides over the image and produces a new image called a **feature map**.\n",
    "\n",
    "- The network learns which filters are useful.\n",
    "- This allows the model to focus on important parts of the image.\n",
    "\n",
    "![vertical filter](https://miro.medium.com/max/1338/1*7IEbBib7Yc7qST5IGQoYXA.jpeg)\n",
    "\n",
    "![horizontal filter](https://miro.medium.com/max/1238/1*PSSAaH2pZbl5bK3Ef_zk4A.jpeg)\n",
    "\n",
    "\n",
    "This remind polarized light, which is a usefull methode to visualize CNNs in my mind...\n",
    "\n",
    "---\n",
    "\n",
    "### **Pooling Layers**\n",
    "\n",
    "After convolution, we often use **pooling layers** to reduce the size of the feature maps.\n",
    "\n",
    "This helps:\n",
    "- Decrease computation\n",
    "- Reduce overfitting\n",
    "- Keep only the most important information\n",
    "\n",
    "Pooling summarizes nearby pixels into a single value.\n",
    "\n",
    "#### Max Pooling\n",
    "\n",
    "**Max pooling** is the most common pooling method.\n",
    "\n",
    "- It looks at a small region (e.g., $2 \\times 2$) of the feature map\n",
    "- It outputs the **maximum value** from that region\n",
    "\n",
    "This keeps the strongest features and throws away the weaker ones.\n",
    "\n",
    "So instead of keeping all pixel values, it just keeps the most \"active\" one from each patch.\n",
    "\n",
    "![Pooling example](https://miro.medium.com/max/1000/1*ydNsGDxMldAiq7b96GDQwg.jpeg)\n",
    "\n",
    "---\n",
    "\n",
    "### **Autoencoders and Variational Autoencoders**\n",
    "\n",
    "Autoencoders are neural networks used to **compress and reconstruct data**.  \n",
    "They learn a compact representation of the input.Then is possible to decode the encode input to reconstruct the original dataset. \n",
    "\n",
    "**This is the deep-learning to the problem of dimensionality reduction.**  You can think of an autoencoder as doing PCA with a neural network -- breaking our data down into the only the most important features that we actually need (finding the intrinsic dimensionality). In fact, if the network uses only linear (or no) activation functions and $L2$ cost function, then we have exactly PCA.  \n",
    "\n",
    "![autoencoder example](https://miro.medium.com/max/1400/1*SxwRp9i23OM0Up4sEze1QQ@2x.png)\n",
    "\n",
    "\n",
    "### **Variational Autoencoders (VAEs)**\n",
    "\n",
    "**VAEs** are a special kind of autoencoder that do more than just compress and reconstruct data — they also learn the **distribution** behind the data.\n",
    "\n",
    "#### How it works:\n",
    "\n",
    "Like a normal autoencoder, a VAE has two parts:\n",
    "- An **encoder**: compresses input data into a smaller representation\n",
    "- A **decoder**: reconstructs the data from this representation\n",
    "\n",
    "But instead of learning a single vector as the encoding, the encoder learns two things:\n",
    "- A **mean** vector  $\\mu$ \n",
    "- A **standard deviation** vector $\\sigma$\n",
    "\n",
    "From these, the model samples a latent variable $z$ from a Gaussian distribution:\n",
    "\n",
    "$$\n",
    "z \\sim \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$\n",
    "\n",
    "This makes the encoding **stochastic**, not deterministic.\n",
    "\n",
    "\n",
    "#### Why this matters:\n",
    "\n",
    "- The decoder learns to generate data from different points in this latent space.\n",
    "- You can **sample** new values of $z$, feed them into the decoder, and get **new, realistic data**.\n",
    "- This is useful for **generating new examples** that are similar to the training data.\n",
    "\n",
    "---\n",
    "\n",
    "### **Generative Adversarial Networks (GANs)**\n",
    "\n",
    "GANs consist of two networks:\n",
    "- A **generator** that tries to make fake data,\n",
    "- A **discriminator** that tries to tell if the data is real or fake.\n",
    "\n",
    "They are trained together in a sort of game, improving each other over time.  \n",
    "GANs are widely used to create realistic images and simulations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Large Language Models (LLMs) and Transformers**\n",
    "\n",
    "LLMs are deep learning models that can understand and generate human language.  \n",
    "They’re built using **transformers**, a special architecture that captures the relationship between words efficiently.\n",
    "\n",
    "These models are used in applications like translation, chatbots, and text generation.\n",
    "\n",
    "**GPT = generative pretrained transformers**\n",
    "\n",
    "A GPT is a (very!) complicated mathematical function that predicts, given a piece of text, what word should come next. \n",
    "\n",
    "GPT-4 by OpenAI allegedly has **1.76 trillion parameters**. The dataset? It's **the entire internet**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Extra Topics: Flows and Simulation-Based Inference**\n",
    "\n",
    "These are more advanced deep learning tools:\n",
    "- **Normalizing Flows** are used to model complex probability distributions.\n",
    "- **Simulation-Based Inference** is used when traditional statistical methods don’t work well, especially in physics or astronomy, where simulations are easier to get than real data.\n",
    "\n",
    "These methods expand what deep learning can do beyond classic prediction tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
